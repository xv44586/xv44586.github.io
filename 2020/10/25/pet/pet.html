<hr><p>title: pet<br>date: 2020-10-22 16:54:19</p><h2 id="tags"><a href="#tags" class="headerlink" title="tags:"></a>tags:</h2><p>之前的一篇<a href="https://xv44586.github.io/2020/09/13/classification-label-augment/">《模型增强-从label下手》</a>中，我们提到想通过转换label，增加一个任务，来尝试增强分类任务的性能。而其中的主要思路是想通过文本生成对应的label description ，将文本分类任务转换为NLG任务来做，不过当时考虑到UniLM中提到seq2seq的训练<code>不能</code>提高NLU的能力，所以当时并没有选择使用MLM来尝试，最后得到的结论是：</p><ol><li><ul><li>将分类转为生成后，性能基本一致；*</li></ul></li><li><ul><li>将分类与生成联合起来训练，性能与单个任务性能基本一致。*</li></ul></li></ol><p>最近看了两篇将分类任务转化为MLM的论文，也让我眼前一亮，发现原来我与顶会思路这么近（误）</p><h1 id="mlm"><a href="#mlm" class="headerlink" title="mlm"></a>mlm</h1><p>mlm,即Masked Language Model,中文翻译又叫“掩码语言模型”，即以自监督的方式，mask 掉一部分，然后通过剩余的部分来还原被mask 掉的部分，示意图如下：<br><img src="pet/mlm.png" alt=""></p><p>而mask的方式也有多种，如随机mask选择token进行mask；将token所在的整个词都mask（whole word mask）；或者将某个span内的token都mask掉（span mask）。<br>虽然mlm在预训练任务上已经被证明十分有效，但是通常认为mlm部分的参数是与mlm任务相关的，而通常在下游任务中我们是别的任务，所以会舍弃掉这部分参数，而只使用encoder部分。<br>但是论文<a href="http://arxiv.org/abs/2009.07118">It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</a>与<a href="http://arxiv.org/abs/2001.07676">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</a>却告诉我们，mlm不仅有用，在few-shot场景下，通过一下简单的融合手段，性能能超过当前的明星GPT-3.</p><h1 id="任务转换"><a href="#任务转换" class="headerlink" title="任务转换"></a>任务转换</h1><p>与之前的思路类似，我们针对分类任务，不再直接对label进行预测，而是预测其label description，即将其转换为完形填空形式的任务，来预测不同label description的概率。<br>而如何转换成完形填空呢？也很简单，我们添加一个简单的语义通顺的描述，然后将其中与分类有关的内容mask掉即可。举个例子：<br>假如我们现在的任务是短文本分类，一个样本为“context：’「天天特价房」华庭仁和国际 3室2厅2卫仅售65万’, label: ‘房产’”，我们添加一个统一的描述句，将其变为：<br>“下面是一则__相关新闻标题: 「天天特价房」华庭仁和国际 3室2厅2卫仅售65万”,其中的空格可选的内容是所有的label description，对应的真实值是”房产”两个字，这样，我们就将分类任务转换为一个完形填空的形式。<br>而添加的方式也可以分为前缀、后缀两种，完整的方式：</p><pre><code class="python"><span class="string">"以下是一则__相关新闻标题: 「天天特价房」华庭仁和国际 3室2厅2卫仅售65万"</span>
<span class="string">"「天天特价房」华庭仁和国际 3室2厅2卫仅售65万，以上是一则__相关新闻标题"</span>
</code></pre><h1 id="Pattern-Exploiting-Training"><a href="#Pattern-Exploiting-Training" class="headerlink" title="Pattern-Exploiting Training"></a>Pattern-Exploiting Training</h1><p>上面我们添加的前缀/后缀句子称为<code>Pattern</code>, 而label description可以有多种方式，比如，对于“房产”这个label，我们也可以用“地产”来表达，对于“娱乐”label，也可以用“八卦”来表达，所以需要一个token到label的映射，这个映射可以是多对一的，这个被称为<code>Verbalizer</code></p>