<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>样本不均衡之难易不均衡 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2020/10/14/focal-loss/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2020/10/14/focal-loss/gn.jpeg" alt="样本不均衡之难易不均衡"></div><header class="post__info"><h1 class="post__title">样本不均衡之难易不均衡</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2020-10-14</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/样本不均衡/">样本不均衡</a></li><li class="mark__item"><a href="/tags/Loss/">Loss</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#cross-entropy">Cross Entropy</a></li><li><a href="#yang-ben-lei-bie-bu-jun-heng">样本类别不均衡</a></li><li><a href="#focal-loss">Focal Loss</a><ul><li><a href="#ru-he-que-ding-alpha-yu-gamma">如何确定$\alpha$ 与 $\gamma$</a></li><li><a href="#shi-yan">实验</a></li></ul></li><li><a href="#ghm">GHM</a></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><p></p></div><br>上篇<a href="https://xv44586.github.io/2020/09/25/fastbert/">看样本下菜的FastBERT</a>提到样本有难易，通过利用样本的这个特性，可以在推理上进行加速，除了在推理上可以利用，在训练时也可以利用，本篇就来说怎么在训练时更充分的利用样本有难易的特性。<p></p><h1><span id="cross-entropy">Cross Entropy</span><a href="#cross-entropy" class="header-anchor"></a></h1><p>对于分类问题，通常我们选择交叉熵作为损失。本文均针对二分类进行说明，多分类的情况可以横向扩展。对于二分类问题来说，其损失CE：</p><p>$$<br>CE = \left\{\begin{matrix}<br>-log(p)&amp; y\_true=1 \\<br>-log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p><h1><span id="yang-ben-lei-bie-bu-jun-heng">样本类别不均衡</span><a href="#yang-ben-lei-bie-bu-jun-heng" class="header-anchor"></a></h1><p>当我们遇到一个正负样本不均衡的情况，如1:1000时，直接训练后效果往往不好，其倾向于将更多的样本预测为类别多的类，而产生的原因是：由于我们训练时使用的 CE:<br>$CE_W = CE_positive + CE_negative$, 其中CE_positive 与 CE_negative 分别代表正负样本的loss，而由于此时的样本不均衡，loss主要有类别多的样本贡献，主导了优化方向，所以模型会偏向数量多的方向，如当前全部预测为正样本，那解决这个问题最简单直接的办法就是在loss上增加一个权重α来均衡一下两方的loss，从而让模型更“公平”的对待不同类别样本,即：</p><p>$$<br>CE_W = \left\{\begin{matrix}<br>-\alpha log(p)&amp; y\_true=1 \\<br>-(1-\alpha)log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p><h1><span id="focal-loss">Focal Loss</span><a href="#focal-loss" class="header-anchor"></a></h1><p>除了在类别上可能存在这种不均衡外，样本在难易程度上往往也会有难易之分。如当训练一个情感分类器时，“不喜欢xx”就比“谁不喜欢xx呢”要容易训练一些. 为了衡量这种“难易”特征，我们定义一个代表预测值与真实label 之间差距的参数$p_t$:</p><p>$$<br>p_t = \left\{\begin{matrix}<br>1-p,&amp; y\_true=1 \\<br>p,&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p><p>即</p><p>$$<br>p_t = \begin{vmatrix}<br>pred - y_{true}<br>\end{vmatrix}<br>$$</p><p>pt越大则说明预测值与其label 相差越大，也即样本越“难训练”，最后我们对整个样本的pt 统计往往得到一个U型分布，如下图所示：<br><img src="/2020/10/14/focal-loss/a1.png" alt><br>即“易训练”样本是“难训练”样本的指数级。虽然此时“易训练”样本由于得到了很好的训练，其loss 很小，当由于其数量庞大，任然可能主动整个训练。<br>所以为了解决难易不均衡的问题，我们采用与样本不均衡一样的方法：对不同样本添加一个权重来平衡，即此时的loss FL:</p><p>$$<br>FL = \left\{\begin{matrix}<br>-\alpha \beta(p\_t) log(p)&amp; y\_true=1 \\<br>-(1-\alpha)\beta(1-p\_t)log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p><p>而前面我们说难易样本的loss 呈指数级差距，所以此时的$\beta(p_t)$ 我们也定义为指数函数，最终的 FL:</p><p>$$<br>FL = \left\{\begin{matrix}<br>-\alpha(1 - p)^{\gamma} log(p)&amp; y\_true=1 \\<br>-(1-\alpha)(p)^{\gamma}log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p><p>此时，$\alpha$ 用来平衡样本不均衡，$(1-p)^{\gamma}$ 用来均衡难易样本。通过平衡难易样本对应损失，让模型更“关注”那些难分的样本。</p><p>以上就是<a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">focal loss</a> 的主要思想，虽然最后我们得到的loss形式上与focal loss一样，但其中参数的含义与focal loss中的内容却有一些不同，主要在于focal loss 中实验证明，由于对难易样本降权后正样本（量少的类）对应的loss反而更易主动优化方向，所以用 $\alpha$ 来降权，而我们上面提到的alpha 主要是用来均衡正负样本，这里读者可以自行判断理解。此外，苏剑林通过硬截断过渡到软阶段也得到了类似的loss，推荐大家也看看：<a href="https://spaces.ac.cn/archives/4733" target="_blank" rel="noopener">从loss的硬截断、软化到focal loss</a></p><h2><span id="ru-he-que-ding-alpha-yu-gamma">如何确定$\alpha$ 与 $\gamma$</span><a href="#ru-he-que-ding-alpha-yu-gamma" class="header-anchor"></a></h2><p>在focal loss论文内，作者是通过搜索一个范围来确定两个参数的最优解，最后给出的结果是 $\alpha = 0.25$, $\gamma=2.$，而通过上面我们提到的两个参数的含义，这里给出一个确定参数范围的方案：<br>1.首先，我们通过统计正负样本，来确定$\alpha$的大致范围；<br>2.通过CE_W我们可以训练一个基础的分类器，通过这个分类器，我们对训练集进行预测，生成对应的prob，然后通过统计$p_t$，我们认为$p_t&lt;=0.1$ 的为主要的“易分样本”，${p_t&gt;=0.9}$ 为主要“难分样本”，由于是指数衰减，所以两者的loss 差距为 $9^\gamma$, 即此时$9^\gamma=C_易/C_难$, 解出此时的$\gamma$ 即可得到其大致的范围。</p><h2><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h2><p>实验时，通过构造一个正负样本8:1的数据集进行实验，在通过权重平衡正负样本不均衡后，对应的pt分布如下图：<br><img src="/2020/10/14/focal-loss/a1.png" alt><br><img src="/2020/10/14/focal-loss/p1.png" alt><br><img src="/2020/10/14/focal-loss/n1.png" alt><br>而focal loss 训练后的pt 分布为：<br><img src="/2020/10/14/focal-loss/a2.png" alt><br><img src="/2020/10/14/focal-loss/p2.png" alt><br><img src="/2020/10/14/focal-loss/n2.png" alt><br>可以看到，在focal loss 下，右侧偏差大的样本基本都被移到了左侧，说明“难样本”大幅度减少变为了“易样本”。<br>实验代码地址：<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_focal_loss.py" target="_blank" rel="noopener">classification use focal loss</a></p><h1><span id="ghm">GHM</span><a href="#ghm" class="header-anchor"></a></h1><p>现在让我们来讨论一下focal loss存在的问题：</p><ol><li>首先，让模型过多的关注那些特别难分的样本没有什么问题，但是这个前提：样本紧凑。而当数据中存在离群点时，那此时就会发生：本来模型已经收敛了，但是由于这些离群点还是会误判，一直存在在pt的最右侧，而让模型再过多的去关注这些点，这明显是不合适的；</li><li>对于focal loss中的两个参数$\alpha$ 和$\gamma$ ，虽然我们能估算一个大致范围，但是由于两者是相互影响的，所以实际使用时还是需要通过实验去寻找最优解，这也为训练增加了一定的难度。</li></ol><p>现在再让我们回过头来重新审视一下我们的原始问题：样本有难易之分，所以训练时存在难易样本不均衡，而”易分”样本占比过高导致主导优化方向。那此时让我们往后再思考一步，当我们对易分样本降权后，对应的pt分布图中最左侧的柱子会降低，而由于模型得到了更好的优化方向，模型的性能提高，所以最右侧的柱子也会降低，两边减少的样本会同时向”中间”扩散，最后得到一个比原始pt分布曲线更”平滑”的分布曲线，正如上文中focal loss对应的pt分布图。<br>而focal loss 由于过度关注”难分样本”，导致存在离群点时不理想的问题。而离群点有一个特点就是：量相对正常样本非常少（否则就是一个”小群”了），利用这个特点，我们就能对focal loss 进行改进了。改进的思路就是利用离群点少的特点，从难易样本的量上来平衡难易样本的loss。<br>具体做法：我们将$p_t$ 按间隔$\varepsilon$均等的分为K个区间，然后统计不同区间内的样本数量$num_k$，然后针对每个区间内的loss 我们用参数$\beta(i)$ 来平滑：</p><p>$$<br>L_{GHN-C} = \sum_{1}^{N}\beta(i)L_{CE}(p_i, \hat p_i)<br>$$</p><p>其中：<br>$\beta(i)$对应$pt_i$所属区间的样本$num_k$在整体样本 $N$ 中占比的倒数。<br>而在实现时，由于通常我们都是采取mini-batch 的方式训练，无法在每个batch内事先得到全局统计量进行$\beta(i)的计算，一种近似的办法是利用动量，逐步近似求。<br>以上就是<a href="http://arxiv.org/abs/1811.05181" target="_blank" rel="noopener">GHM</a> 在分类情况下的loss，原始论文中的pt 分布对比图中也能看到，利用GHM确实更平滑。<br><img src="/2020/10/14/focal-loss/gn.jpeg" alt></p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文介绍了两种针对样本难易不均衡问题的loss：focal loss 与 GHM，并通过实验进一步验证了其有效性，在一些样本不均衡的场景下均可尝试使用。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://arxiv.org/pdf/1811.05181.pdf" target="_blank" rel="noopener">Gradient Harmonized Single-stage Detector</a>中配图。</p><div class="content-footer-sponsor"><h1><span id="sponsor">Buy me a coffee</span></h1><p>如果觉得这篇文章不错，对你有帮助，欢迎打赏一杯蜜雪冰城。</p><img src="/img/sponsor.JPG" alt="logo" title="sponsor"></div><div class="post__prevs"><div class="post__prev"><a href="/2020/09/30/location/" title="年轻人的第一个swift：ios 模拟定位打卡"><i class="iconfont icon-prev"></i>年轻人的第一个swift：ios 模拟定位打卡</a></div><div class="post__prev post__prev--right"><a href="/2020/10/19/triangle/" title="一切三段成三角形">一切三段成三角形<i class="iconfont icon-next"></i></a></div></div></div></article><script src="https://giscus.app/client.js" data-repo="xv44586/giscus" data-repo-id="R_kgDOIC6Ipg" data-category="Announcements" data-category-id="DIC_kwDOIC6Ips4CRkmo" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="zh-CN" crossorigin="anonymous" async></script></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">33</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2023/03/25/gpt4/" title="GPT-4 yes!! but"><div class="item__cover"><img src="/2023/03/25/gpt4/bg.jpeg" alt="GPT-4 yes!! but"></div><div class="item__info"><h3 class="item__title">GPT-4 yes!! but</h3><span class="item__text">2023-03-25</span></div></a></li><li class="latest-post-item"><a href="/2023/03/10/llm-inf/" title="LLM Inference串讲"><div class="item__cover"><img src="/2023/03/10/llm-inf/sd.PNG" alt="LLM Inference串讲"></div><div class="item__info"><h3 class="item__title">LLM Inference串讲</h3><span class="item__text">2023-03-10</span></div></a></li><li class="latest-post-item"><a href="/2023/02/01/fine-tuning-at-few-shot/" title="few-shot视角下的fine-tuning"><div class="item__cover"><img src="/2023/02/01/fine-tuning-at-few-shot/himalayas.JPG" alt="few-shot视角下的fine-tuning"></div><div class="item__info"><h3 class="item__title">few-shot视角下的fine-tuning</h3><span class="item__text">2023-02-01</span></div></a></li><li class="latest-post-item"><a href="/2023/01/09/zero-to-chatgpt/" title="From zero to ChatGPT"><div class="item__cover"><img src="/2023/01/09/zero-to-chatgpt/chatgpt-bg.jpeg" alt="From zero to ChatGPT"></div><div class="item__info"><h3 class="item__title">From zero to ChatGPT</h3><span class="item__text">2023-01-09</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/BPE/">BPE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/ChatGPT/">ChatGPT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-3/">GPT-3</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-4/">GPT-4</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/In-context-learning/">In-context learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Inference/">Inference</a></li><li class="tag-item"><a class="tag-link" href="/tags/LLM/">LLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/T5/">T5</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Unigram/">Unigram</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordPiece/">WordPiece</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/debug/">debug</a></li><li class="tag-item"><a class="tag-link" href="/tags/faster-decoder/">faster decoder</a></li><li class="tag-item"><a class="tag-link" href="/tags/fine-tuning/">fine-tuning</a></li><li class="tag-item"><a class="tag-link" href="/tags/horovod/">horovod</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/nohup/">nohup</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/simbert/">simbert</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/speed-up/">speed-up</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>