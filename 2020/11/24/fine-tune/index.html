<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>如何提升bert在下游任务中的性能 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2020/11/24/fine-tune/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2020/08/31/bert-01/bert.jpg" alt="如何提升bert在下游任务中的性能"></div><header class="post__info"><h1 class="post__title">如何提升bert在下游任务中的性能</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2020-11-24</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/BERT/">BERT</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#further-pre-training">Further Pre-training</a><ul><li><a href="#er-jie-duan-vs-san-jie-duan-vs-si-jie-duan">二阶段 vs 三阶段 vs 四阶段</a><ul><li><a href="#san-jie-duan">三阶段</a></li><li><a href="#si-jie-duan">四阶段</a></li></ul></li></ul></li><li><a href="#ru-he-further-pre-training">如何further pre-training</a><ul><li><a href="#how-to-mask">how to mask</a></li><li><a href="#when-to-stop">when to stop</a></li><li><a href="#how-to-fine-tuning">how to fine-tuning</a><ul><li><a href="#optimizer">optimizer</a></li><li><a href="#learning-rate">learning rate</a></li><li><a href="#multi-task">multi-task</a></li><li><a href="#which-layer">which layer</a></li></ul></li></ul></li><li><a href="#self-knowledge-distillation">Self-Knowledge Distillation</a></li><li><a href="#zhi-shi-zhu-ru">知识注入</a></li><li><a href="#shu-ju-zeng-qiang">数据增强</a></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><p>随着Transformer 在NLP中的表现，Bert已经成为主流模型，然而大家在下游任务中使用时，是不是也会发现模型的性能时好时坏，甚至相同参数切换一下随机种子结果都不一样，又或者自己不管如何调，模型总达不到想象中的那么好，那如何才能让Bert在下游任务中表现更好更稳呢？本文以文本分类为例，介绍几种能帮你提高下游任务性能的方法。</p><h1><span id="further-pre-training">Further Pre-training</span><a href="#further-pre-training" class="header-anchor"></a></h1><p>最稳定也是最常用的提升下游任务性能的手段就是继续进行预训练了。</p><h2><span id="er-jie-duan-vs-san-jie-duan-vs-si-jie-duan">二阶段 vs 三阶段 vs 四阶段</span><a href="#er-jie-duan-vs-san-jie-duan-vs-si-jie-duan" class="header-anchor"></a></h2><p>首先回顾一下，Bert 是如何使用的呢？我们设通用泛化语料为$D_g$，下游任务相关的数据为$D_t$, Bert 即在通用语料$D_g$ 上训练一个通用的Language Model， 然后利用这个模型学到的通用知识来做下游任务，也就是在下游任务上做fine-tune，这就是<code>二阶段模式</code>。大多数情况下我们也都是这么使用的：下载一个预训练模型，然后在自己的数据上直接fine-tune。</p><h3><span id="san-jie-duan">三阶段</span><a href="#san-jie-duan" class="header-anchor"></a></h3><p>在论文<a href="http://arxiv.org/abs/1801.06146" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a>中，作者提出了一个通用的范式ULMFiT：</p><ol><li><em>在大量的通用语料上训练一个LM（Pretrain）；</em></li><li><em>在任务相关的小数据上继续训练LM（Domain transfer）；</em></li><li><em>在任务相关的小数据上做具体任务（Fine-tune）。</em></li></ol><p>那我们在使用Bert 时能不能也按这种范式，进行三阶段的fine-tune 从而提高性能呢？答案是：<code>能！</code><br>比如邱锡鹏老师的论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>和<a href="arXiv:2004.10964 [cs]" target="_blank" rel="noopener">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a>中就验证了，在任务数据$D_t$ 继续进行pretraining 任务，可以提高模型的性能。<br>那如果我们除了任务数据没有别的数据时，怎么办呢？简单，任务数据肯定是相同领域的，此时直接将任务数据看作相同领域数据即可。所以，在进行下游任务之前，不妨先在任务数据上继续进行pre-training 任务继续训练LM ，之后再此基础上进行fine-tune。</p><h3><span id="si-jie-duan">四阶段</span><a href="#si-jie-duan" class="header-anchor"></a></h3><p>我们在实际工作上，任务相关的label data 较难获得，而unlabeled data 却非常多，那如何合理利用这部分数据，是不是也能提高模型在下游的性能呢？答案是：<code>也能！</code></p><ol><li><em>在大量通用语料上训练一个LM（Pretrain）；</em></li><li><em>在相同领域$D_{in_domain}$上继续训练LM（Domain transfer）；</em></li><li><em>在任务相关的小数据上继续训练LM（Task transfer）；</em></li><li><em>在任务相关数据上做具体任务（Fine-tune）。</em></li></ol><p>而且上述两篇论文中也给出了结论：先Domain transfer 再进行Task transfer 最后Fine-tune 性能是最好的。</p><h1><span id="ru-he-further-pre-training">如何further pre-training</span><a href="#ru-he-further-pre-training" class="header-anchor"></a></h1><h2><span id="how-to-mask">how to mask</span><a href="#how-to-mask" class="header-anchor"></a></h2><p>首先，在further pre-training时，我们应该如何进行mask 呢？不同的mask 方案是不是能起到更好的效果呢？<br>在Roberta 中提出，动态mask 方案比固定mask 方案效果更好，此外，在做Task transfer 时，由于数据通常较小，固定的mask 方案通常也容易过拟合，所以further pre-training 时，动态随机mask 方案通常比固定mask 效果更好。<br>而ERNIE 和 SpanBert 中都给出了结论，更有针对性的mask 方案可以提升下游任务的性能，那future pre-training 时是否有什么方案能更有针对性的mask 呢？<br>刘知远老师的论文<a href="http://arxiv.org/abs/2004.09733" target="_blank" rel="noopener">Train No Evil: Selective Masking for Task-Guided Pre-Training</a>就提出了一种更有针对性的mask 方案<code>Selective Mask</code>,进行further pre-training 方案，该方案的整体思路是：</p><ol><li><em>在$D_t$上训练一个下游任务模型 $Model_0$;</em></li><li><em>利用$Model_0$判断token 是否是下游任务中的重要token，具体计算公式为：$S(w_i) = P(y_t|s) - P(y_t|s^{‘}_{i-1}W_i)$, 其中$s$为完整句子（序列），$s^{‘}$为一个初始化为空的buffer，每次将句子中的token 往buffer中添加，如果加入的token 对当前任务的表现与完整句子在当前任务的表现差距小于阈值，则认为该token 为重要token，并从buffer 中剔除；</em></li><li><em>利用上一步中得到的token label，训练一个二分类模型$Model_b$，来判断句子中的token 是否为重要token；</em></li><li><em>利用$Model_b$，在domain 数据上进行预测，根据预测结果进行mask ；</em></li><li><em>进行Domain transfer pre-training；</em></li><li><em>在下游任务进行Fine-tuning。</em><br>上述方案验证了更有针对性的mask 重要的token，下游任务中能得到不错的提升。综合下来，<code>Selective Mask &gt; Dynamic Mask &gt; Static Mask</code></li></ol><p>虽然selective mask 有提升，但是论文给出的思路太过繁琐了，本质上是判断token 在下游任务上的影响，所以这里给出一个笔者自己脑洞的一个方案：通过$Model_0$在unlabeled 的Domain data 上直接预测，然后通过不同token 下结果的熵的波动来确定token 对下游任务的影响。这个方案我没有做过实验，有兴趣的可以试试。</p><h2><span id="when-to-stop">when to stop</span><a href="#when-to-stop" class="header-anchor"></a></h2><p>在further pretraining 时，该何时停止呢？是否训练的越久下游任务就提升的越多呢？答案是否定的。在进行Task transfer 时，应该训练多少步，论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>进行了实验，最后得出的结论是<code>100k</code>步左右，下游任务上提升是最高的，这也与我自己的实验基本吻合，训练过多就会过拟合，导致下游任务上提升小甚至降低。</p><p><img src="/2020/11/24/fine-tune/step.png" alt></p><p>此外，由于下游任务数据量的不同，进行多少步结果是最优的也许需要实验测试。这里给出一个更快捷稳妥的方案：借鉴PET本质上也是在训练MLM 任务，我们可以先利用利用PET做fine-tuning，然后将最优模型作为预训练后的模型来进行分类任务fine-tuning，这种方案我实验后的结论是与直接进行Task transfer性能提升上相差不大。不了解PET的可以查看我之前博文<a href="https://xv44586.github.io/2020/10/25/pet/">PET-文本分类的又一种妙解</a>.</p><h2><span id="how-to-fine-tuning">how to fine-tuning</span><a href="#how-to-fine-tuning" class="header-anchor"></a></h2><p>不同的fine-tuning 方法也是影响下游任务性能的关键因素。</p><h3><span id="optimizer">optimizer</span><a href="#optimizer" class="header-anchor"></a></h3><p>关于优化方案上，Bert 的论文中建议使用与bert 预训练时一致的方案进行fine-tuning，即使用weighted decay修正后的Adam，并使用warmup策略 搭配线性衰减的学习率。不熟悉的同学可以查看我之前的博文<a href="https://xv44586.github.io/2020/08/01/optimizer-in-bert/">optimizer of bert</a></p><h3><span id="learning-rate">learning rate</span><a href="#learning-rate" class="header-anchor"></a></h3><p>不合适的learning rate可能会导致<code>灾难性遗忘</code>,通常learning rate 在$[-e^{-5}, 1e^{-4}]$之间，更大的learning rate可能就会发生灾难性遗忘，不利于优化。</p><p><img src="/2020/11/24/fine-tune/lrt.png" alt></p><p>此外，对transformer 逐层降低学习率也能降低发生灾难性遗忘的同时提升一些性能。</p><h3><span id="multi-task">multi-task</span><a href="#multi-task" class="header-anchor"></a></h3><p>Bert在预训练时，使用了两个task：NSP 和 MLM，那在下游任务中，增加一个辅助的任务是否能带来提升呢？答案是否定的。如我之前尝试过在分类任务的同时，增加一个相似性任务：让样本与label desc的得分高于样本与其他样本的得分，但是最终性能并没有得到提升。具体的实验过程请看博文<a href="https://xv44586.github.io/2020/09/13/classification-label-augment/">模型增强之从label下手</a>。<br>此外，论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>也任务multi-task不能带来下游任务的提升。</p><h3><span id="which-layer">which layer</span><a href="#which-layer" class="header-anchor"></a></h3><p>Bert的结构上是一个12层的transformer，在做文本分类时，通常我们是直接使用最后一层的<code>[CLS]</code>来做fine-tuning，这样是最优的吗？有没有更好的方案？<br>论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>中针对这个问题也做了实验，对比了不同的layer不同的抽取策略，最终结论是所有层拼接效果最好，但是与直接使用最后一层差距不大。<br><img src="/2020/11/24/fine-tune/layer.png" alt></p><p>而论文<a href="http://arxiv.org/abs/2008.06460" target="_blank" rel="noopener">Hate Speech Detection and Racial Bias Mitigation in Social Media based on BERT model</a>中，作者通过组合多种粒度的语义信息，即将12层的<code>[CLS]</code>拼接后，送人CNN，在Hate Speech Detection 中能带来<code>8个点</code>的提升！<br><img src="/2020/11/24/fine-tuning/cnn.png" alt></p><p>所以在fine-tuning时，也可以想一想到底是哪种粒度的语义信息对任务更重要。</p><h1><span id="self-knowledge-distillation">Self-Knowledge Distillation</span><a href="#self-knowledge-distillation" class="header-anchor"></a></h1><p>self-knowledge distillation（自蒸馏）也是一种常用的提升下游任务的手段。做法是先在Task data上fine-tuning 一个模型，然后通过模型得到Task data 的soft labels，然后使用soft labels 代替hard label 进行fine-tuning。更多细节可以查看之前的博文<a href="https://xv44586.github.io/2020/08/31/bert-01/">Knowledge Distillation之知识迁移</a></p><h1><span id="zhi-shi-zhu-ru">知识注入</span><a href="#zhi-shi-zhu-ru" class="header-anchor"></a></h1><p>通过注入外部知识到bert中也能提升Bert的性能，常用的方式主要有两种：</p><ol><li><em>在bert embedding 层注入：通过将外部Embedding 与Bert token-embedding 拼接（相加）进行融合，然后进行transformer一起作用下游；</em></li><li><em>在transformer的最后一层，拼接外部embedding，然后一起作用下游。</em><br>如<a href="http://arxiv.org/abs/1909.08402" target="_blank" rel="noopener">Enriching BERT with Knowledge Graph Embeddings for Document Classification</a>中，通过在<br>transformer的最后一层中拼接其他信息，提高模型的性能。<br><img src="/2020/11/24/fine-tune/kg.png" alt></li></ol><h1><span id="shu-ju-zeng-qiang">数据增强</span><a href="#shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>NLP中数据增强主要有两种方式：一种是保持语义的数据增强，一种是可能破坏语义的局部扰动增强。保持语义通常采用回译法，局部扰动的通常使用EDA，更多细节可以查看之前博文<a href="https://xv44586.github.io/2020/11/10/eda/">NLP中的数据增强</a></p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文总结了使用bert 时，当前主要的提升Bert 在下游任务上的性能的方法，遇到相关问题时，可以尝试一下。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><div class="post__prevs"><div class="post__prev"><a href="/2020/11/23/scl/" title="Contrastive Learning"><i class="iconfont icon-prev"></i>Contrastive Learning</a></div><div class="post__prev post__prev--right"><a href="/2021/01/12/matrix/" title="重新认识矩阵">重新认识矩阵<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">22</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">5</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2021/01/20/ccf-qa-2/" title="ccf问答匹配比赛（下）：如何只用“bert”夺冠"><div class="item__cover"><img src="/2021/01/20/ccf-qa-2/head.png" alt="ccf问答匹配比赛（下）：如何只用“bert”夺冠"></div><div class="item__info"><h3 class="item__title">ccf问答匹配比赛（下）：如何只用“bert”夺冠</h3><span class="item__text">2021-01-20</span></div></a></li><li class="latest-post-item"><a href="/2021/01/12/matrix/" title="重新认识矩阵"><div class="item__cover"><img src="/2021/01/12/matrix/det.png" alt="重新认识矩阵"></div><div class="item__info"><h3 class="item__title">重新认识矩阵</h3><span class="item__text">2021-01-12</span></div></a></li><li class="latest-post-item"><a href="/2020/11/24/fine-tune/" title="如何提升bert在下游任务中的性能"><div class="item__cover"><img src="/2020/08/31/bert-01/bert.jpg" alt="如何提升bert在下游任务中的性能"></div><div class="item__info"><h3 class="item__title">如何提升bert在下游任务中的性能</h3><span class="item__text">2020-11-24</span></div></a></li><li class="latest-post-item"><a href="/2020/11/23/scl/" title="Contrastive Learning"><div class="item__cover"><img src="/img/default_cover.jpeg" alt="Contrastive Learning"></div><div class="item__info"><h3 class="item__title">Contrastive Learning</h3><span class="item__text">2020-11-23</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA-match/">QA match</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>