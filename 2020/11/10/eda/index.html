<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>NLP中的数据增强 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2020/11/10/eda/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2020/11/10/eda/latent.png" alt="NLP中的数据增强"></div><header class="post__info"><h1 class="post__title">NLP中的数据增强</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2020-11-10</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/EDA/">EDA</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#shu-ju-zeng-qiang">数据增强</a></li><li><a href="#bao-chi-yu-yi-shu-ju-zeng-qiang">保持语义数据增强</a><ul><li><a href="#hui-yi">回译</a></li><li><a href="#sheng-cheng">生成</a></li></ul></li><li><a href="#ju-bu-rao-dong">局部扰动</a><ul><li><a href="#tong-yi-ci-ti-huan">同义词替换</a></li><li><a href="#cha-ru">插入</a></li><li><a href="#shan-chu">删除</a></li><li><a href="#hu-huan">互换</a></li></ul></li><li><a href="#zong-jie">总结</a></li></ul><p></p></div><br>最近有同学问nlp中如何做data augmentation，这篇总结下目前知道的方法。<p></p><h1><span id="shu-ju-zeng-qiang">数据增强</span><a href="#shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>数据增强技术已经是图像领域的标配了，如旋转、镜像、翻转等。由于图像本身的特性，通过这些操作后生成的图像虽然与原始图像不同，但其图像的内容确实基本一致的。所以可以增强模型的鲁棒性和泛化能力。<br>而在NLP领域情况确是不同的，因为NLP中改变一个词有可能变为语义完全想反的句子，比如：<code>“这好吃吧”</code> -&gt; <code>“这好吃吗”</code>.<br>所以，NLP中数据增强主要有两种方式：一种是保持语义的数据增强，一种是可能破坏语义的局部扰动增强。</p><h1><span id="bao-chi-yu-yi-shu-ju-zeng-qiang">保持语义数据增强</span><a href="#bao-chi-yu-yi-shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>保持语义的数据增强主要是构造与原句子语义一样的新句子，如回译、生成等。</p><h2><span id="hui-yi">回译</span><a href="#hui-yi" class="header-anchor"></a></h2><p>回译即将句子从当前语种翻译至新的语种，然后再翻译回来，得到语义相同表达不同的句子。如将句子从中文翻译为英文然后再翻译回中文。可以借助各大互联网平台的免费API来完成。除此之外，还可以多翻译几组中间语种，增加其丰富性。</p><h2><span id="sheng-cheng">生成</span><a href="#sheng-cheng" class="header-anchor"></a></h2><p>生成的方式即通过样本构建一个生成模型，生成与样本语义相同的句子。如<a href="https://arxiv.org/abs/1906.06045" target="_blank" rel="noopener">Learning to Ask Unanswerable Questions for Machine Reading Comprehension </a>就是通过生成新的问题来做SQuAD2.0. 此外，之前的文章<a href="https://xv44586.github.io/2020/08/22/qa-augmentation/">利用NLG 增强QA 任务性能</a>里也总结了通过生成问题及问题答案对来增强qa模型性能，不熟悉的可以翻看一下。</p><p>由于两种方式构造的新句子都是与原句子语义相同的句子，所以，这种方式进行数据增强表达模型偏好是：模型应对于不同表达形式的同一语义的文本具有不变性。</p><h1><span id="ju-bu-rao-dong">局部扰动</span><a href="#ju-bu-rao-dong" class="header-anchor"></a></h1><p>局部扰动主要包括同义词替换、插入、删除、互换四种操作，出自论文<a href="http://arxiv.org/abs/1901.11196" target="_blank" rel="noopener">EDA: Easy Data Augmentation Techniques for Boosting Performance on<br>Text Classification Tasks</a>,因为操作简单，所以也叫<code>EDA</code>（Easy Data Augmentation)。下面分别介绍一下这四种策略。</p><h2><span id="tong-yi-ci-ti-huan">同义词替换</span><a href="#tong-yi-ci-ti-huan" class="header-anchor"></a></h2><p>从句子中随机找出1个非停用词，并求出其同义词，然后用同义词替换该词，重复n次操作</p><h2><span id="cha-ru">插入</span><a href="#cha-ru" class="header-anchor"></a></h2><p>从句子中随机找出一个非停用词，并求出其同义词，然后将同义词插入句子中的一个随机位置，重复n次操作。</p><h2><span id="shan-chu">删除</span><a href="#shan-chu" class="header-anchor"></a></h2><p>以概率p，随机删除句子中的每一个单词</p><h2><span id="hu-huan">互换</span><a href="#hu-huan" class="header-anchor"></a></h2><p>随机选择句子中的两个词，然后互换其位置，重复n次。<br>此外，论文中给出了替换删除等操作的比例$\alpha$ 与新增句子数量$n$ 的建议值：<br><img src="/2020/11/10/eda/eda.png" alt></p><p>而现在我们通常都使用bert等transformer模型做下游任务，所以删除操作可以使用padding，即删除token但保留其占位，即保留其位置编码；互换操作可以选择更大的span进行；插入和同义词替换操作也可以尝试从当前句子选择一个词代替同义词等。<br>文章开头我们也提到了，对句子中的词进行改变时，很可能得到语义完全相反的句子，那上面这四种方式为何有效呢？首先，句子中引入的新词都是同义词，所以语义不会发生很大的变换，其次，论文作者通过分析发现，虽然构造的新句子变得可能都不是一个通顺的句子了，但其特征空间分布下的label并没有发散，即经过EDA变换后，原始数据一方面引入了很多噪声，扩大了数据集，同时又保持了原有的标签，因而有效的扩大了样本集的信息容量。<br>此外，上面的方式相当于对模型增加了一个正则约束，其所表达的模型偏好是：模型应该对文本的局部噪声不敏感</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>以上就是当前NLP中常用的几种数据增强方案，尤其在样本不均衡及小样本任务下，数据增强往往能带来非常不错的提升。所以值得尝试。</p><div class="post__prevs"><div class="post__prev"><a href="/2020/11/08/ccf-qa/" title="ccf问答匹配比赛"><i class="iconfont icon-prev"></i>ccf问答匹配比赛</a></div><div class="post__prev post__prev--right"><a href="/2020/11/21/ad-dti/" title="跨界之阿尔滋海默病的分类竞赛">跨界之阿尔滋海默病的分类竞赛<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">29</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2022/09/08/tokenizers/" title="tokenizers 总结"><div class="item__cover"><img src="/2022/09/08/tokenizers/cannot_code.PNG" alt="tokenizers 总结"></div><div class="item__info"><h3 class="item__title">tokenizers 总结</h3><span class="item__text">2022-09-08</span></div></a></li><li class="latest-post-item"><a href="/2022/07/06/horovod-multi-nodes/" title="训练加速篇（3）horovod之多机多卡"><div class="item__cover"><img src="/2022/07/06/horovod-multi-nodes/cat.JPG" alt="训练加速篇（3）horovod之多机多卡"></div><div class="item__info"><h3 class="item__title">训练加速篇（3）horovod之多机多卡</h3><span class="item__text">2022-07-06</span></div></a></li><li class="latest-post-item"><a href="/2022/07/05/nohup-debug/" title="nohup踩坑记"><div class="item__cover"><img src="/2022/07/05/nohup-debug/dl.jpeg" alt="nohup踩坑记"></div><div class="item__info"><h3 class="item__title">nohup踩坑记</h3><span class="item__text">2022-07-05</span></div></a></li><li class="latest-post-item"><a href="/2022/05/25/horovod/" title="训练加速篇（2）-horovod"><div class="item__cover"><img src="/2022/05/25/horovod/cat.JPG" alt="训练加速篇（2）-horovod"></div><div class="item__info"><h3 class="item__title">训练加速篇（2）-horovod</h3><span class="item__text">2022-05-25</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/BPE/">BPE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/T5/">T5</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Unigram/">Unigram</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordPiece/">WordPiece</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/debug/">debug</a></li><li class="tag-item"><a class="tag-link" href="/tags/faster-decoder/">faster decoder</a></li><li class="tag-item"><a class="tag-link" href="/tags/horovod/">horovod</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/nohup/">nohup</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/simbert/">simbert</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/speed-up/">speed-up</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>