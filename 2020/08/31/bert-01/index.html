<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Knowledge Distillation (2) &#58; 知识迁移 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2020/08/31/bert-01/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2020/08/31/bert-01/bert.jpg" alt="Knowledge Distillation (2) &#58; 知识迁移"></div><header class="post__info"><h1 class="post__title">Knowledge Distillation (2) &#58; 知识迁移</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2020-08-31</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/BERT/">BERT</a></li><li class="mark__item"><a href="/tags/Distillation/">Distillation</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#distilling-knowledge">Distilling Knowledge</a><ul><li><a href="#distilling-the-knowledge-in-a-neural-network">Distilling the Knowledge in a Neural Network</a><ul><li><a href="#distilling">Distilling</a></li></ul></li><li><a href="#distill-bert">Distill BERT</a></li><li><a href="#tinybert">TinyBERT</a><ul><li><a href="#you-dian">优点</a></li><li><a href="#que-dian">缺点</a></li></ul></li><li><a href="#distilbert">DistilBERT</a><ul><li><a href="#you-dian-1">优点</a></li></ul></li><li><a href="#mobilebert">MobileBERT</a><ul><li><a href="#you-dian-2">优点</a></li><li><a href="#que-dian-1">缺点</a></li></ul></li></ul></li><li><a href="#lun-wen-zong-jie">论文总结</a></li><li><a href="#xiang-fa">想法</a></li><li><a href="#shi-yan">实验</a><ul><li><a href="#teacher-to-student">Teacher-to-Student</a></li><li><a href="#student-to-student">student-to-student</a></li><li><a href="#normal-noise-training">normal-noise-training</a></li></ul></li><li><a href="#shi-yan-jie-guo">实验结果</a></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><p>上篇讨论了<a href="https://xv44586.github.io/2020/08/31/bert-01/">bert-of-theseus</a>，算是一个开篇，本文继续讨论关于模型蒸馏（Distilling Knowledge）及关于BERT模型的知识蒸馏。<br>模型蒸馏的最重要的一个特点就是降低资源使用和加速模型推理速度，而小模型往往性能较低，本文总结一些如何通过蒸馏来使小模型具有更好的性能。</p><h1><span id="distilling-knowledge">Distilling Knowledge</span><a href="#distilling-knowledge" class="header-anchor"></a></h1><h2><span id="distilling-the-knowledge-in-a-neural-network">Distilling the Knowledge in a Neural Network</span><a href="#distilling-the-knowledge-in-a-neural-network" class="header-anchor"></a></h2><p>这篇是2015年Hinton发表的,也是我看到的最早提出Knowledge Distillation的<a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">论文</a>。<br>在这篇论文中，Hinton指出one-hot 的label只指示了true label 的信息，但是没有给出negative label 之间、negative 与 true label之间<br>的相对关系，比如：比如现在的任务是给定一个词（比如：苹果），然后判断词对应的类别（电视/手机/水果/汽车），假如现在我们有两个样本：<br>（苹果，[0,0,1,0]）和 （小米，[0,1,0,0]), 而one-hot 形式的label并不能告诉我们，苹果中 label是水果的概率高出label是拖拉机的概率，<br>稍低于是手机的概率，而小米中label是电视的概率稍低于是手机的概率，但是同时要高于是汽车和水果的概率，这些相对关系在one-hot 形式的label中<br>是无法得到的，而这些信息非常重要，有了这些信息，我们可以更容易的学习任务。于是提出了Teacher-Student模式，<br>即用一个大的复杂的模型（也可以是ensemble后的）来先学习，然后得到label的相对关系（logits），然后将学习到的知识迁移到一个小模型（Student）。</p><h3><span id="distilling">Distilling</span><a href="#distilling" class="header-anchor"></a></h3><p>具体迁移过程是Student 在进行training 时，除了学习ground truth 外，还需要学习label 的probability（softmax output），但是不是直接学习<br>softmax output，而是学习<code>soften labels</code>，所谓soften labels 即经过<code>Temperature</code> 平滑后的 probability，具体形式：<br>$$<br>q_{i} = \frac{exp(z_{i}/T)}{\sum_{j}^{}exp(z_{j}/T)}<br>$$<br>其中T 越大，对应的probability 越平滑，如下图所示。而平滑probability 可以看作是对soften label的一种正则化手段。<br><img src="/2020/08/31/bert-01/soften.png" alt></p><p>更直观的实验请查阅<a href="https://github.com/xv44586/Knowledge-Distillation-NLP/blob/master/Knowledge_Distillation_From_Scratch.ipynb" target="_blank" rel="noopener">Knowledge Distillation From Scratch</a></p><h2><span id="distill-bert">Distill BERT</span><a href="#distill-bert" class="header-anchor"></a></h2><p>看到的第一篇针对BERT 模型做蒸馏的是<a href="http://arxiv.org/abs/1903.12136" target="_blank" rel="noopener">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a>,<br>在这篇论文中，作者延续Hinton 的思路在BERT 上做实验，首先用BERT-12 做Teacher，然后用一个单层Bi-LSTM 做Student，loss 上除了<br>ground truth 外，也选择了使用teacher 的logits，包括Temperature 平滑后的soften labels 的CrossEntropy和 logits 之间的MSE，<br>最后实验验证MSE效果优于CE。<br>此外，由于是从头开始训练Student，所以只用任务相关数据会严重样本不足，所以作者提出了三种NLP的任务无关的data augment策略：</p><ol><li><strong>mask：随机mask一部分token作为新样本，让teacher去生成对应logits ;</strong></li><li><strong>根据POS标签去替换，得到 ”What do pigs eat?” -&gt; “ How do pigs ear?”</strong></li><li><strong>n-gram采样：随机选取n-gram，n取[1-5]，丢弃其余部分。</strong></li></ol><p>在<a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>中曾指出 logits 之间的CrossEntropy是可以看作<br>是MSE 的近似版本，不过这里作者的结论是MSE 更好，此外，由于Hinton 实验时是巨大数据量，所以不存在样本不足的情况，而普通实验时都会遇到<br>迁移时训练样本不足，需要做数据增强的问题。</p><h2><span id="tinybert">TinyBERT</span><a href="#tinybert" class="header-anchor"></a></h2><p>TinyBERT 出自<a href="http://arxiv.org/abs/1909.10351" target="_blank" rel="noopener">TinyBERT: Distilling BERT for Natural Language Understanding</a>,由于Transformer 结构<br>在NLP 任务中的强大能力，作者选择用与BERT 同结构的方式做Student，此外，为了提高KD后模型性能，做了更细致的工作：</p><ol><li><strong>Student选择一个更窄更浅的transformer;</strong></li><li><strong>将KD也分为两个阶段：pre-train 和 fine-tuning，并且在两个阶段上都进行KD;</strong></li><li><strong>使用了更多的loss：Embedding之间的MSE，Attention Matrix中的logits之间的MSE，Hidden state之间的MSE以及最后的分类层的CE;</strong></li><li><strong>为了提高下游任务fine-tuning后的性能，使用了近义词替换的策略进行数据增强.</strong></li></ol><h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><ol><li><strong>6层transformer基本达到了bert-12的性能，并且hidden size更小，实际是比bert-6更小的;</strong></li><li><strong>因为有pre-train KD，所以可以拿来当bert 一样直接在下游fine-tuning.</strong></li></ol><h3><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h3><ol><li><strong>由于hidden size的不同，所以为了进行MSE，需要用一个参数矩阵W 来调节，这个参数只在训练时使用，训练完后丢弃，这个矩阵没有任何约束，觉得不优雅;</strong></li><li><strong>其次，student model的每一层都需要去学习teacher model的对应的block的输出，如何对不同的层如何设计更好的权重也是一个费力的事；</strong></li><li><strong>虽然student的结构也是transformer，但是由于hidden size 不同，没法使用teacher的预训练结果，但是我觉得这里其实可以用降维的方式用<br>teacher的预训练结果，可能不需要pretraining的阶段了也说不定。</strong></li></ol><h2><span id="distilbert">DistilBERT</span><a href="#distilbert" class="header-anchor"></a></h2><p>DistilBERT 出自<a href="http://arxiv.org/abs/1910.01108" target="_blank" rel="noopener">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>,<br>论文中作者通过调查发现BERT 中的hidden size 的对计算效率的改变比hidden layer nums 的影响小，说白了就是让模型变矮比让模型变瘦效率更高，<br>所以作者使用了一个更矮的BERT来做Student 来迁移BERT 中的知识。由于DistilBERT 是一个与BERT 同结构只是层数更小，所以DistilBERT 可以用<br>BERT 的预训练的权重进行初始化，此外，DistilBERT 是一个与任务无关的模型，即与BERT 一样，可以对很多下游任务进行fine-tuning。<br>由于DistilBERT 与 BERT 的前几层一致，所以loss 的选择上就更多一些，作者选择了triple loss：<br>MLM loss + embedding cosin loss + soften labels cross entropy .s</p><h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><p>DistilBERT 做到了与BERT 一样，完全与任务无关，不需要添加额外的Distillation 阶段（添加后结果会更好)。</p><h2><span id="mobilebert">MobileBERT</span><a href="#mobilebert" class="header-anchor"></a></h2><p>MobileBERT 出自<a href="http://arxiv.org/abs/2004.02984" target="_blank" rel="noopener">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a>,<br>作者同样采用一个transformer 作为基本结构，但作者认为深度很重要，宽度较小对模型损坏较小，所以整体架构是保持模型深度不变，<br>通过一个矩阵来改变feature size，即bottleneck，在通过在block的前后插入两个bottleneck，来scale feature size。由于<br>MobileBERT太窄太深，所以不好训练，作者提出新的方式，通过一个同深但是更宽的同架构的模型来训练 作为teacher，然后用MobileBERT迁移。<br>loss 设计上主要包括三部分：feature map之间的MSE，Attention logits之间的KL，以及pre-training MLM + pre-training-NSP + pre-training-KD<br>训练策略上，有三种方式：</p><ol><li><strong>将KD作为附加预训练的附加任务，即一起训练；</strong></li><li><strong>分层训练，每次训练一层，同时冻结之前的层；</strong></li><li><strong>分开训练，首先训练迁移，然后单独进行pre-training.</strong></li></ol><p>此外，为了提高推理速度，将gelu 替换为更快的 relu ，LayerNormalization 替换为 更简单的NoNorm，也做了量化的实验。</p><h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><ol><li><strong>首先mobileBERT容量更小，推理更快，与任务无关，可以当bert来直接在下游fine-tuning，而之前的KD大多数时候需要与任务绑定并使用数据<br>增强，才能达到不错的性能；</strong></li><li><strong>论文实验非常详实，包括如何选择inter-block size, intra-block size, 不同训练策略如何影响等;</strong></li><li><strong>训练策略上，除了之前的一起训练完，实验了两种新的训练方式，而最终的一层一层的训练与skip connection 有异曲同工的作用：每层都学一小部分<br>内容，从而降低学习的难度；</strong></li><li><strong>替换了gelu 和 LayerNormalization,进一步提速.</strong></li></ol><h3><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h3><ol><li>要训练一个IBBERT作为teacher，而这个模型容量与BERT-Large差不多，增加了训练难度.</li></ol><h1><span id="lun-wen-zong-jie">论文总结</span><a href="#lun-wen-zong-jie" class="header-anchor"></a></h1><p>以上论文的迁移过程其实可以总结为两类：</p><ol><li><strong>soft label迁移，即主要迁移Teacher 模型最后分类层的logits 及相应的soft label；</strong></li><li><strong>feature迁移，即除了最后分类层外，还迁移Teacher 模型中的output/attention/embedding等特征。</strong></li></ol><p>Student 的选择上，除了自定义外，还可以选择跟Teacher 同结构，而为了降低参数量，可以选择将模型变矮/变窄/减小hidden size 等方式。<br>而为了蒸馏后的模型能更加的general，适应更多的task，就需要迁移更多的信息，设计上也越复杂。</p><h1><span id="xiang-fa">想法</span><a href="#xiang-fa" class="header-anchor"></a></h1><p>实际工作上，大多数时候我们都是需要一个task 来做模型，而以上论文中告诉我们，迁移的信息越多，Student 的性能越好。而针对具体task ，我觉得<br>比较简洁有效的一种方式是采用更矮的Teacher 来作为Student ，这样可以直接将Teacher 中的前几层的信息完全迁移过来，然后在object 上，<br>加入迁移Teacher 在train data 上的logits ，这样就可以比较有效的进行蒸馏了。<br>除此之外，让我们换个角度看看为什么logits 能增强Student 模型的性能呢？除了迁移的角度外，其实logits 提供了label<br>更多的信息（不同类别的相对关系），而这个额外信息只要优于随机分布，就能对模型提供更多的约束信息，从而增强模型性能，即当前的模型可以看<br>作是分别拟合ground truth 和 logits的两个模型的<code>ensemble</code>，只不过是两个模型共享参数。<br>上面我们提到只要logits 优于随机，对Student 模型来说就会有所提升，那logits 由谁产生的其实并不重要。所以，我们除了可以用Teacher 产生的<br>logits来增强Student 模型外，我们还可以增强Teacher 模型，或者直接用Student 先学习一下，产生logits，再用Student 去迁移上次产生的logits。<br>想到这里，我不禁的有个大胆的想法：<code>既然我可以一边生成logits， 一边学习logits，那我不是可以持续这个过程，直到模型完全拟合train data，<br>生成的logits退化为one-hot，那此时的模型是不是能得到一个非常大的提升呢？</code></p><h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>实验的基本设置是用12层bert 作为Teacher model ，用3层bert 作为Student model 。soften labels 采用Temperature 平滑后的结果，此外，<br>Student model 除了学习 soften labels 的外，也需要学习ground truth。</p><h2><span id="teacher-to-student">Teacher-to-Student</span><a href="#teacher-to-student" class="header-anchor"></a></h2><p>Teacher model 在train data 上训练，然后在train data 上生成对应的soften labels，Student model 学习ground truth 和 soften labels。</p><h2><span id="student-to-student">student-to-student</span><a href="#student-to-student" class="header-anchor"></a></h2><p>既然soften labels 是一种对labels 的一种平滑估计，那我们可以用任何方式去估计他，所以这里我们就用student 去做一个估计：<br>student model 在train data 上进行训练，然后在train data 上生成对应的soften labels ，将 student model 利用bert 预训练结果重新初始化，<br>然后去学习ground truth 和 soften labels.</p><h2><span id="normal-noise-training">normal-noise-training</span><a href="#normal-noise-training" class="header-anchor"></a></h2><p>既然是对labels 的一个估计，那假如给一个随机的估计，只要保证生成的logits 中true label 对应的值最大，就能对Student 模型进行一定程度的提升：<br>直接在train label 上添加一个normal noise ，然后重新进行平滑后归一，作为soften labels让student model 去学习。</p><h1><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h1><p>$$<br>\begin{array}{c|c|c}<br>\hline \\<br>\text{teacher standalone} &amp; \text{student standalone} &amp; \text{teacher-to-student} &amp; \text{teacher-to-teacher} &amp; \text{student-to-student} &amp; \text{normal-noise-student}\\<br>\hline \\<br>60.21\% &amp; 58.14\% &amp; 60.14\% &amp; 60.14\% &amp; 61.07\% &amp; 59.5\%<br>\end{array}<br>$$</p><p>从结果中可以看到：</p><ol><li><strong>优于随机的logits 对Student 模型有一定的提升，估计越准确，提升越高；</strong></li><li><strong>越大的模型性能越好;</strong><br>3.<strong>迭代进行logits 的生成与训练不能进一步提高模型性能，原因主要是新的logits 分布相比之前的对模型的提升非常小，此外这个分布也比较容易拟<br>合，所以无法进一步提升。</strong></li></ol><p>完整实验代码地址<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/distilling_knowledge_bert.py" target="_blank" rel="noopener">distilling_knowledge_bert</a></p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文主要针对目前针对BERT 的知识蒸馏进行了总结，并提出了针对具体任务时可行的简洁方案，同时在新的视角下探讨了知识蒸馏有效的一些原因，<br>并通过实验进行了验证，发表顺序上上篇<a href="https://xv44586.github.io/2020/08/09/bert-of-theseus/">bert-of-theseus</a> 更晚一些，有兴趣的可以再去看一下上一篇。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>芝麻街</p><div class="content-footer-sponsor"><h1><span id="sponsor">Buy me a coffee</span></h1><p>如果觉得这篇文章不错，对你有帮助，欢迎打赏一杯蜜雪冰城。</p><img src="/img/sponsor.JPG" alt="logo" title="sponsor"></div><div class="post__prevs"><div class="post__prev"><a href="/2020/08/22/qa-augmentation/" title="模型增强（1）&#58; 利用NLG 增强QA 任务性能"><i class="iconfont icon-prev"></i>模型增强（1）&#58; 利用NLG 增强QA 任务性能</a></div><div class="post__prev post__prev--right"><a href="/2020/09/13/classification-label-augment/" title="模型增强（2）&#58; 从label下手">模型增强（2）&#58; 从label下手<i class="iconfont icon-next"></i></a></div></div></div></article><script src="https://giscus.app/client.js" data-repo="xv44586/giscus" data-repo-id="R_kgDOIC6Ipg" data-category="Announcements" data-category-id="DIC_kwDOIC6Ips4CRkmo" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="zh-CN" crossorigin="anonymous" async></script></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">33</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2023/03/25/gpt4/" title="GPT-4 yes!! but"><div class="item__cover"><img src="/2023/03/25/gpt4/bg.jpeg" alt="GPT-4 yes!! but"></div><div class="item__info"><h3 class="item__title">GPT-4 yes!! but</h3><span class="item__text">2023-03-25</span></div></a></li><li class="latest-post-item"><a href="/2023/03/10/llm-inf/" title="LLM Inference串讲"><div class="item__cover"><img src="/2023/03/10/llm-inf/sd.PNG" alt="LLM Inference串讲"></div><div class="item__info"><h3 class="item__title">LLM Inference串讲</h3><span class="item__text">2023-03-10</span></div></a></li><li class="latest-post-item"><a href="/2023/02/01/fine-tuning-at-few-shot/" title="few-shot视角下的fine-tuning"><div class="item__cover"><img src="/2023/02/01/fine-tuning-at-few-shot/himalayas.JPG" alt="few-shot视角下的fine-tuning"></div><div class="item__info"><h3 class="item__title">few-shot视角下的fine-tuning</h3><span class="item__text">2023-02-01</span></div></a></li><li class="latest-post-item"><a href="/2023/01/09/zero-to-chatgpt/" title="From zero to ChatGPT"><div class="item__cover"><img src="/2023/01/09/zero-to-chatgpt/chatgpt-bg.jpeg" alt="From zero to ChatGPT"></div><div class="item__info"><h3 class="item__title">From zero to ChatGPT</h3><span class="item__text">2023-01-09</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/BPE/">BPE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/ChatGPT/">ChatGPT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-3/">GPT-3</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-4/">GPT-4</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/In-context-learning/">In-context learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Inference/">Inference</a></li><li class="tag-item"><a class="tag-link" href="/tags/LLM/">LLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/T5/">T5</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Unigram/">Unigram</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordPiece/">WordPiece</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/debug/">debug</a></li><li class="tag-item"><a class="tag-link" href="/tags/faster-decoder/">faster decoder</a></li><li class="tag-item"><a class="tag-link" href="/tags/fine-tuning/">fine-tuning</a></li><li class="tag-item"><a class="tag-link" href="/tags/horovod/">horovod</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/nohup/">nohup</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/simbert/">simbert</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/speed-up/">speed-up</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>