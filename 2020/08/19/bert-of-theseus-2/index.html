<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-下篇 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2020/08/19/bert-of-theseus-2/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2020/08/09/bert-of-theseus/theseus.png" alt="Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-下篇"></div><header class="post__info"><h1 class="post__title">Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-下篇</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2020-08-19</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/BERT/">BERT</a></li><li class="mark__item"><a href="/tags/Distillation/">Distillation</a></li><li class="mark__item"><a href="/tags/speed-up/">Speed-Up</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#fu-xian-shi-de-wen-ti">复现时的问题</a><ul><li><a href="#si-kao-1-wei-shi-me-shi-xiao">思考1：为什么失效</a></li><li><a href="#si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me">思考二 ：bert-of–theseus有效的本质是什么</a></li><li><a href="#shi-yan-1">实验1</a></li><li><a href="#si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing">思考三：直接在predecessor 上抽successor行不行？</a></li><li><a href="#shi-yan-2">实验2</a></li></ul></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><p>上一篇<a href="https://xv44586.github.io/2020/08/09/bert-of-theseus/">模块替换之bert-of-theseus-上篇</a>中介绍了bert-of-theseus论文的主要思路，并贴了两组实验的结论，这篇是对上篇的后续一些思考与实验。</p><h1><span id="fu-xian-shi-de-wen-ti">复现时的问题</span><a href="#fu-xian-shi-de-wen-ti" class="header-anchor"></a></h1><p>在复现时，遇到最大的问题就是结果不稳定。首先每次训练predecessor时，其最优结果就会有上下1个点左右的波动，而因为theseus 中引入了随机数<br>来概率替换对应block，所以结果上一言难尽，有时能比12层bert低0.6个点, 有时只能达到直接3层fine tuning 的效果，于是我做了些观察与思考。</p><h2><span id="si-kao-1-wei-shi-me-shi-xiao">思考1：为什么失效</span><a href="#si-kao-1-wei-shi-me-shi-xiao" class="header-anchor"></a></h2><p>在训练theseus model时，其中抽出的successor在每个epoch结束后在验证集上的结果有时会很高，基本到达只比三层fine-tuning低6个点，有时又很<br>低，基本不到0.1%, 第一种明显是successor在theseus中训练太多，以至于接近直接fine tuning，而另一种情况下可能是successor训练不充足，<br>也可能是替换次数太少导致没有被训练，而且大多数情况下successor的验证集上都是不到0.1%。<br>为了验证第二种情况下是否是未替换导致successor在做fine tuning，我将successor进行单独fine tuning后,将得到的classifier 拼回predecessor，<br>发现此时在验证集上d结果只下降了2个点，所以此时大概率是替换次数过少，基本没有训练到successor，所以导致结果不好，而这里开始我以为是我<br>实现问题，后来来来回回检查了一周，也没发现问题，于是我就想换一种更稳定的方式。</p><h2><span id="si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me">思考二 ：bert-of–theseus有效的本质是什么</span><a href="#si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me" class="header-anchor"></a></h2><p>熟悉bert的同学肯定对warm up不陌生，而warm up之所以有效，我认为比较重要的一点是如果在最初的steps中，模型提前拟合了样本，进入了一个局部最优区域，后期无论你怎么迭代他都跳不出来，而由已经<code>fine tuned predecessor</code>带着一起再进行训练，也和warm up有些相似，即用小的<br>步子带着你朝着更优的方向走几步，跳出来，让你有进入更好的局部最优点的可能，此外，概率替换的思路也与<code>Dropout</code>有几分相似，让successor<br>有一定的几率参与训练，从而让successor在缺少predecessor的情况下也有一定的robust。<br><a href="https://spaces.ac.cn/archives/7575" target="_blank" rel="noopener">苏剑林的博客</a>里也提到了替换的数学形式：<br>$$<br>\begin{equation}\begin{aligned}<br>&amp;\varepsilon^{(l)}\sim U(\{0, 1\})\\<br>&amp;x^{(l)} = x_p^{(l)} \times \varepsilon^{(l)} + x_s^{(l)} \times \left(1 - \varepsilon^{(l)}\right)\\<br>&amp;x_p^{(l+1)} = F_p^{(l+1)}\left(x^{(l)}\right)\\<br>&amp;x_s^{(l+1)} = F_s^{(l+1)}\left(x^{(l)}\right)<br>\end{aligned}\end{equation}<br>$$<br>同时，他也提到$\epsilon$能否不取非0即1，那既然我们是想让successor在task方向上warm up一下，那直接相加，即此时 $\epsilon = k$,<br>k是常数也是可以的。此时只要调节k 就能避免successor训练不充分或太充分的情况了，模型也就稳定了，可以满足我们的要求了。</p><h2><span id="shi-yan-1">实验1</span><a href="#shi-yan-1" class="header-anchor"></a></h2><p>实验代码其实比较容易修改，只需将BinaryRandomChoice 层替换为相加即可。具体代码在<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py" target="_blank" rel="noopener">classification_ifytek_bert_of_theseus</a><br>中可以看到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProportionalAdd</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""将两层的结果乘比例后相加，output = (input_1 * proportion + input_2 * (1 - proportion)) / 2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, proportion=<span class="number">0.5</span>, **kwargs)</span>:</span></span><br><span class="line">        super(ProportionalAdd, self).__init__(**kwargs)</span><br><span class="line">        self.supports_masking = <span class="literal">True</span></span><br><span class="line">        self.proportion = proportion</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_mask</span><span class="params">(self, inputs, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> mask[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        source, target = inputs</span><br><span class="line">        source = source * self.proportion</span><br><span class="line">        target = target * (<span class="number">1</span> - self.proportion)</span><br><span class="line">        output = (source + target)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> K.in_train_phase(output, target)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> input_shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>文本分类：CLUE的iflytek数据集</p><p>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 60.11\% &amp; 58.99\% &amp; 57.96\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 59.7\% &amp; 59.5\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p><p>结果上看确实更稳定了，也更好一点点了，基本比predecessor低<code>0.5%~1%</code> .</p><h2><span id="si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing">思考三：直接在predecessor 上抽successor行不行？</span><a href="#si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing" class="header-anchor"></a></h2><p>既然我们说bert-of-theseus有效的原因是在task 的方向进行了warm up，那predecessor已经在task上fine tuned了，能不能<code>直接抽取某几<br>层作为successor来直接fine tuning?</code>此外，之前我们也说了，predecessor与successor的classifer差距很小，那我们能不能改变successor<br>的classifer的学习率，让他进一步学习，来弥补一部分前三层无法拟合的分布呢？</p><h2><span id="shi-yan-2">实验2</span><a href="#shi-yan-2" class="header-anchor"></a></h2><p>具体实验代码<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/two_stage_fine_tuning.py" target="_blank" rel="noopener">two-stage-fine-tuning</a><br>实验时尝试了<code>随机初始化classifier/predecessor classifier初始化classifier/ 放大classifier lr</code>组合策略，最后的结果就不贴了，基本都没有<br>超过3层bert fine tuning的效果。</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>尝试分析了bert-of-theseus复现中的问题，并尝试了一些修复方案，同时，实验测试了theseus model的必要性，最后结论是binary random choice<br>策略不如 proportion add 策略稳定，同时，theseus是必须的。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://github.com/JetRunner/BERT-of-Theseus" target="_blank" rel="noopener">论文原作者配图</a></p><div class="content-footer-sponsor"><h1><span id="sponsor">Buy me a coffee</span></h1><p>如果觉得这篇文章不错，对你有帮助，欢迎打赏一杯蜜雪冰城。</p><img src="/img/sponsor.JPG" alt="logo" title="sponsor"></div><div class="post__prevs"><div class="post__prev"><a href="/2020/08/09/bert-of-theseus/" title="Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-上篇"><i class="iconfont icon-prev"></i>Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-上篇</a></div><div class="post__prev post__prev--right"><a href="/2020/08/22/qa-augmentation/" title="模型增强（1）&#58; 利用NLG 增强QA 任务性能">模型增强（1）&#58; 利用NLG 增强QA 任务性能<i class="iconfont icon-next"></i></a></div></div></div></article><script src="https://giscus.app/client.js" data-repo="xv44586/giscus" data-repo-id="R_kgDOIC6Ipg" data-category="Announcements" data-category-id="DIC_kwDOIC6Ips4CRkmo" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="zh-CN" crossorigin="anonymous" async></script></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">33</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2023/03/25/gpt4/" title="GPT-4 yes!! but"><div class="item__cover"><img src="/2023/03/25/gpt4/bg.jpeg" alt="GPT-4 yes!! but"></div><div class="item__info"><h3 class="item__title">GPT-4 yes!! but</h3><span class="item__text">2023-03-25</span></div></a></li><li class="latest-post-item"><a href="/2023/03/10/llm-inf/" title="LLM Inference串讲"><div class="item__cover"><img src="/2023/03/10/llm-inf/sd.PNG" alt="LLM Inference串讲"></div><div class="item__info"><h3 class="item__title">LLM Inference串讲</h3><span class="item__text">2023-03-10</span></div></a></li><li class="latest-post-item"><a href="/2023/02/01/fine-tuning-at-few-shot/" title="few-shot视角下的fine-tuning"><div class="item__cover"><img src="/2023/02/01/fine-tuning-at-few-shot/himalayas.JPG" alt="few-shot视角下的fine-tuning"></div><div class="item__info"><h3 class="item__title">few-shot视角下的fine-tuning</h3><span class="item__text">2023-02-01</span></div></a></li><li class="latest-post-item"><a href="/2023/01/09/zero-to-chatgpt/" title="From zero to ChatGPT"><div class="item__cover"><img src="/2023/01/09/zero-to-chatgpt/chatgpt-bg.jpeg" alt="From zero to ChatGPT"></div><div class="item__info"><h3 class="item__title">From zero to ChatGPT</h3><span class="item__text">2023-01-09</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/BPE/">BPE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/ChatGPT/">ChatGPT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-3/">GPT-3</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-4/">GPT-4</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/In-context-learning/">In-context learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Inference/">Inference</a></li><li class="tag-item"><a class="tag-link" href="/tags/LLM/">LLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/T5/">T5</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Unigram/">Unigram</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordPiece/">WordPiece</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/debug/">debug</a></li><li class="tag-item"><a class="tag-link" href="/tags/faster-decoder/">faster decoder</a></li><li class="tag-item"><a class="tag-link" href="/tags/fine-tuning/">fine-tuning</a></li><li class="tag-item"><a class="tag-link" href="/tags/horovod/">horovod</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/nohup/">nohup</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/simbert/">simbert</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/speed-up/">speed-up</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>