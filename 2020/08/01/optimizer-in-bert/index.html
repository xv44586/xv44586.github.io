<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>optimizer of bert | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2020/08/01/optimizer-in-bert/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2020/08/01/optimizer-in-bert/head.jpeg" alt="optimizer of bert"></div><header class="post__info"><h1 class="post__title">optimizer of bert</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2020-08-01</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/BERT/">BERT</a></li><li class="mark__item"><a href="/tags/Optimizer/">Optimizer</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#zheng-ti-you-hua-fang-an">整体优化方案</a></li><li><a href="#adam-in-bert">Adam in bert</a><ul><li><a href="#weight-decay">weight decay</a><ul><li><a href="#weight-decay-1">weight decay</a></li><li><a href="#l2-regularization">L2 regularization</a></li></ul></li></ul></li><li><a href="#learning-rate">Learning rate</a><ul><li><a href="#learning-rate-decay">Learning rate decay</a></li><li><a href="#warmup">warmup</a></li></ul></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><p>最近尝试实现了 bert ,在最后 pretraining 时发现 bert 中的优化方法比较有趣，所以记录一下自己的理解。</p><h1><span id="zheng-ti-you-hua-fang-an">整体优化方案</span><a href="#zheng-ti-you-hua-fang-an" class="header-anchor"></a></h1><p>bert中的优化方案可以总结为：线性分段学习率 + weight decay Adam</p><h1><span id="adam-in-bert">Adam in bert</span><a href="#adam-in-bert" class="header-anchor"></a></h1><p>首先简单回忆一下 Adam Optimizer：<br>整体框架：<br>$$<br>g_{t}=\bigtriangledown f(w_{t})<br>$$<br>$$<br>m_{t}=\Phi (g_{1},g_{2},…,g_{t})<br>$$<br>$$<br>v_{t}=\Psi (g_{1},g_{2},…,g_{t})<br>$$<br>$$<br>\eta =\alpha \cdot m_{t}/\sqrt{V_{t}}<br>$$<br>$$<br>\omega_{t+1}=\omega_{t}-\eta_{t}<br>$$</p><p>其中一阶动量 m 与二阶动量 v 的计算方式：<br>$$<br>m_{t}=\beta_{1}m_{t-1} + (1-\beta_{1})\cdot g_{t}<br>$$<br>$$<br>v_{t}=\beta_{2}v_{t-1} + (1-\beta_{2})\cdot g_{t}^{2}<br>$$<br>参数一般取值：ß1=0.9，ß2=0.999<br>而也是这个原因，初期对一阶动量与二阶动量v的估算都偏小，会导致优化方向朝着 0 走，所以，一般会进行一个修正（bias correct），方式是：<br>$$<br>\hat{m_{t}}=m_{t}/1-{\beta_{1}}^{t}<br>$$<br>$$<br>\hat{v_{t}}=v_{t}/1-{\beta_{2}}^{t}<br>$$<br>而 bert 中实现的 Adam 却没有进行这个修正，至于原因，放在下面一起说。</p><h2><span id="weight-decay">weight decay</span><a href="#weight-decay" class="header-anchor"></a></h2><p>在 bert 中对 Adam 进行了weight decay，具体代码上是这一段：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Just adding the square of the weights to the loss function is *not*</span></span><br><span class="line"><span class="comment"># the correct way of using L2 regularization/weight decay with Adam,</span></span><br><span class="line"><span class="comment"># since that will interact with the m and v parameters in strange ways.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Instead we want ot decay the weights in a manner that doesn't interact</span></span><br><span class="line"><span class="comment"># with the m/v parameters. This is equivalent to adding the square</span></span><br><span class="line"><span class="comment"># of the weights to the loss with plain (non-momentum) SGD.</span></span><br><span class="line"><span class="keyword">if</span> self._do_use_weight_decay(param_name):</span><br><span class="line">      update += self.weight_decay_rate * param</span><br></pre></td></tr></table></figure><p></p><p>这里讲到<code>直接将权重的平方加入到loss 上进行L2 regularization 在 Adam 上是一种错误到方式</code></p><h3><span id="weight-decay">weight decay</span><a href="#weight-decay" class="header-anchor"></a></h3><p>Weight decay是在每次更新的梯度基础上减去一个梯度</p><p>$$\theta_{t+1}=(1-\lambda )\theta_{t} -\alpha \bigtriangledown f_{t}(\theta_{t})$$</p><h3><span id="l2-regularization">L2 regularization</span><a href="#l2-regularization" class="header-anchor"></a></h3><p>L2 regularrization是在参数上加上L2惩罚</p><p>$$ f_{t}^{reg}(\theta)=f_{t}(\theta)+\frac{ {\lambda }’}{2}\left \| \theta\right \| _{2}^{2}$$</p><p>可以看出，在标准SGD下，两者是等价的<br>但是，在Adam下，两者却不是。我们将Adam下的梯度更新完整公式写出来：</p><p>$$ \theta_{t}\leftarrow \theta_{t-1} -\alpha \frac{\beta_{1}m_{t-1}+(1-\beta_{1})(\bigtriangledown f_{t}+\lambda \theta_{t-1})}{\sqrt{\hat{v_{t}}} + \varepsilon }$$</p><p>而与参数有关的是右上角的部分：<code>$\frac{\lambda \theta_{t-1}}{\sqrt{v_{t}}}$</code> 而这一项表明，在梯度变化越大的方向上，v的值也越大，但对应的权重约束却越小，这显然是不合理的，此外，L2 与 weight decay 都是各个方向同性的，<br>所以针对这一问题，一种调整方式是将梯度更新与weight decay 解偶，<br><img src="/2020/08/01/optimizer-in-bert/de.png" alt><br>具体参考<a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="noopener">DECOUPLED WEIGHT DECAY REGULARIZATION</a><br>而 bert 中也是使用了这种weight decay 方式，来达到与L2正则等效</p><h1><span id="learning-rate">Learning rate</span><a href="#learning-rate" class="header-anchor"></a></h1><h3><span id="learning-rate-decay">Learning rate decay</span><a href="#learning-rate-decay" class="header-anchor"></a></h3><p>通常，为了让模型在后期避免震荡，更加稳定，都会随着训练的进行，将learning rate 进行调整，即越是后期learning rate 越小。</p><h3><span id="warmup">warmup</span><a href="#warmup" class="header-anchor"></a></h3><p>而bert中的learning rate的调整是两段线性调整学习率：前 10% steps 将learning rate 从 0 增长到 init_learning_rate，然后，再一致递减 到0<br>而warmup为何有效？</p><ol><li>可以避免较早的对mini-batch过拟合，即较早的进入不好的局部最优而无法跳出；</li><li>保持模型深层的稳定性<br>具体可以参考<a href="https://www.zhihu.com/question/338066667/answer/771252708" target="_blank" rel="noopener">warmup 为什么有效</a></li></ol><p>此外，由于warmup要求前期保持较小的更新，所以Adam中由于前期会导致更新变小而需要进行的bias correct也可以去掉了。这也就是最初留下到那个问题到答案</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>bert在 pretraining 为了让模型收敛到一个较好的点，不但在优化器 Adam 上使用了与 L2 regularization等效的weight decay，为了避免模型前期过早拟合进入local minimal，使用了warmup 策略。<br>bert作者也建议在进行fine-tuning时，使用与bert源码中相同的优化器，我也做了一些实验，提升有大概不到0.5个点（没有细调），所以在下游任务上可以尝试使用。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于圆明园荷花池</p><div class="post__prevs"><div class="post__prev"><a href="/2020/05/05/make-a-computer/" title="装机指北"><i class="iconfont icon-prev"></i>装机指北</a></div><div class="post__prev post__prev--right"><a href="/2020/08/09/bert-of-theseus/" title="Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-上篇">Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-上篇<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">24</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2021/07/06/cl2rdrop/" title="对比学习心路历程"><div class="item__cover"><img src="/2021/07/06/cl2rdrop/ai.jpeg" alt="对比学习心路历程"></div><div class="item__info"><h3 class="item__title">对比学习心路历程</h3><span class="item__text">2021-07-06</span></div></a></li><li class="latest-post-item"><a href="/2021/03/28/multi-task/" title="多任务学习-以天池比赛为例的三种思路"><div class="item__cover"><img src="/2021/03/28/multi-task/bg.jpeg" alt="多任务学习-以天池比赛为例的三种思路"></div><div class="item__info"><h3 class="item__title">多任务学习-以天池比赛为例的三种思路</h3><span class="item__text">2021-03-28</span></div></a></li><li class="latest-post-item"><a href="/2021/02/19/happy-new-year/" title="辞旧迎新"><div class="item__cover"><img src="/2021/02/19/happy-new-year/white-whale.jpeg" alt="辞旧迎新"></div><div class="item__info"><h3 class="item__title">辞旧迎新</h3><span class="item__text">2021-02-19</span></div></a></li><li class="latest-post-item"><a href="/2021/01/20/ccf-qa-2/" title="ccf问答匹配比赛（下）：如何只用“bert”夺冠"><div class="item__cover"><img src="/2021/01/20/ccf-qa-2/head.png" alt="ccf问答匹配比赛（下）：如何只用“bert”夺冠"></div><div class="item__info"><h3 class="item__title">ccf问答匹配比赛（下）：如何只用“bert”夺冠</h3><span class="item__text">2021-01-20</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Contrastive-Learning/">Contrastive Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>