<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-上篇 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2020/08/09/bert-of-theseus/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2020/08/09/bert-of-theseus/theseus.png" alt="Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-上篇"></div><header class="post__info"><h1 class="post__title">Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-上篇</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2020-08-09</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/BERT/">BERT</a></li><li class="mark__item"><a href="/tags/Distillation/">Distillation</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#mo-xing-ya-suo">模型压缩</a><ul><li><a href="#jian-zhi">剪枝</a></li><li><a href="#liang-hua">量化</a></li><li><a href="#zhi-shi-zheng-liu">知识蒸馏</a></li><li><a href="#quan-chong-gong-xiang">权重共享</a></li><li><a href="#quan-chong-fen-jie">权重分解</a></li><li><a href="#mo-xing-ya-suo-de-bi-yao-xing">模型压缩的必要性</a></li><li><a href="#bert-of-theseus">Bert of theseus</a></li><li><a href="#ju-ti-liu-cheng">具体流程</a></li><li><a href="#shi-yan-xiao-guo">实验效果</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><blockquote>如果忒修斯的船上的木头被逐渐替换，直到所有的木头都不是原来的木头，那这艘船还是原来的那艘船吗？<br><br>-普鲁塔克</blockquote><p>最近遇到一个需要对算法加速的场景，了解到了一个比较简洁实用的方法：<a href="https://arxiv.org/abs/2002.02925" target="_blank" rel="noopener">Bert-of-theseus</a>,<br>了解了原理后参考代码实验后，验证了其有效性，所以总结一下。</p><h1><span id="mo-xing-ya-suo">模型压缩</span><a href="#mo-xing-ya-suo" class="header-anchor"></a></h1><p>模型在设计之初都是过参数化的，这是因为模型的参数量与复杂度代表着模型的容量与学习能力，但当我们实际使用时，我们需要更好的部署他（低资源），更快的响应（快速推理），常常需要进行模型压缩。<br>模型压缩就是<code>简化大的模型，得到推理快资源占用低的小模型</code>，而想”即要马而跑又不用吃草”通常是很难的，所以压缩后的模型常常也会有不同程度的牺牲，如模型性能下降。<br>此外，模型压缩是作用在推理阶段，带来的常常是训练时间的增加。<br>模型压缩又分为两种方式：一种是<code>剪枝(Pruning)</code>与<code>量化(Quantization)</code>,一种是<code>知识蒸馏(Knowledge Distillation)</code>,<br>还有一种是<code>权重共享（Sharing）与因数分解（Factorization）</code>。该部分内容推荐一篇博客：<a href="http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html" target="_blank" rel="noopener">All The Ways You Can Compress BERT</a></p><h2><span id="jian-zhi">剪枝</span><a href="#jian-zhi" class="header-anchor"></a></h2><p>剪枝技术是通过将大模型中一些”不重要”的连接剪断，得到一个”稀疏”结构的模型。剪枝又分为”结构性剪枝”与”非结构性剪枝”.剪枝可以作用在权重粒度，<br>也可以作用在attention heads / layer粒度上。不过剪枝技术感觉会逐步被<cdoe>NAS（Neural Architecture Search）取。</cdoe></p><h2><span id="liang-hua">量化</span><a href="#liang-hua" class="header-anchor"></a></h2><p>量化不改变模型的网络结构，而是改变模型的参数的数据格式，通常模型在建立与训练时使用的是 float32 格式的，量化就是将格式转换为 <code>low-bit</code>, 如 float16 甚至二值化，如此即提速又省显存。</p><h2><span id="zhi-shi-zheng-liu">知识蒸馏</span><a href="#zhi-shi-zheng-liu" class="header-anchor"></a></h2><p>知识蒸馏是训练一个小模型(student)来学习大模型(teacher)，由于大模型是之前已经fine-tuning的，所以此时学习的目标已经转换为对应的logit而<br>不再是one-hot编码了，所以student有可能比teacher的性能更好。这样即小又准的模型实在太好了。不过为了达到这样的效果，通常设计小模型时不<br>光要学习大模型的输出，还要学习各个中间层结果，相关矩阵等，这就需要仔细设计模型的结构与loss及loss融合方案了。一种简单的方法是只学习大模型的logit，这与对label做embedding有点类似，不过我没做过实验还。</p><h2><span id="quan-chong-gong-xiang">权重共享</span><a href="#quan-chong-gong-xiang" class="header-anchor"></a></h2><p>将部分权重在多个层中共享以达到压缩模型的效果，如ALBERT中共享self-attention中的参数</p><h2><span id="quan-chong-fen-jie">权重分解</span><a href="#quan-chong-fen-jie" class="header-anchor"></a></h2><p>将权重矩阵进行因数分解，形成两个低秩的矩阵相乘的形式，从而降低计算量</p><h2><span id="mo-xing-ya-suo-de-bi-yao-xing">模型压缩的必要性</span><a href="#mo-xing-ya-suo-de-bi-yao-xing" class="header-anchor"></a></h2><p>看了上面模型压缩的方法，每一个都有种”脱裤子放屁”的感觉，与其训练一个大模型，再费力把它变小，为何不直接开始就弄个小的呢？<br>首先，模型在设计之初是都是会或多或少的过参数化，因为模型的参数量与复杂度代表着模型的容量与学习能力；<br>其次，开始就用一个小模型，那这个小模型也是需要设计的，不能随便拿来一个，而设计一个性能高参数规模小的小模型难度是非常大的，往往是模型小了性能也低了；<br>第三点，大模型压缩后与小模型虽然参数规模相当，但是对应的模型空间并不相同<br>此外，为了更好的部署，如手机或FPGA等，得到精度更高模型更小(distillation)或者利用硬件加速(low-bit)，模型压缩都是值得试一试的手段。<br>更详细的讨论，可以参考<a href="https://www.zhihu.com/question/303922732" target="_blank" rel="noopener">为什么要压缩模型，而不直接训练一个小的CNN</a></p><h2><span id="bert-of-theseus">Bert of theseus</span><a href="#bert-of-theseus" class="header-anchor"></a></h2><p>Bert of theseus 方法属于上面提到的知识蒸馏，知识蒸馏中我们提到，在蒸馏时，我们不光要学习teacher的输出，对中间层我们也希望他们直接尽量相似，<br>那想象一个这种状态对应对理想情况：<code>中间层的结果一致，最终的结果一致</code>,既然我们的期望中间结果一致，那也就意味着两者可以互相替换。<br>正如开头提到的忒修斯之船一样。所以核心思想是：<br><code>与其设计复杂的loss来让中间层结果相似不如直接用小模型替换大模型来训练</code><br>通过复杂loss来达到与中间层结果相似可以看作是一种整体渐进式的逼近，让小模型一点点去学习，而直接替换可以看作是一种简单粗暴的方式，<br>但是他不需要设计各种loss，优化目标也是同一个，就只有一个下游任务相关的loss，突出一个<code>简洁</code>。<br>这就好比高中上学一样，即使花高价也要让孩子去一所好高中，因为学校的”氛围”能让孩子的学习成绩进步，其实是因为周围的孩子带着一起学，<br>弱鸡也能学的比平时更多一点。bert-of-theseus也是类似的道理，跟着大佬（teacher）总比单独fine-tuning效果好。</p><h2><span id="ju-ti-liu-cheng">具体流程</span><a href="#ju-ti-liu-cheng" class="header-anchor"></a></h2><p>如果直接将小模型替换大模型，那其实是在对小模型进行微调，与大模型就脱离了，也达不到对应的效果，所以作者采用了一种概率替换的方式。<br>首先呢，想象我们现在已经训练好了一个6层的BERT，我们成为<code>Predecessor（前辈）</code>, 而我们需要训练一个三层的bert，<br>他的结果近似12层BERT的效果，我们成为<code>Successor(传承者)</code>,那 bert-of-theseus的模型结构如<a href="https://spaces.ac.cn/archives/7575" target="_blank" rel="noopener">下图</a>所示：</p><p><img src="/2020/08/09/bert-of-theseus/bert-of-theseus.png" alt="bert-of-theseus"></p><p>在bert-of-theseus中，首先固定predecessor的权重，然后将6层的Bert分为3个block，每个block与successor的一层对应，训练过程分为两个stage：<br>首先用successor中的层概率替换predecessor中对应的block，在下游任务中直接fine-tuning（只训练successor），<br>然后将successor从bert-of-theseus中分离出来，单独在下游任务中进行fine-tuning，直到指标不再上升。<br>所谓替换，就是输出的替换，在进入下一层前在predecessor和successor的输出中二选一。<br>替换概率作者也给出了两种方式，一种是固定 0.5,一种是线性从0-1,如下图所示：<br><img src="/2020/08/09/bert-of-theseus/figure2.png" alt></p><h2><span id="shi-yan-xiao-guo">实验效果</span><a href="#shi-yan-xiao-guo" class="header-anchor"></a></h2><p>实验代码主要参考<a href="https://github.com/bojone/bert-of-theseus" target="_blank" rel="noopener">bert-of-theseus</a>, 实验主要做了三组，一组文本分类两组ner-crf，结果如下：</p><p>文本分类：CLUE的iflytek数据集</p><p>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \\text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 60.11\% &amp; 58.99\% &amp; 57.96\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 59.6\% &amp; 59.3\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p><p>ner-crf: 公司数据<br>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \\text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 97.5\% &amp; 97.0\% &amp; 96.1\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 97.3\% &amp; 96.6\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p><p>可以看到，相比直接那前几层微调，bert-of-theseus的效果确实更好，此外，我还尝试了线性策略的替换概率，效果上差别不大。<br>实验代码：<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py" target="_blank" rel="noopener">classification_ifytek_bert_of_theseus</a><br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/sequence_labeling_ner_bert_of_theseus.py" target="_blank" rel="noopener">sequence_labeling_ner_bert_of_theseus</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://github.com/JetRunner/BERT-of-Theseus" target="_blank" rel="noopener">论文原作者配图</a></p><div class="post__prevs"><div class="post__prev"><a href="/2020/08/01/optimizer-in-bert/" title="optimizer of bert"><i class="iconfont icon-prev"></i>optimizer of bert</a></div><div class="post__prev post__prev--right"><a href="/2020/08/19/bert-of-theseus-2/" title="Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-下篇">Knowledge Distillation (1) &#58; 模块替换之bert-of-theseus-下篇<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">22</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">5</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2021/01/20/ccf-qa-2/" title="ccf问答匹配比赛（下）：如何只用“bert”夺冠"><div class="item__cover"><img src="/2021/01/20/ccf-qa-2/head.png" alt="ccf问答匹配比赛（下）：如何只用“bert”夺冠"></div><div class="item__info"><h3 class="item__title">ccf问答匹配比赛（下）：如何只用“bert”夺冠</h3><span class="item__text">2021-01-20</span></div></a></li><li class="latest-post-item"><a href="/2021/01/12/matrix/" title="重新认识矩阵"><div class="item__cover"><img src="/2021/01/12/matrix/det.png" alt="重新认识矩阵"></div><div class="item__info"><h3 class="item__title">重新认识矩阵</h3><span class="item__text">2021-01-12</span></div></a></li><li class="latest-post-item"><a href="/2020/11/24/fine-tune/" title="如何提升bert在下游任务中的性能"><div class="item__cover"><img src="/2020/08/31/bert-01/bert.jpg" alt="如何提升bert在下游任务中的性能"></div><div class="item__info"><h3 class="item__title">如何提升bert在下游任务中的性能</h3><span class="item__text">2020-11-24</span></div></a></li><li class="latest-post-item"><a href="/2020/11/23/scl/" title="Contrastive Learning"><div class="item__cover"><img src="/img/default_cover.jpeg" alt="Contrastive Learning"></div><div class="item__info"><h3 class="item__title">Contrastive Learning</h3><span class="item__text">2020-11-23</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA-match/">QA match</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>