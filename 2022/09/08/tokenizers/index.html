<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>tokenizers 总结 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2022/09/08/tokenizers/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2022/09/08/tokenizers/cannot_code.PNG" alt="tokenizers 总结"></div><header class="post__info"><h1 class="post__title">tokenizers 总结</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2022-09-08</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/BPE/">BPE</a></li><li class="mark__item"><a href="/tags/WordPiece/">WordPiece</a></li><li class="mark__item"><a href="/tags/Unigram/">Unigram</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#tokenizer">tokenizer</a><ul><li><a href="#word-level">word level</a></li><li><a href="#char-level">char level</a></li><li><a href="#subword-level">subword level</a></li><li><a href="#bpe">BPE</a></li><li><a href="#bytes-bpe">Bytes BPE</a></li><li><a href="#wordpiece">WordPiece</a></li><li><a href="#unigram">Unigram</a></li><li><a href="#sentencepiece">SentencePiece</a></li><li><a href="#train-from-scratch">train from scratch</a></li></ul></li><li><a href="#tui-jian">推荐</a></li><li><a href="#bu-chong-yue-du">补充阅读</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><p></p></div><p></p><h1><span id="tokenizer">tokenizer</span><a href="#tokenizer" class="header-anchor"></a></h1><p>目前的机器学习模型都是数学模型，其对应的输入要求必须是数字形式（number）的，而我们处理的真实场景往往会包含许多非数字形式的输入（有时候即使原始输入是数字形式，我们也需要转换），最典型的就是NLP 中的文字(string),为了让文字能够作为输入参与到模型的计算中去，我们就需要构建一个映射关系(mapping):将对应的文字映射到一个数字形式上去，而其对应的数字就是token。而对应的这个映射关系，就是我们的tokenizer：他可以将文字映射到其对应的数字上去(encode)，也可以将数字映射回对应的文字上(decode).</p><h2><span id="word-level">word level</span><a href="#word-level" class="header-anchor"></a></h2><p>那如何构建这个映射关系呢？最简单的想法：一个词对应一个id 不就好了。也就是”word level”。然后，”词”缺不是那么好分的:对于中文，词到底分成什么粒度，”苹果手机“到底是一个词还是二个词呢？”武汉市/长江/大桥/欢迎/你“与”武汉/市长/江大桥/欢迎/你“应该选择哪个方案呢？此外，对于”虚坤“/”鸡你太美“等这些新词怎么识别呢？对这部分感兴趣的可以看我更早期关于分词总结的博客：<a href="https://xv44586.github.io/2019/10/22/cutwords/">分词算法综述</a>.</p><p>中文有分词的问题，那英语总没有了吧，空格+标点就是天然分隔符，然而事情也没那么简单：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Don<span class="string">'t cry!</span></span><br></pre></td></tr></table></figure><p></p><p>按照空格与标点切分，得到的结果是：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;Don&quot;, &quot;&apos;&quot;, &quot;t&quot;, &quot;cry&quot;, &quot;!&quot;]</span><br></pre></td></tr></table></figure><p></p><p>哈，有点怪，我们需要把”Don’t” -&gt; “Do”/“n’t”,这样才更合理一点。即：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;Do&quot;, &quot;n&apos;t&quot;, &quot;cry&quot;, &quot;!&quot;]</span><br></pre></td></tr></table></figure><p></p><p>除了如何切分成词外，还有一个最大的问题是结果集太大。比如TransformerXL 中，使用标点和空格，切分后的词表大小有267K，如此大的词表，不管是对存储还是计算都有压力。</p><p>此外，随着文化发展，每天都有大批的新词出现，用词粒度做映射就越来越不理想了。</p><h2><span id="char-level">char level</span><a href="#char-level" class="header-anchor"></a></h2><p>既然分词这么麻烦，我不分不好了，我就按“字”(char) 来做最小单元做映射。这样词表就小多了：英文只需要26个字母即可，中文根据2013年<a href="http://www.moe.gov.cn/jyb_xwfb/s5147/201308/t20130828_156423.html" target="_blank" rel="noopener">中华人民共和国教育部《通用规范汉字表》定义“规范汉字”</a>，国家规定的通用规范汉字一共为8105个，相比之下也不算大。</p><p>然而char level 的主要问题是切分的太细：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pneumonoultramicroscopicsilicovolcanoconiosis</span><br><span class="line"></span><br><span class="line">（医）肺尘病、矽肺病</span><br></pre></td></tr></table></figure><p></p><p>上面这个单词是我找到的最长的英语单词，他有45个字母组成，而中文中也存在大量的成语/歇后语/专用名词等，如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">只许州官放火不许百姓点灯</span><br></pre></td></tr></table></figure><p></p><p>目前我们NLP 的主要思路是对句子进行，即：<br>$$<br>P(S)=P(w_1,w_2,..w_n)=P(w_1)∗P(w_2|w_1)∗P(w_3|w_1,w_2)∗…∗P(w_n|w_1,w_2,..w_{n−1})<br>$$<br>而切分太细，则对应$S$ 的长度会变长，无疑大大增加建模难度（通过字来学习词的语义），也常常导致模型效果不理想。</p><h2><span id="subword-level">subword level</span><a href="#subword-level" class="header-anchor"></a></h2><p>为了缓解两种方式的问题，一个想法是取长补短，即目前的主流方案：subword level.</p><p>subword level 的主要出发点是：我们应该尽量保留高频词，对低频词进行逐级切分，以获得一个词表大小合适，又对后续建模友好的方案。</p><p>目前subword level 的tokenizer 方法主要有BPE, Bytes BPE, WordPiece, Unigram, SentencePiece,下面简单总结一下各个方法。</p><h2><span id="bpe">BPE</span><a href="#bpe" class="header-anchor"></a></h2><p>bpe 的方案是通过统计词频来确定两个相邻的pair subwords 要不要合并，具体做法：<br>1.统计pre-tokenize 的word 的词频；<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">"hug"</span>, 10), (<span class="string">"pug"</span>, 5), (<span class="string">"pun"</span>, 12), (<span class="string">"bun"</span>, 4), (<span class="string">"hugs"</span>, 5)</span><br></pre></td></tr></table></figure><p></p><p>2.使用词典对word 进行切分：<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># base vocabulary: ["b", "g", "h", "n", "p", "s", "u"]</span></span><br><span class="line">(<span class="string">"h"</span> <span class="string">"u"</span> <span class="string">"g"</span>, 10), (<span class="string">"p"</span> <span class="string">"u"</span> <span class="string">"g"</span>, 5), (<span class="string">"p"</span> <span class="string">"u"</span> <span class="string">"n"</span>, 12), (<span class="string">"b"</span> <span class="string">"u"</span> <span class="string">"n"</span>, 4), (<span class="string">"h"</span> <span class="string">"u"</span> <span class="string">"g"</span> <span class="string">"s"</span>, 5)</span><br></pre></td></tr></table></figure><p></p><p>3.统计相邻两个subword pair 词频，将top-k 高的pair 合并生成新的subword，添加进vocabulary，同时，如果当前的subword 只会同pair 一起出现，则同时将vocabulary 中对应subword 删除。<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># count pair</span></span><br><span class="line">h + u = 10 + 5 = 15</span><br><span class="line">u + g = 10 + 5 +  = 20</span><br><span class="line">...</span><br><span class="line"><span class="comment"># merge top k</span></span><br><span class="line"><span class="built_in">set</span> k = 1</span><br><span class="line">ug -&gt; vocabulary</span><br><span class="line">base vocabulary: [<span class="string">"b"</span>, <span class="string">"g"</span>, <span class="string">"h"</span>, <span class="string">"n"</span>, <span class="string">"p"</span>, <span class="string">"s"</span>, <span class="string">"u"</span>, <span class="string">"ug"</span>]</span><br><span class="line">​</span><br><span class="line">loop until vocabulary match vocab_size</span><br><span class="line">​</span><br></pre></td></tr></table></figure><p></p><h2><span id="bytes-bpe">Bytes BPE</span><a href="#bytes-bpe" class="header-anchor"></a></h2><p>Bytes BPE 与BPE基本相同，唯一不同的是：BPE 中会存在UNK 的情况，为了解决unk 的问题，一个非常天才的想法是将所有text 先转为<code>bytes</code> ，这样就不会存在unk 的问题，尤其是在多语言中，这种方式可以大大缩减词表大小;此外即使不是目标语言训练的模型也可以拿来使用。通常词表大小包括256 个基本bytes + &lt;end|of|text&gt; + vocab-size,如gpt2 的词表为50257: 256 base bytes tokens + &lt;end|of|text&gt; + 50,000 merges.<br>此外，训练bytes bpe 时，通常我们还会选择先将文本进行normalize，这部分后面会进一步说明。</p><h2><span id="wordpiece">WordPiece</span><a href="#wordpiece" class="header-anchor"></a></h2><p>WordPiece 与BPE 也非常相似，区别主要在于merge 的策略：BPE 中选择频率最高的pair 进行合并，WordPiece 则选择使用语言模型来进行选择：<br>$$<br>L = logP(S) = \sum^Nlog(P_i)<br>$$<br>对于两个subword： $t_x$, $t_y$ ，合并后为 $t_z$ ，则合并前后的增益：<br>$$<br>Loss = logP(t_z) - (logP(t_x) + logP(t_y))<br>$$<br>通过计算合并增益是否增大来决定是否合并subword pair.</p><h2><span id="unigram">Unigram</span><a href="#unigram" class="header-anchor"></a></h2><p>Unigram 与 上述的方法都略有不同：Unigram 不再是通过合并base vocabulary 中的subword 来新增，他选择在初始化时初始化一个非常大的subword set，通过计算是否需要将一个subword 切分为多个base subword （remove 这个subword）来减小vocabulary size 直到达到vocab size。<br>这里有一个假设：句子之间是独立的，subword 与 subword 之间是独立的。对应的句子的语言模型似然值就是其subword 的概率的乘积。目标是保存vocab size 的同时语言模型似然值最大。<br>$$<br>x^* = argmax_{x \in U}P(\overrightarrow{x})<br>$$<br>整个求解过程是一个简单的EM 或者说一个迭代过程：<br>0.建立一个足够大的种子subword vocabulary，可以用字典树构建可以是所有字符的组合，也可以用bpe 构建；<br>1.（期望E）统计vocabulary 中每个subword 的频率，计算其对应概率值；<br>2.（最大化M）根据其概率，使用维特比算法返回其语言模型似然值最大化下的最佳分割方案；<br>3.计算最佳分割方案下每个新子词的loss，这里的loss 是指将当前subword 从vocabulary 中移除时，对应的语言模型似然值，即<br>$$<br>L = − \sum^Nlog (\sum_{x∈S(x_i)}p(x))<br>$$<br>4.丢弃掉loss 前x% 对应的subword；<br>5.重复2-4阶段，直到vocabulary 达到指定的vocab size。</p><h2><span id="sentencepiece">SentencePiece</span><a href="#sentencepiece" class="header-anchor"></a></h2><p>SentencePiece 其实并不是一个新的tokenizer 方法，他其实是一个实现了BPE/Unigram tokenizer 的一个集合，不过他有一些创新的地方。<br>上述方法中有一些问题：<br>1.都有字，子词或词的概念，然而在很多语言中并没有这样的概念；<br>2.都默认需要自己进行pre-tokenize，如英语则利用“空格”作为词的分割符，中文则一般选择jieba 进行pre-tokenize，这个过程不同的语言有自己的一套做法，不统一；<br>3.token 格式不统一。以英文为例，表示token 时会有 ##xx, xx/s 这种，表示subword 是否出现在词的首尾，然而中文中是没有这种概念的；<br>4.解码困难，如BPE解码时需要进行一些标准化，最常见的是去除标点符号。而我们解码后是 [new] [york]两个token，我们并不知道原来的词是 newyork/new york/new-york 中的哪一个.<br>SentencePiece 的做法：<br><a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">SentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol. To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol “▁” (U+2581) as follows.</a></p><p>即首先将空格转换为一个标准的字符”▁”,然后将text 转换为unicode，其实这里的unicode 是NFKC-based normalization后的unicode，至于unicode 标准化，可以参考<a href="https://blog.csdn.net/weixin_43866211/article/details/98384017" target="_blank" rel="noopener">unicode文本标准化</a> ，虽然通常我们使用NFKC 标准化，但sentencepiece 内部四种方法都实现了。<br>通过上述的空格转换加normalize，所有的语言经过转换后就有统一的格式了，这样多语言的问题就彻底的与token 切分解偶了，tokenizer 就有了一个完全端到端的解决方案。</p><h2><span id="train-from-scratch">train from scratch</span><a href="#train-from-scratch" class="header-anchor"></a></h2><p>训练一个tokenizer model 主要有两个仓库可以参考： <a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">huggingface/tokenizers</a> 和 <a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">google/sentencepiece</a>.<br>其中tokenizers 支持bpe/bytes bpe/unigram/wordpiece, sentencepiece 支持bpe/unigram.两者都支持四种标注化方法。<br>此外，tokenizers 不支持自定义的pre_tokenizer的保存，如中文时我们常用的jieba.lcut ；bytes bpe不支持big dataset 的训练，1T 内存训练100G 文本也会因内存不足被killed，（一个办法是缩小语料训练，因为没有oov 的问题，基本上小语料下训练也能用）。<br>在实际使用时，通常会结合huggingface/transformers 一起使用，这里也看了一下transformers/tokenizers 的实现。原始的GPT2 中的tokenizer 是没有做normalize 的，所以 transformers中的GPT2Tokenizer 也是没有做normalize 的，而通常我们自己训练的bytes bpe 是会加一个normalize 的过程，所以如果是通过huggingface/tokenizers 训练的tokenier，迁移到transformers 时需要注意normalize 是否实现。</p><h1><span id="tui-jian">推荐</span><a href="#tui-jian" class="header-anchor"></a></h1><p>当目标语言为中文时，推荐使用WordPiece + jieba 的方案；而是多语言场景时，推荐使用SentencePieceBPE/SentencePieceUnigram.<br>无论哪种合并/切分 subword 的策略，我们的初衷是:<br><code>尽量不切分常用词，而是将不常用词切分为常用的子词.</code><br>而中文中，有明确的字/词概念，却没有子词的概念（如英文中有”app”, “##le”, 中文却没有”苹” “##果”），而转bytes 后对子词更友好，此外，中文通常需要3个bytes（GBK）或者4个bytes（Chinese-Japanese character set），对于一个中文的字，很有可能需要大于1个token 来表示，反而会增加tokenize 后序列的长度，对模型的训练与使用不利；此外，中文中空格也没有切分词/句子 的语义，保留空格反而会由于各种空格的错误使用带来问题，最终的推荐方案就是jieba + Word Piece/SentencePieceUnigram。<br>而多语言场景下，推荐使用SentencePieceBPE，他提供一个端到端的方案，而不需要再根据不同语言进行不同的pre-tokenize/subword 格式，此外，SentencePiece 都是bytes 粒度的，这样既能大大缩减词表又能避免unk 的情况。</p><h1><span id="bu-chong-yue-du">补充阅读</span><a href="#bu-chong-yue-du" class="header-anchor"></a></h1><p><a href="https://huggingface.co/docs/transformers/tokenizer_summary" target="_blank" rel="noopener">tokenizer summary-huggingface</a><br><a href="https://towardsdatascience.com/difference-between-nfd-nfc-nfkd-and-nfkc-explained-with-python-code-e2631f96ae6c" target="_blank" rel="noopener">Difference Between NFD, NFC, NFKD, and NFKC Explained with Python Code-medium</a><br><a href="https://zhuanlan.zhihu.com/p/86965595" target="_blank" rel="noopener">深入理解NLP Subword算法：BPE、WordPiece、ULM-知乎</a><br><a href="https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html#121_expectation-maximization" target="_blank" rel="noopener">natural_language_understanding/subword_units/subword_units-github.io</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>本人真实写照🐶</p><div class="content-footer-sponsor"><h1><span id="sponsor">Buy me a coffee</span></h1><p>如果觉得这篇文章不错，对你有帮助，欢迎打赏一杯蜜雪冰城。</p><img src="/img/sponsor.JPG" alt="logo" title="sponsor"></div><div class="post__prevs"><div class="post__prev"><a href="/2022/07/06/horovod-multi-nodes/" title="训练加速篇（3）horovod之多机多卡"><i class="iconfont icon-prev"></i>训练加速篇（3）horovod之多机多卡</a></div><div class="post__prev post__prev--right"><a href="/2023/01/09/zero-to-chatgpt/" title="From zero to ChatGPT">From zero to ChatGPT<i class="iconfont icon-next"></i></a></div></div></div></article><script src="https://giscus.app/client.js" data-repo="xv44586/giscus" data-repo-id="R_kgDOIC6Ipg" data-category="Announcements" data-category-id="DIC_kwDOIC6Ips4CRkmo" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="zh-CN" crossorigin="anonymous" async></script></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">33</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2023/03/25/gpt4/" title="GPT-4 yes!! but"><div class="item__cover"><img src="/2023/03/25/gpt4/bg.jpeg" alt="GPT-4 yes!! but"></div><div class="item__info"><h3 class="item__title">GPT-4 yes!! but</h3><span class="item__text">2023-03-25</span></div></a></li><li class="latest-post-item"><a href="/2023/03/10/llm-inf/" title="LLM Inference串讲"><div class="item__cover"><img src="/2023/03/10/llm-inf/sd.PNG" alt="LLM Inference串讲"></div><div class="item__info"><h3 class="item__title">LLM Inference串讲</h3><span class="item__text">2023-03-10</span></div></a></li><li class="latest-post-item"><a href="/2023/02/01/fine-tuning-at-few-shot/" title="few-shot视角下的fine-tuning"><div class="item__cover"><img src="/2023/02/01/fine-tuning-at-few-shot/himalayas.JPG" alt="few-shot视角下的fine-tuning"></div><div class="item__info"><h3 class="item__title">few-shot视角下的fine-tuning</h3><span class="item__text">2023-02-01</span></div></a></li><li class="latest-post-item"><a href="/2023/01/09/zero-to-chatgpt/" title="From zero to ChatGPT"><div class="item__cover"><img src="/2023/01/09/zero-to-chatgpt/chatgpt-bg.jpeg" alt="From zero to ChatGPT"></div><div class="item__info"><h3 class="item__title">From zero to ChatGPT</h3><span class="item__text">2023-01-09</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/BPE/">BPE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/ChatGPT/">ChatGPT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-3/">GPT-3</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-4/">GPT-4</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/In-context-learning/">In-context learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Inference/">Inference</a></li><li class="tag-item"><a class="tag-link" href="/tags/LLM/">LLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/T5/">T5</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Unigram/">Unigram</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordPiece/">WordPiece</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/debug/">debug</a></li><li class="tag-item"><a class="tag-link" href="/tags/faster-decoder/">faster decoder</a></li><li class="tag-item"><a class="tag-link" href="/tags/fine-tuning/">fine-tuning</a></li><li class="tag-item"><a class="tag-link" href="/tags/horovod/">horovod</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/nohup/">nohup</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/simbert/">simbert</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/speed-up/">speed-up</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>