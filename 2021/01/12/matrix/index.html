<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>重新认识矩阵 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2021/01/12/matrix/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2021/01/12/matrix/det.png" alt="重新认识矩阵"></div><header class="post__info"><h1 class="post__title">重新认识矩阵</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2021-01-12</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/Math/">Math</a></li><li class="mark__item"><a href="/tags/Matrix/">Matrix</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#shi-me-shi-1">什么是1</a></li><li><a href="#xiang-liang">向量</a></li><li><a href="#ju-zhen">矩阵</a></li><li><a href="#ju-zhen-cheng-fa">矩阵乘法</a></li><li><a href="#xiang-si-ju-zhen">相似矩阵</a></li><li><a href="#xing-lie-shi">行列式</a></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><p>最近有篇很火的论文bert-flow，其中的flow是出了GAN和VAE之外的第三种生成模型，我竟然是第一次听说，所以引起了我的好奇心，然而看flow模型时，发现里面有一个很重要的概念就是行列式，数学渣太久没碰过他所以再看到有些陌生，于是就找了一些文章来重新学习一下，后来发现了孟岩的认识矩阵系列博客与苏神的新认识矩阵系列博客，看完感觉对矩阵的认识refresh了一下，所以做一些总结记录。</p><h1><span id="shi-me-shi-1">什么是1</span><a href="#shi-me-shi-1" class="header-anchor"></a></h1><p>首先我们来讨论一下，什么是<code>1</code>，所谓1，就是参照物，或者是基，而其他的数字都是在参考他得到的，比如2,2就是2 个1，也就是1<em>2.<br>而1又是一个抽象的东西，你没法直观感受什么是1，只有在参考系下，才能直观感受到什么是1，如1瓶水，1米长，而1 与一米的关系就像类与对象一样。所以在参考系下，2米的含义就是2个1米，1米 </em>2，即在以1米为基的坐标系下，他在2的“位置”。而5瓶水，对应的含义是在以1瓶水为基的参考系下，<br>他处在5的“位置”。而不同的坐标系，在不同的情况下也有优劣之分，如同样的酒，当用来统计销售额时，我们用“瓶”来统计就比用“杯”来统计方便，而当用来劝酒时，用“杯”就明显比用“瓶”要合适一些。也就是，酒还是那么多酒，但是当你用不同的方式来度量时，得到的结果也是不同的，是有优劣之分的。</p><h1><span id="xiang-liang">向量</span><a href="#xiang-liang" class="header-anchor"></a></h1><p>对于向量$A(a, b)$,其对应的含义是在直角坐标系（以$\vec{i}=(1,0),\vec{j}=(0,1)$为基）中，A是在x轴上为a,y轴上为b的点。<br>比如，在以x轴为斤y轴为瓶的直角坐标系（以$\vec{i}=(一斤,0瓶), \vec{j}=(0斤,1瓶)$为基）中，三瓶500毫升的肥仔水对应的向量就是B(3,3).所以，<code>所谓向量即在线性空间内，选定一组基后，用来刻画一个对象</code>。</p><h1><span id="ju-zhen">矩阵</span><a href="#ju-zhen" class="header-anchor"></a></h1><p>那何为矩阵呢？比如对于矩阵<br>$$A=\begin{pmatrix}<br>a &amp; c \\<br>b &amp; d<br>\end{pmatrix}$$<br>我们能观察到什么呢？首先，矩阵也是由向量（列向量）组成的，对于当前矩阵A，也就对应着$\vec{i}=(a,b),\vec{j}=(c,d)$ 两个向量，然后将他们按一定的顺序排列就组成了矩阵；此外，这个矩阵又代表了由这两个向量为基组成的坐标系（线性空间），在这个空间内，所有的对象都能通过这两个基来进行刻画，即对应于一个“向量”。这里如何更直观的理解呢？还是考虑之前的例子，同样是酒，我们可以用“杯”来度量，也可以用“瓶”来度量，假如我们用“杯”作为最初的基，<br>那矩阵对应的就是“瓶”作为基构成的坐标系，而构成矩阵的向量又是在以“杯”为基下度量出来的，其对应的向量中的值的含义就是“瓶”的基在“杯”这个基下的表示，<br>同样的酒，假如在“杯”坐标系下为10（杯），通过这个矩阵，就变为了2（瓶），<code>即矩阵是线性空间里变换（运动）的描述。</code></p><h1><span id="ju-zhen-cheng-fa">矩阵乘法</span><a href="#ju-zhen-cheng-fa" class="header-anchor"></a></h1><p>那何为矩阵乘法呢？比如现在有矩阵<br>$$<br>A=\begin{pmatrix}<br>a &amp; c \\<br>b &amp; d<br>\end{pmatrix}<br>$$<br>向量$x=(e,f)$,对于$Ax$这个矩阵与向量的乘法，这个是什么含义呢？上面我们已经提到了，矩阵是由列向量组成的，而列向量又可以看做是对应坐标系下的一组基，那根据之前我们提到向量与基的关系，就是在基确定后，用来刻画其空间内对象的，对应的就是乘法，那这个矩阵与向量的乘法，我们也可以用这种方式来看待，即矩阵与向量的乘法代表的是在矩阵对应的列向量为基组成的空间内，由向量刻画的对象。这也是为什么矩阵乘法要求对应维度要相等，<br>对应的就是需要用所有的基，才能正确刻画在这组基对应坐标系下的位置（对象）。而矩阵与向量乘法就是用新的基来刻画对象，<code>也即矩阵与向量的乘法代表施加变换。</code></p><p>那为什么矩阵的乘法公式里为对应行与列相乘并求和呢？这是因为最终的结果，我们是需要变换到对应的直角坐标系下的，因为我们书写出来的所有向量，都默认是在直角坐标系下的结果，所以我们需要将结果向量在每个轴上进行分解，然后合并得到最终的结果，对应的就是行与列相乘后求和。</p><p>那何为矩阵与矩阵的乘法呢？如$A * B$,同理，矩阵与矩阵的乘法也可以看做是连续变换后，构造的新的坐标系。矩阵是由向量组成的，而矩阵与向量乘法的含义是在新的基下刻画的对象表示，所以矩阵与矩阵的乘法，对应B这个变换在施加了A这个变换后，形成的新的变换，而施加这个变换，对应B中的列向量都是在A坐标下刻画得到的。</p><p>那何为线性方程呢？比如$Ax = y$, 对应的线性方程是什么含义呢？这个方程左侧有向量有矩阵，而右侧缺只有向量，有点不和谐，我们变换一下，让他们形式一样，$Ax = Iy$,这样，等式成立，两边形式也一样。那写成这个变换，我们就能猜到这个方程中等式的意义了：在直角坐标系($I$)下的y向量，在A坐标系下该如何刻画(x)呢？那再来看看这个方程的解，即$x = A^{-1}y$，而这个式子可以理解为将施加的变换“逆”着再变回去，就还原了原始位置。</p><p>普通乘法有交换律、结合律、分配律，那矩阵乘法是否也满足呢？对于交换律，矩阵乘法是不满足的，想象一下，5“瓶”酒换成“杯”可能是25，而5“杯”酒换成“瓶”，可能也就是1了，所以两个变换交换位置，得到的是不同的变换；而结合律与分配律是满足的，结合律可以看做是将变换分成“几步”走，即先进行“子变换a”,在进行“子变换b”，几步走与一步到位的结果是一致的；而分配律也是类似的思路。</p><h1><span id="xiang-si-ju-zhen">相似矩阵</span><a href="#xiang-si-ju-zhen" class="header-anchor"></a></h1><p>那何为相似矩阵呢？假设有一个矩阵A，对应$y=Ax$, 而现在有一个新的坐标系P，对应有${y}’=B{x}’$,在P坐标系下，x与y对应着$P{x}’=x$, $P{y}’=y$,代入后得到：$P{x}’=AP{y}’ = P(P^{-1}AP){x}’$，即在P坐标系下，从${x}’$ 到${y}’$的变换用矩阵$B=P^{-1}AP$来表示，这就是相似矩阵，<a href="https://spaces.ac.cn/archives/1777" target="_blank" rel="noopener">即同一个线性变换在不同坐标系下的一个测量结果而已。</a></p><h1><span id="xing-lie-shi">行列式</span><a href="#xing-lie-shi" class="header-anchor"></a></h1><p>行列式是在通过<a href="https://zhuanlan.zhihu.com/p/37111386" target="_blank" rel="noopener">高斯消元法解线性方程组时引入的数学工具</a>, 对应的定义为：<br>$$<br>D = \sum(-1)^t a_{1p_n}a_{2p_n}…a_{np_n}<br>$$<br>其中$t$为排列$a_{1p_n}a_{2p_n}…a_{np_n}$的逆序数，$\sum$ 为对所有可能的排列求和。<br>这个计算方式看起来好奇怪，还有一个逆序数，无法直观的理解，那怎么才能有个直观印象，让我们知道为什么这里是这么计算的呢？<br>说实话我虽然几年前在网易云课堂重新学了一遍线性代数，但是今天又基本全忘记了，所以这次，我希望找到一些更“直观”的东西。<br>如果按照矩阵就是对应坐标系的变换，也就是对应基的变换的思路，那矩阵的行列式应该是对应着变换的某种度量（实际是对应坐标系的缩放，也就是基的体积变换）。顺着这个思路，苏神的一篇博文<a href="https://kexue.fm/archives/2208" target="_blank" rel="noopener">新理解矩阵5：体积=行列式</a>中，通过两者的性质相同，证明了行列式的几何意义，就是其对应的n维平行n维体的体积。<br>知道了这个，我们就可以尝试通过计算平行n维体的体积来“直观”感受一下了。<br><img src="/2021/01/12/matrix/det.png" alt></p><p>这里我们讨论二维空间，如上图所示，其中$\overrightarrow{A}$与$\overrightarrow{B}$分别代表两个向量，而其围成的面积S 为(O,A,I,B)四个点所围成的图形。而这个图形又可以看成两个部分的组合：一部分为与四边形(O,D，P,G)重叠的部分，另一部分为剩下的部分。而剩下的部分又可以通过$PI$切分为两个三角形：$\bigtriangleup BPI$ $\bigtriangleup API$，<br>我们分别做两个三角形的高（蓝色线）$PJ=b, PK=c$，此时我们将$\bigtriangleup BPI$ 与 $\bigtriangleup ODA$ 进行对比，两者的高都是$b$,而底的差为$c$，也就是$S\bigtriangleup ODA - S\bigtriangleup BPI = S\bigtriangleup OCM$， 类似的，我们可以求出$S\bigtriangleup OBG - S\bigtriangleup API = S\bigtriangleup OFM$,于是，我们就可以得到一个结论：<br>$$<br>S_{\square OAIB} = S_{\square ODPG} - S_{\square OCMF} = ad - bc = \begin{Vmatrix}<br>a &amp;b \\<br>c &amp; d<br>\end{Vmatrix} = det(A,B)<br>$$</p><p>而两个四边形的位置都是由向量在坐标轴上投影后确定的，也就是对应着行列式中的“排列相乘”，而面积不足需要被“减掉”的小面积就对应逆序数，所以完整的过程就是“通过排列求长方形的面积，然后用大面积减去小面积”。<br>以上的思路可以推广到N维空间，思想是一样的。 而理解了矩阵的行列式就是平行N维体的体积后，对应行列式的计算公式也就显得相当“直观”了。如，对行列式的某一列乘上$\alpha$，则对应的行列式的值也乘上$\alpha$，这里就是对应其中的一个基向量放大了$\alpha$倍，所以体积放大了$\alpha$倍。</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文主要是参考了许多文章后，发现将矩阵看做是变换的表述这个视角下，很多相关问题都变得非常直观又容易理解，最后经过思考做的部分总结，如果对矩阵感兴趣或者总觉得不容易理解，推荐孟岩的<a href="http://blog.csdn.net/myan/article/details/647511" target="_blank" rel="noopener">理解矩阵系列</a>、苏神的<a href="[https://kexue.fm/archives/1765">新理解矩阵系列</a>、<a href="https://www.matongxue.com/madocs/247/" target="_blank" rel="noopener">马同学的矩阵与行列式系列</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><div class="content-footer-sponsor"><h1><span id="sponsor">Buy me a coffee</span></h1><p>如果觉得这篇文章不错，对你有帮助，欢迎打赏一杯蜜雪冰城。</p><img src="/img/sponsor.JPG" alt="logo" title="sponsor"></div><div class="post__prevs"><div class="post__prev"><a href="/2020/11/24/fine-tune/" title="如何提升bert在下游任务中的性能"><i class="iconfont icon-prev"></i>如何提升bert在下游任务中的性能</a></div><div class="post__prev post__prev--right"><a href="/2021/01/20/ccf-qa-2/" title="ccf问答匹配比赛（下）：如何只用“bert”夺冠">ccf问答匹配比赛（下）：如何只用“bert”夺冠<i class="iconfont icon-next"></i></a></div></div></div></article><script src="https://giscus.app/client.js" data-repo="xv44586/giscus" data-repo-id="R_kgDOIC6Ipg" data-category="Announcements" data-category-id="DIC_kwDOIC6Ips4CRkmo" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="zh-CN" crossorigin="anonymous" async></script></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">33</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2023/03/25/gpt4/" title="GPT-4 yes!! but"><div class="item__cover"><img src="/2023/03/25/gpt4/bg.jpeg" alt="GPT-4 yes!! but"></div><div class="item__info"><h3 class="item__title">GPT-4 yes!! but</h3><span class="item__text">2023-03-25</span></div></a></li><li class="latest-post-item"><a href="/2023/03/10/llm-inf/" title="LLM Inference串讲"><div class="item__cover"><img src="/2023/03/10/llm-inf/sd.PNG" alt="LLM Inference串讲"></div><div class="item__info"><h3 class="item__title">LLM Inference串讲</h3><span class="item__text">2023-03-10</span></div></a></li><li class="latest-post-item"><a href="/2023/02/01/fine-tuning-at-few-shot/" title="few-shot视角下的fine-tuning"><div class="item__cover"><img src="/2023/02/01/fine-tuning-at-few-shot/himalayas.JPG" alt="few-shot视角下的fine-tuning"></div><div class="item__info"><h3 class="item__title">few-shot视角下的fine-tuning</h3><span class="item__text">2023-02-01</span></div></a></li><li class="latest-post-item"><a href="/2023/01/09/zero-to-chatgpt/" title="From zero to ChatGPT"><div class="item__cover"><img src="/2023/01/09/zero-to-chatgpt/chatgpt-bg.jpeg" alt="From zero to ChatGPT"></div><div class="item__info"><h3 class="item__title">From zero to ChatGPT</h3><span class="item__text">2023-01-09</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/BPE/">BPE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/ChatGPT/">ChatGPT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-3/">GPT-3</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-4/">GPT-4</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/In-context-learning/">In-context learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Inference/">Inference</a></li><li class="tag-item"><a class="tag-link" href="/tags/LLM/">LLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/T5/">T5</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Unigram/">Unigram</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordPiece/">WordPiece</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/debug/">debug</a></li><li class="tag-item"><a class="tag-link" href="/tags/faster-decoder/">faster decoder</a></li><li class="tag-item"><a class="tag-link" href="/tags/fine-tuning/">fine-tuning</a></li><li class="tag-item"><a class="tag-link" href="/tags/horovod/">horovod</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/nohup/">nohup</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/simbert/">simbert</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/speed-up/">speed-up</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>