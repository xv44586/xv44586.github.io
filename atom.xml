<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小蛋子</title>
  
  <subtitle>小蛋子</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xv44586.github.io/"/>
  <updated>2020-09-12T16:04:15.240Z</updated>
  <id>https://xv44586.github.io/</id>
  
  <author>
    <name>[object Object]</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>distilling knowledge of bert</title>
    <link href="https://xv44586.github.io/2020/08/31/bert-01/"/>
    <id>https://xv44586.github.io/2020/08/31/bert-01/</id>
    <published>2020-08-31T15:14:55.000Z</published>
    <updated>2020-09-12T16:04:15.240Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#distilling-knowledge">Distilling Knowledge</a><ul><li><a href="#distilling-the-knowledge-in-a-neural-network">Distilling the Knowledge in a Neural Network</a><ul><li><a href="#distilling">Distilling</a></li></ul></li><li><a href="#distill-bert">Distill BERT</a></li><li><a href="#tinybert">TinyBERT</a><ul><li><a href="#you-dian">优点</a></li><li><a href="#que-dian">缺点</a></li></ul></li><li><a href="#distilbert">DistilBERT</a><ul><li><a href="#you-dian-1">优点</a></li></ul></li><li><a href="#mobilebert">MobileBERT</a><ul><li><a href="#you-dian-2">优点</a></li><li><a href="#que-dian-1">缺点</a></li></ul></li></ul></li><li><a href="#lun-wen-zong-jie">论文总结</a></li><li><a href="#xiang-fa">想法</a></li><li><a href="#shi-yan">实验</a><ul><li><a href="#teacher-to-student">Teacher-to-Student</a></li><li><a href="#student-to-student">student-to-student</a></li><li><a href="#normal-noise-training">normal-noise-training</a></li></ul></li><li><a href="#shi-yan-jie-guo">实验结果</a></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>上篇讨论了bert-of-theseus，算是一个开篇，本文继续讨论关于模型蒸馏（Distilling Knowledge）及关于BERT模型的知识蒸馏。</p><h1><span id="distilling-knowledge">Distilling Knowledge</span><a href="#distilling-knowledge" class="header-anchor"></a></h1><h2><span id="distilling-the-knowledge-in-a-neural-network">Distilling the Knowledge in a Neural Network</span><a href="#distilling-the-knowledge-in-a-neural-network" class="header-anchor"></a></h2><p>这篇是2015年Hinton发表的,也是我看到的最早提出Knowledge Distillation的论文。<br>在这篇论文中，Hinton指出one-hot 的label只指示了true label 的信息，但是没有给出negative label 之间、negative 与 true label之间<br>的相对关系，比如：比如现在的任务是给定一个词（比如：苹果），然后判断词对应的类别（电视/手机/水果/汽车），假如现在我们有两个样本：<br>（苹果，[0,0,1,0]）和 （小米，[0,1,0,0]), 而one-hot 形式的label并不能告诉我们，苹果中 label是水果的概率高出label是拖拉机的概率，<br>稍低于是手机的概率，而小米中label是电视的概率稍低于是手机的概率，但是同时要高于是汽车和水果的概率，这些相对关系在one-hot 形式的label中<br>是无法得到的，而这些信息非常重要，有了这些信息，我们可以更容易的学习任务。于是提出了Teacher-Student模式，<br>即用一个大的复杂的模型（也可以是ensemble后的）来先学习，然后得到label的相对关系（logits），然后将学习到的知识迁移到一个小模型（Student）。</p><h3><span id="distilling">Distilling</span><a href="#distilling" class="header-anchor"></a></h3><p>具体迁移过程是Student 在进行training 时，除了学习ground truth 外，还需要学习label 的probability（softmax output），但是不是直接学习<br>softmax output，而是学习<code>soften labels</code>，所谓soften labels 即经过<code>Temperature</code> 平滑后的 probability，具体形式：<br>$$<br>q_{i} = \frac{exp(z_{i}/T)}{\sum_{j}^{}exp(z_{j}/T)}<br>$$<br>其中T 越大，对应的probability 越平滑，如下图所示。而平滑probability 可以看作是对soften label的一种正则化手段。<br> <img src="/2020/08/31/bert-01/soften.png" alt></p><p>更直观的实验请查阅<a href="https://github.com/xv44586/Knowledge-Distillation-NLP/blob/master/Knowledge_Distillation_From_Scratch.ipynb" target="_blank" rel="noopener">Knowledge Distillation From Scratch</a></p><h2><span id="distill-bert">Distill BERT</span><a href="#distill-bert" class="header-anchor"></a></h2><p>看到的第一篇针对BERT 模型做蒸馏的是<a href="http://arxiv.org/abs/1903.12136" target="_blank" rel="noopener">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a>,<br>在这篇论文中，作者延续Hinton 的思路在BERT 上做实验，首先用BERT-12 做Teacher，然后用一个单层Bi-LSTM 做Student，loss 上除了<br>ground truth 外，也选择了使用teacher 的logits，包括Temperature 平滑后的soften labels 的CrossEntropy和 logits 之间的MSE，<br>最后实验验证MSE效果优于CE。<br>此外，由于是从头开始训练Student，所以只用任务相关数据会严重样本不足，所以作者提出了三种NLP的任务无关的data augment策略：<br>1.mask：随机mask一部分token作为新样本，让teacher去生成对应logits<br>2.根据POS标签去替换，得到 ”What do pigs eat?” -&gt; “ How do pigs ear?”<br>3.n-gram采样：随机选取n-gram，n取[1-5]，丢弃其余部分<br>在<a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>中曾指出 logits 之间的CrossEntropy是可以看作<br>是MSE 的近似版本，不过这里作者的结论是MSE 更好，此外，由于Hinton 实验时是巨大数据量，所以不存在样本不足的情况，而普通实验时都会遇到<br>迁移时训练样本不足，需要做数据增强的问题。</p><h2><span id="tinybert">TinyBERT</span><a href="#tinybert" class="header-anchor"></a></h2><p>TinyBERT 出自<a href="http://arxiv.org/abs/1909.10351" target="_blank" rel="noopener">TinyBERT: Distilling BERT for Natural Language Understanding</a>,由于Transformer 结构<br>在NLP 任务中的强大能力，作者选择用与BERT 同结构的方式做Student，此外，为了提高KD后模型性能，做了更细致的工作：<br>1.Student选择一个更窄更浅的transformer;<br>2.将KD也分为两个阶段：pre-train 和 fine-tuning，并且在两个阶段上都进行KD;<br>3.使用了更多的loss：Embedding之间的MSE，Attention Matrix中的logits之间的MSE，Hidden state之间的MSE以及最后的分类层的CE;<br>4.为了提高下游任务fine-tuning后的性能，使用了近义词替换的策略进行数据增强.</p><h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><p>1.6层transformer基本达到了bert-12的性能，并且hidden size更小，实际是比bert-6更小的;<br>2.因为有pre-train KD，所以可以拿来当bert 一样直接在下游fine-tuning.</p><h3><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h3><p>1.由于hidden size的不同，所以为了进行MSE，需要用一个参数矩阵W 来调节，这个参数只在训练时使用，训练完后丢弃，这个矩阵没有任何约束，觉得不优雅;<br>2.其次，student model的每一层都需要去学习teacher model的对应的block的输出，如何对不同的层如何设计更好的权重也是一个费力的事；<br>3.虽然student的结构也是transformer，但是由于hidden size 不同，没法使用teacher的预训练结果，但是我觉得这里其实可以用降维的方式用<br>teacher的预训练结果，可能不需要pretraining的阶段了也说不定。</p><h2><span id="distilbert">DistilBERT</span><a href="#distilbert" class="header-anchor"></a></h2><p>DistilBERT 出自<a href="http://arxiv.org/abs/1910.01108" target="_blank" rel="noopener">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>,<br>论文中作者通过调查发现BERT 中的hidden size 的对计算效率的改变比hidden layer nums 的影响小，说白了就是让模型变矮比让模型变瘦效率更高，<br>所以作者使用了一个更矮的BERT来做Student 来迁移BERT 中的知识。由于DistilBERT 是一个与BERT 同结构只是层数更小，所以DistilBERT 可以用BERT 的<br>预训练的权重进行初始化，此外，DistilBERT 是一个与任务无关的模型，即与BERT 一样，可以对很多下游任务进行fine-tuning。<br>由于DistilBERT 与 BERT 的前几层一致，所以loss 的选择上就更多一些，<br>作者选择了triple loss：MLM loss + embedding cosin loss + soften labels cross entropy </p><h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><p>DistilBERT 做到了与BERT 一样，完全与任务无关，不需要添加额外的Distillation 阶段（添加后结果会更好）</p><h2><span id="mobilebert">MobileBERT</span><a href="#mobilebert" class="header-anchor"></a></h2><p>MobileBERT 出自<a href="http://arxiv.org/abs/2004.02984" target="_blank" rel="noopener">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a>,<br>作者同样采用一个transformer 作为基本结构，但作者认为深度很重要，宽度较小对模型损坏较小，所以整体架构是保持模型深度不变，<br>通过一个矩阵来改变feature size，即bottleneck，在通过在block的前后插入两个bottleneck，来scale feature size。<br>由于MobileBERT太窄太深，所以不好训练，作者提出新的方式，通过一个同深但是更宽的同架构的模型来训练 作为teacher，然后用MobileBERT迁移。<br>loss 设计上主要包括三部分：feature map之间的MSE，Attention logits之间的KL，以及pre-training MLM + pre-training-NSP + pre-training-KD<br>训练策略上，有三种方式：<br>1.将KD作为附加预训练的附加任务，即一起训练；<br>2.分层训练，每次训练一层，同时冻结之前的层；<br>3.分开训练，首先训练迁移，然后单独进行pre-training<br>此外，为了提高推理速度，将gelu 替换为更快的 relu ，LayerNormalization 替换为 更简单的NoNorm，也做了量化的实验。</p><h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><p>1.首先mobileBERT容量更小，推理更快，与任务无关，可以当bert来直接在下游fine-tuning，而之前的KD大多数时候需要与任务绑定并使用数据<br>增强，才能达到不错的性能；<br>2.论文实验非常详实，包括如何选择inter-block size, intra-block size, 不同训练策略如何影响等<br>3.训练策略上，除了之前的一起训练完，实验了两种新的训练方式，而最终的一层一层的训练与skip connection 有异曲同工的作用：每层都学一小部分<br>内容，从而降低学习的难度；<br>4.替换了gelu 和 LayerNormalization,进一步提速</p><h3><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h3><ol><li>要训练一个IBBERT作为teacher，而这个模型容量与BERT-Large差不多，增加了训练难度</li></ol><h1><span id="lun-wen-zong-jie">论文总结</span><a href="#lun-wen-zong-jie" class="header-anchor"></a></h1><p>可以看到，以上论文在针对BERT 做Distillation 时，主要的思路是利用一个容量大的Teacher 模型来先学习，然后再用一个参数量小的Student 模型<br>来迁移大模型学到的知识，在迁移时，除了最后分类层的logits 外，还可以进一步的迁移Teacher 中各个layer 的output ，而Student 的选择上，<br>除了自定义外，还可以选择跟Teacher 同结构，而为了降低参数量，可以选择将模型变矮/变窄/减小hidden size 等方式。而为了蒸馏后的模型能更加的<br>general，适应更多的task，就需要迁移更多的信息，设计上也越复杂。</p><h1><span id="xiang-fa">想法</span><a href="#xiang-fa" class="header-anchor"></a></h1><p>实际工作上，大多数时候我们都是需要一个task 来做模型，而以上论文中告诉我们，迁移的信息越多，Student 的性能越好。而针对具体task ，我觉得<br>比较简洁有效的一种方式是采用更矮的Teacher 来作为Student ，这样可以直接将Teacher 中的前几层的信息完全迁移过来，然后在object 上，<br>加入迁移Teacher 在train data 上的logits ，这样就可以比较有效的进行蒸馏了。<br>除此之外，让我们换个角度看看为什么logits 能增强Student 模型的性能呢？除了迁移的角度外，其实logits 提供了label<br>更多的信息（不同类别的相对关系），而这个额外信息只要优于随机分布，就能对模型提供更多的约束信息，从而增强模型性能，即当前的模型可以看作是<br>分别拟合ground truth 和 logits的两个模型的<code>ensemble</code>，只不过是两个模型共享参数。<br>上面我们提到只要logits 优于随机，对Student 模型来说就会有所提升，那logits 由谁产生的其实并不重要。所以，我们除了可以用Teacher 产生的<br>logits来增强Student 模型外，我们还可以增强Teacher 模型，或者直接用Student 先学习一下，产生logits，再用Student 去迁移上次产生的logits。<br>想到这里，我不禁的有个大胆的想法：<code>既然我可以一边生成logits， 一边学习logits，那我不是可以持续这个过程，直到模型完全拟合train data，<br>生成的logits退化为one-hot，那此时的模型是不是能得到一个非常大的提升呢？</code></p><h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>实验的基本设置是用12层bert 作为Teacher model ，用3层bert 作为Student model 。soften labels 采用Temperature 平滑后的结果，此外，<br>Student model 除了学习 soften labels 的外，也需要学习ground truth。</p><h2><span id="teacher-to-student">Teacher-to-Student</span><a href="#teacher-to-student" class="header-anchor"></a></h2><p>Teacher model 在train data 上训练，然后在train data 上生成对应的soften labels，Student model 学习ground truth 和 soften labels </p><h2><span id="student-to-student">student-to-student</span><a href="#student-to-student" class="header-anchor"></a></h2><p>既然soften labels 是一种对labels 的一种平滑估计，那我们可以用任何方式去估计他，所以这里我们就用student 去做一个估计：<br>student model 在train data 上进行训练，然后在train data 上生成对应的soften labels ，将 student model 利用bert 预训练结果重新初始化，<br>然后去学习ground truth 和 soften label</p><h2><span id="normal-noise-training">normal-noise-training</span><a href="#normal-noise-training" class="header-anchor"></a></h2><p>既然是对labels 的一个估计，那假如给一个随机的估计，只要保证生成的logits 中true label 对应的值最大，就能对Student 模型进行一定程度的提升：<br>直接在train label 上添加一个normal noise ，然后重新进行平滑后归一，作为soften labels让student model 去学习。</p><h1><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h1><p>$$<br>\begin{array}{c|c|c}<br>\hline \\<br>\text{teacher standalone} &amp; \text{student standalone} &amp; \text{teacher-to-student} &amp; \text{teacher-to-teacher} &amp; \text{student-to-student} &amp; \text{normal-noise-student}\\<br>\hline \\<br>60.21\% &amp; 58.14\% &amp; 60.14\% &amp; 60.14\% &amp; 61.07\% &amp; 59.5\%<br>\end{array}<br>$$</p><p>从结果中可以看到：<br>1.优于随机的logits 对Student 模型有一定的提升，估计越准确，提升越高；<br>2.越大的模型性能越好<br>3.迭代进行logits 的生成与训练不能进一步提高模型性能，原因主要是新的logits 分布相比之前的对模型的提升非常小，此外这个分布也比较容易拟<br>合，所以无法进一步提升。<br>完整实验代码地址<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/distilling_knowledge_bert.py" target="_blank" rel="noopener">distilling_knowledge_bert</a></p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文主要针对目前针对BERT 的知识蒸馏进行了总结，并提出了针对具体任务时可行的简洁方案，同时在新的视角下探讨了知识蒸馏有效的一些原因，<br>并通过实验进行了验证，发表顺序上上篇<a href="https://xv44586.github.io/2020/08/09/bert-of-theseus/">bert-of-theseus</a> 更晚一些，有兴趣的可以再去看一下上一篇。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>芝麻街</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Programming" scheme="https://xv44586.github.io/categories/Programming/"/>
    
    
      <category term="Distillation" scheme="https://xv44586.github.io/tags/Distillation/"/>
    
      <category term="BERT" scheme="https://xv44586.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>qa-augmentation</title>
    <link href="https://xv44586.github.io/2020/08/22/qa-augmentation/"/>
    <id>https://xv44586.github.io/2020/08/22/qa-augmentation/</id>
    <published>2020-08-22T01:38:13.000Z</published>
    <updated>2020-08-28T10:04:41.231Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#bei-jing">背景</a></li><li><a href="#unilm">UniLM</a></li><li><a href="#shu-ju-zeng-qiang">数据增强</a></li><li><a href="#shi-yan">实验</a><ul><li><a href="#wen-ti-sheng-cheng">问题生成</a></li><li><a href="#wen-ti-da-an-dui-sheng-cheng">问题答案对生成</a></li></ul></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="bei-jing">背景</span><a href="#bei-jing" class="header-anchor"></a></h1><p>上周打算把UniLM在<a href="https://github.com/xv44586/toolkit4nlp" target="_blank" rel="noopener">toolkit4nlp</a>的基础上实现一下，又刷了一遍<a href="https://arxiv.org/pdf/1905.03197.pdf" target="_blank" rel="noopener">论文</a>,发现作者提到用UniLM做问题生成，来增强QA任务的性能，觉得很有意思，所以想尝试一下。</p><h1><span id="unilm">UniLM</span><a href="#unilm" class="header-anchor"></a></h1><p>因为这篇 UniLM 是主角，所以简单介绍一下该模型。该模型是通过灵活使用 attention mask ，将 NLG 与 NLU 任务统一在来一起，所以叫 unified LM，<br>他的做法是将 left-to-right/right-to-left/masked lm/seq2seq lm/放在一个框架里训练，从而让模型兼具 NLU 与 NLG 的能力。<br><img src="/2020/08/22/qa-augmentation/lm.png" alt><br>而为了达到这个训练，只需要在 bert 的基础上根据不同的 lm 调整 attention mask 即可。所以利用 bert 做 NLG 时，只需要调整 attention mask<br>为 seq2seq lm 对应mask即可。</p><h1><span id="shu-ju-zeng-qiang">数据增强</span><a href="#shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>通常增强都是同义词/近义词替换，subsequence的随机删除/重复/互换等，我之前在做百度比赛时尝试过随机删除和随机两个片段互换位置，提升不是<br>非常大而论文里大问题生成带来大提升还是相当大的：<br><img src="/2020/08/22/qa-augmentation/table9.png" alt><br>仔细想一下，由于attention机制，互换只是改变了position embedding部分内容，而这部分的互换对模型的影响是很弱的；随机删除可能会破坏语义，<br>所以增加模型robust的同时可能会降低模型性能。而问题生成，则可以看作是同义词/近义词替换的句子级别替换，所以理论上能带来不错的提升。<br>从对抗的角度来看，生成的问题在语义上与原问题基本一致，这也正好符合<code>输入的微小改变</code>，从而让模型在这种带有微小扰动的前提下仍然能很好的预测。</p><h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>既然UniLM具有很强的NLG能力能力，那就有很多不同的玩法。首先，可以训练一个模型，来针对 context 和 answer 生成对应的问题，来对问题进行<br><code>“换个问法”</code>，其次，既然可以对问题<cdoe>“换个问法”,自然也可以<code>“换个问题”</code>,也就是根据 context 生成新的问题<br>和答案。另外，由于是扩增训练数据，所以有一个技巧是做生成是将 train data 与 dev data 互换，不过由于我用的是百度比赛数据，dev data 太少，<br>所以我是 train + dev。</cdoe></p><h2><span id="wen-ti-sheng-cheng">问题生成</span><a href="#wen-ti-sheng-cheng" class="header-anchor"></a></h2><p>问题生成时，就是将 context 与 answer 拼接，然后生成对应的question。具体样本形如：<code> [CLS] answer + context [SEP] question [SEP]</code> .<br>模型直接用bert base权重按UniLM的seq2seq方式来构建，可以看到效果还是很不错的，比如：</p><blockquote><br>context：报雅思或者托付培训班,一般情况下要900元左右。 雅思和托福考试可以自学: 一、基础知识准备:单词、基本语法、长难句分析; 二、板块训练:听说读写,四个板块; 三、合理备考计划,可以参见中国上别人经验结合自己的自身条件; 四、效果强化跟踪,使用合理的备考软件或者是自测题目随时跟踪自己的学习状态<br>question：雅思班价格<br>answer: [‘900元’, ‘900元左右’]<br>generate question: 雅思班报名多少钱<br></blockquote><blockquote>context：USB电压有5伏 USB一般4根线, 中间两根是数据线, 左右各为 +- 线 只要不短路是不会烧主板该插口的 ,我想你应该这样做,手机的线的一端直接插入手提电脑,另一头剪掉头子,从线中分离出四根线, 用万用表测出(红色+和其它色如黑-)剩下两根用胶布包扎(不用)然后 在这两根线上(正电极中最好串一50到100欧电阻)后接入一支高亮度发光二极管就成功了.<br>question：usb线电压<br>answer: [‘5伏’]<br>generate question: usb线电压 </blockquote><p>解码时，有两种选择：随机抽样与 beam search 。随机抽样可以增加问题的多样性，并且可以生成多个问题；beam search近似最优，得到一个最优的<br>问题。由于我们是使用 train data 训练模型，在对 train data 生成新的问题时，beam search 将可能产生很多一摸一样的问题，这样将降低新增<br>数据的量；而随机抽样能产生很多新的问题，但可能新生成的问题与答案并不配套，还需要一些后处理之后才能真正拿来用。这里两种方式都拿来做实验，<br>并对生成的问题做一个简单的过滤：新生成的问题与原问题中有70%以上的字是重合的。<br>$$<br>\begin{array}{c|c|c}<br>\hline \\<br>\text{base line} &amp; \text{beam search} &amp; \text{random sample}\\<br>\hline \\<br>80.39\% &amp; 81.0\% &amp; 79.8\%<br>\end{array}<br>$$</p><p>random sample的样本经过了很多次过滤之后才能基本达到baseline的效果，所以生成的问题如果”问非所答”，对最终的效果反而是不好的，这也符合预期。</p><h2><span id="wen-ti-da-an-dui-sheng-cheng">问题答案对生成</span><a href="#wen-ti-da-an-dui-sheng-cheng" class="header-anchor"></a></h2><p>问题答案对生成时，由于答案是在 context 内的，相对问题生成简单一些，所以我们先生成答案，再根据 context 和生成的 answer 来生成对应的<br>question。不过为了让问题答案对更丰富多样，解码答案时我们采用随机抽样，而生成问题时，为了让问题尽量准确，我们采用 beam search。<br>样本形如 <code>[CLS]context[SEP]answer[SEP][question][SEP]</code>，生成的效果如下：</p><p><blockquote><br>context：您好，孕妇整个孕期体重增加12.5公斤左右，在增加的这12.5公斤中，胎儿的体重占3公斤左右，胎盘和羊水约是2公斤左右。在孕早期（怀孕3个月以内）增加2公斤，中期（怀孕3－6个月）以及末期（怀孕7－9个月）各增加5公斤左右。所以怀孕6个月体重增加7公斤左右比较正常。希望我的回答对你有所帮助。<br>question：孕妇6个月体重增加多少<br>answer: 7公斤左右<br>generate question: 孕妇6个月体重增加多少<br>generate answer: 12.5公斤左右<br></blockquote><br>不过也由于train data 参与训练，所以很多生成的问题答案对与原始问题答案对一致，如果有更多的外部数据，可以利用外部数据来训练。<br>$$<br>\begin{array}{c|c|c|c}<br>\hline \\<br>\text{base line} &amp; \text{beam search} &amp; \text{random sample} &amp; \text{question answer generation}\\<br>\hline \\<br>80.39\% &amp; 81.0\% &amp; 79.8\% &amp; 81.76\% \\<br>\hline<br>\end{array}<br>$$</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>通过生成新的问题与新的问题答案对能在一定程度上提高qa 任务的性能，在生成问题时，用beam search 得到的新问题虽然量少但由于更准确，所以<br>能带来一定的提升；用随机采样生成的问题会有部分与答案无关的或者语义有点不通顺的问题，所以可能反而会导致性能降低；问题答案对的生成时，<br>先生成相对简单的回答再生成对应问题，能对性能带来不错的提升，在做qa相关任务时，可以尝试使用一下。<br>实验代码：<br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/qa_baseline.py" target="_blank" rel="noopener">qa_baseline</a><br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/qa_question_generation_seq2seq.py" target="_blank" rel="noopener">qa_question_generation_seq2seq</a><br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/qa_question_answer_generation_seq2seq.py" target="_blank" rel="noopener">qa_question_answer_generation_seq2seq</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>看瓜的怒气小猫</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Programming" scheme="https://xv44586.github.io/categories/Programming/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
      <category term="QA" scheme="https://xv44586.github.io/tags/QA/"/>
    
      <category term="UniLM" scheme="https://xv44586.github.io/tags/UniLM/"/>
    
  </entry>
  
  <entry>
    <title>bert-of-theseus-2</title>
    <link href="https://xv44586.github.io/2020/08/19/bert-of-theseus-2/"/>
    <id>https://xv44586.github.io/2020/08/19/bert-of-theseus-2/</id>
    <published>2020-08-19T14:12:28.000Z</published>
    <updated>2020-08-19T15:38:53.360Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#fu-xian-shi-de-wen-ti">复现时的问题</a><ul><li><a href="#si-kao-1-wei-shi-me-shi-xiao">思考1：为什么失效</a></li><li><a href="#si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me">思考二 ：bert-of–theseus有效的本质是什么</a></li><li><a href="#shi-yan-1">实验1</a></li><li><a href="#si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing">思考三：直接在predecessor 上抽successor行不行？</a></li><li><a href="#shi-yan-2">实验2</a></li></ul></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>上一篇中介绍了bert-of-theseus论文的主要思路，并贴了两组实验的结论，这篇是对上篇的后续一些思考与实验。</p><h1><span id="fu-xian-shi-de-wen-ti">复现时的问题</span><a href="#fu-xian-shi-de-wen-ti" class="header-anchor"></a></h1><p>在复现时，遇到最大的问题就是结果不稳定。首先每次训练predecessor时，其最优结果就会有上下1个点左右的波动，而因为theseus 中引入了随机数<br>来概率替换对应block，所以结果上一言难尽，有时能比12层bert低0.6个点, 有时只能达到直接3层fine tuning 的效果，于是我做了些观察与思考。</p><h2><span id="si-kao-1-wei-shi-me-shi-xiao">思考1：为什么失效</span><a href="#si-kao-1-wei-shi-me-shi-xiao" class="header-anchor"></a></h2><p>在训练theseus model时，其中抽出的successor在每个epoch结束后在验证集上的结果有时会很高，基本到达只比三层fine-tuning低6个点，有时又很<br>低，基本不到0.1%, 第一种明显是successor在theseus中训练太多，以至于接近直接fine tuning，而另一种情况下可能是successor训练不充足，<br>也可能是替换次数太少导致没有被训练，而且大多数情况下successor的验证集上都是不到0.1%。<br>为了验证第二种情况下是否是未替换导致successor在做fine tuning，我将successor进行单独fine tuning后,将得到的classifier 拼回predecessor，<br>发现此时在验证集上d结果只下降了2个点，所以此时大概率是替换次数过少，基本没有训练到successor，所以导致结果不好，而这里开始我以为是我<br>实现问题，后来来来回回检查了一周，也没发现问题，于是我就想换一种更稳定的方式。</p><h2><span id="si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me">思考二 ：bert-of–theseus有效的本质是什么</span><a href="#si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me" class="header-anchor"></a></h2><p>熟悉bert的同学肯定对warm up不陌生，而warm up之所以有效，我认为比较重要的一点是如果在最初的steps中，模型提前拟合了样本，进入了一个局部<br>最优区域，后期无论你怎么迭代他都跳不出来，而由已经<code>fine tuned predecessor</code>带着一起再进行训练，也和warm up有些相似，即用小的<br>步子带着你朝着更优的方向走几步，跳出来，让你有进入更好的局部最优点的可能，此外，概率替换的思路也与<code>Dropout</code>有几分相似，让successor<br>有一定的几率参与训练，从而让successor在缺少predecessor的情况下也有一定的robust。<br><a href="https://spaces.ac.cn/archives/7575" target="_blank" rel="noopener">苏剑林的博客</a>里也提到了替换的数学形式：<br>$$<br>\begin{equation}\begin{aligned}<br>&amp;\varepsilon^{(l)}\sim U(\{0, 1\})\\<br>&amp;x^{(l)} = x_p^{(l)} \times \varepsilon^{(l)} + x_s^{(l)} \times \left(1 - \varepsilon^{(l)}\right)\\<br>&amp;x_p^{(l+1)} = F_p^{(l+1)}\left(x^{(l)}\right)\\<br>&amp;x_s^{(l+1)} = F_s^{(l+1)}\left(x^{(l)}\right)<br>\end{aligned}\end{equation}<br>$$<br>同时，他也提到$\epsilon$能否不取非0即1，那既然我们是想让successor在task方向上warm up一下，那直接相加，即此时 $\epsilon = k$,<br>k是常数也是可以的。此时只要调节k 就能避免successor训练不充分或太充分的情况了，模型也就稳定了，可以满足我们的要求了。</p><h2><span id="shi-yan-1">实验1</span><a href="#shi-yan-1" class="header-anchor"></a></h2><p>实验代码其实比较容易修改，只需将BinaryRandomChoice 层替换为相加即可。具体代码在<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py" target="_blank" rel="noopener">classification_ifytek_bert_of_theseus</a><br>中可以看到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProportionalAdd</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""将两层的结果乘比例后相加，output = (input_1 * proportion + input_2 * (1 - proportion)) / 2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, proportion=<span class="number">0.5</span>, **kwargs)</span>:</span></span><br><span class="line">        super(ProportionalAdd, self).__init__(**kwargs)</span><br><span class="line">        self.supports_masking = <span class="literal">True</span></span><br><span class="line">        self.proportion = proportion</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_mask</span><span class="params">(self, inputs, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> mask[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        source, target = inputs</span><br><span class="line">        source = source * self.proportion</span><br><span class="line">        target = target * (<span class="number">1</span> - self.proportion)</span><br><span class="line">        output = (source + target)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> K.in_train_phase(output, target)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> input_shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>文本分类：CLUE的iflytek数据集</p><p>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \\text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 60.11\% &amp; 58.99\%  &amp; 57.96\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 59.7\%  &amp; 59.5\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p><p>结果上看确实更稳定了，也更好一点点了，基本比predecessor低<code>0.5%~1%</code> .</p><h2><span id="si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing">思考三：直接在predecessor 上抽successor行不行？</span><a href="#si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing" class="header-anchor"></a></h2><p>既然我们说bert-of-theseus有效的原因是在task 的方向进行了warm up，那predecessor已经在task上fine tuned了，能不能<code>直接抽取某几<br>层作为successor来直接fine tuning?</code>此外，之前我们也说了，predecessor与successor的classifer差距很小，那我们能不能改变successor<br>的classifer的学习率，让他进一步学习，来弥补一部分前三层无法拟合的分布呢？</p><h2><span id="shi-yan-2">实验2</span><a href="#shi-yan-2" class="header-anchor"></a></h2><p>具体实验代码<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/two_stage_fine_tuning.py" target="_blank" rel="noopener">two-stage-fine-tuning</a><br>实验时尝试了<code>随机初始化classifier/predecessor classifier初始化classifier/ 放大classifier lr</code>组合策略，最后的结果就不贴了，基本都没有<br>超过3层bert fine tuning的效果。</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>尝试分析了bert-of-theseus复现中的问题，并尝试了一些修复方案，同时，实验测试了theseus model的必要性，最后结论是binary random choice<br>策略不如 proportion add 策略稳定，同时，theseus是必须的。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://github.com/JetRunner/BERT-of-Theseus" target="_blank" rel="noopener">论文原作者配图</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Programming" scheme="https://xv44586.github.io/categories/Programming/"/>
    
    
      <category term="Distillation" scheme="https://xv44586.github.io/tags/Distillation/"/>
    
  </entry>
  
  <entry>
    <title>bert-of-theseus</title>
    <link href="https://xv44586.github.io/2020/08/09/bert-of-theseus/"/>
    <id>https://xv44586.github.io/2020/08/09/bert-of-theseus/</id>
    <published>2020-08-09T13:26:26.000Z</published>
    <updated>2020-08-28T07:40:33.723Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#mo-xing-ya-suo">模型压缩</a><ul><li><a href="#jian-zhi">剪枝</a></li><li><a href="#liang-hua">量化</a></li><li><a href="#zhi-shi-zheng-liu">知识蒸馏</a></li><li><a href="#quan-chong-gong-xiang">权重共享</a></li><li><a href="#quan-chong-fen-jie">权重分解</a></li><li><a href="#mo-xing-ya-suo-de-bi-yao-xing">模型压缩的必要性</a></li><li><a href="#bert-of-theseus">Bert of theseus</a></li><li><a href="#ju-ti-liu-cheng">具体流程</a></li><li><a href="#shi-yan-xiao-guo">实验效果</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><blockquote>如果忒修斯的船上的木头被逐渐替换，直到所有的木头都不是原来的木头，那这艘船还是原来的那艘船吗？<br><br>-普鲁塔克</blockquote><p>最近遇到一个需要对算法加速的场景，了解到了一个比较简洁实用的方法：<a href="https://arxiv.org/abs/2002.02925" target="_blank" rel="noopener">Bert-of-theseus</a>,<br>了解了原理后参考代码实验后，验证了其有效性，所以总结一下。</p><h1><span id="mo-xing-ya-suo">模型压缩</span><a href="#mo-xing-ya-suo" class="header-anchor"></a></h1><p>模型在设计之初都是过参数化的，这是因为模型的参数量与复杂度代表着模型的容量与学习能力，但当我们实际使用时，我们需要更好的部署他（低资源），更快的响应（快速推理），常常需要进行模型压缩。<br>模型压缩就是<code>简化大的模型，得到推理快资源占用低的小模型</code>，而想”即要马而跑又不用吃草”通常是很难的，所以压缩后的模型常常也会有不同程度的牺牲，如模型性能下降。<br>此外，模型压缩是作用在推理阶段，带来的常常是训练时间的增加。<br>模型压缩又分为两种方式：一种是<code>剪枝(Pruning)</code>与<code>量化(Quantization)</code>,一种是<code>知识蒸馏(Knowledge Distillation)</code>,<br>还有一种是<code>权重共享（Sharing）与因数分解（Factorization）</code>。该部分内容推荐一篇博客：<a href="http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html" target="_blank" rel="noopener">All The Ways You Can Compress BERT</a></p><h2><span id="jian-zhi">剪枝</span><a href="#jian-zhi" class="header-anchor"></a></h2><p>剪枝技术是通过将大模型中一些”不重要”的连接剪断，得到一个”稀疏”结构的模型。剪枝又分为”结构性剪枝”与”非结构性剪枝”.剪枝可以作用在权重粒度，<br>也可以作用在attention heads / layer粒度上。不过剪枝技术感觉会逐步被<cdoe>NAS（Neural Architecture Search）取。</cdoe></p><h2><span id="liang-hua">量化</span><a href="#liang-hua" class="header-anchor"></a></h2><p>量化不改变模型的网络结构，而是改变模型的参数的数据格式，通常模型在建立与训练时使用的是 float32 格式的，量化就是将格式转换为 <code>low-bit</code>, 如 float16 甚至二值化，如此即提速又省显存。</p><h2><span id="zhi-shi-zheng-liu">知识蒸馏</span><a href="#zhi-shi-zheng-liu" class="header-anchor"></a></h2><p>知识蒸馏是训练一个小模型(student)来学习大模型(teacher)，由于大模型是之前已经fine-tuning的，所以此时学习的目标已经转换为对应的logit而<br>不再是one-hot编码了，所以student有可能比teacher的性能更好。这样即小又准的模型实在太好了。不过为了达到这样的效果，通常设计小模型时不<br>光要学习大模型的输出，还要学习各个中间层结果，相关矩阵等，这就需要仔细设计模型的结构与loss及loss融合方案了。一种简单的方法是只学习大模型的logit，这与对label做embedding有点类似，不过我没做过实验还。</p><h2><span id="quan-chong-gong-xiang">权重共享</span><a href="#quan-chong-gong-xiang" class="header-anchor"></a></h2><p>将部分权重在多个层中共享以达到压缩模型的效果，如ALBERT中共享self-attention中的参数</p><h2><span id="quan-chong-fen-jie">权重分解</span><a href="#quan-chong-fen-jie" class="header-anchor"></a></h2><p>将权重矩阵进行因数分解，形成两个低秩的矩阵相乘的形式，从而降低计算量</p><h2><span id="mo-xing-ya-suo-de-bi-yao-xing">模型压缩的必要性</span><a href="#mo-xing-ya-suo-de-bi-yao-xing" class="header-anchor"></a></h2><p>看了上面模型压缩的方法，每一个都有种”脱裤子放屁”的感觉，与其训练一个大模型，再费力把它变小，为何不直接开始就弄个小的呢？<br>首先，模型在设计之初是都是会或多或少的过参数化，因为模型的参数量与复杂度代表着模型的容量与学习能力；<br>其次，开始就用一个小模型，那这个小模型也是需要设计的，不能随便拿来一个，而设计一个性能高参数规模小的小模型难度是非常大的，往往是模型小了性能也低了；<br>第三点，大模型压缩后与小模型虽然参数规模相当，但是对应的模型空间并不相同<br>此外，为了更好的部署，如手机或FPGA等，得到精度更高模型更小(distillation)或者利用硬件加速(low-bit)，模型压缩都是值得试一试的手段。<br>更详细的讨论，可以参考<a href="https://www.zhihu.com/question/303922732" target="_blank" rel="noopener">为什么要压缩模型，而不直接训练一个小的CNN</a></p><h2><span id="bert-of-theseus">Bert of theseus</span><a href="#bert-of-theseus" class="header-anchor"></a></h2><p>Bert of theseus 方法属于上面提到的知识蒸馏，知识蒸馏中我们提到，在蒸馏时，我们不光要学习teacher的输出，对中间层我们也希望他们直接尽量相似，<br>那想象一个这种状态对应对理想情况：<code>中间层的结果一致，最终的结果一致</code>,既然我们的期望中间结果一致，那也就意味着两者可以互相替换。<br>正如开头提到的忒修斯之船一样。所以核心思想是：<br><code>与其设计复杂的loss来让中间层结果相似不如直接用小模型替换大模型来训练</code><br>通过复杂loss来达到与中间层结果相似可以看作是一种整体渐进式的逼近，让小模型一点点去学习，而直接替换可以看作是一种简单粗暴的方式，<br>但是他不需要设计各种loss，优化目标也是同一个，就只有一个下游任务相关的loss，突出一个<code>简洁</code>。<br>这就好比高中上学一样，即使花高价也要让孩子去一所好高中，因为学校的”氛围”能让孩子的学习成绩进步，其实是因为周围的孩子带着一起学，<br>弱鸡也能学的比平时更多一点。bert-of-theseus也是类似的道理，跟着大佬（teacher）总比单独fine-tuning效果好。</p><h2><span id="ju-ti-liu-cheng">具体流程</span><a href="#ju-ti-liu-cheng" class="header-anchor"></a></h2><p>如果直接将小模型替换大模型，那其实是在对小模型进行微调，与大模型就脱离了，也达不到对应的效果，所以作者采用了一种概率替换的方式。<br>首先呢，想象我们现在已经训练好了一个6层的BERT，我们成为<code>Predecessor（前辈）</code>, 而我们需要训练一个三层的bert，<br>他的结果近似12层BERT的效果，我们成为<code>Successor(传承者)</code>,那 bert-of-theseus的模型结构如<a href="https://spaces.ac.cn/archives/7575" target="_blank" rel="noopener">下图</a>所示：</p><p><img src="/2020/08/09/bert-of-theseus/bert-of-theseus.png" alt="bert-of-theseus"></p><p>在bert-of-theseus中，首先固定predecessor的权重，然后将6层的Bert分为3个block，每个block与successor的一层对应，训练过程分为两个stage：<br>首先用successor中的层概率替换predecessor中对应的block，在下游任务中直接fine-tuning（只训练successor），<br>然后将successor从bert-of-theseus中分离出来，单独在下游任务中进行fine-tuning，直到指标不再上升。<br>所谓替换，就是输出的替换，在进入下一层前在predecessor和successor的输出中二选一。<br>替换概率作者也给出了两种方式，一种是固定 0.5,一种是线性从0-1,如下图所示：<br><img src="/2020/08/09/bert-of-theseus/figure2.png" alt></p><h2><span id="shi-yan-xiao-guo">实验效果</span><a href="#shi-yan-xiao-guo" class="header-anchor"></a></h2><p>实验代码主要参考<a href="https://github.com/bojone/bert-of-theseus" target="_blank" rel="noopener">bert-of-theseus</a>, 实验主要做了三组，一组文本分类两组ner-crf，结果如下：</p><p>文本分类：CLUE的iflytek数据集</p><p>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \\text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 60.11\% &amp; 58.99\%  &amp; 57.96\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 59.6\%  &amp; 59.3\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p><p>ner-crf: 公司数据<br>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \\text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 97.5\% &amp; 97.0\%  &amp; 96.1\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 97.3\%  &amp; 96.6\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p><p>可以看到，相比直接那前几层微调，bert-of-theseus的效果确实更好，此外，我还尝试了线性策略的替换概率，效果上差别不大。<br>实验代码：<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py" target="_blank" rel="noopener">classification_ifytek_bert_of_theseus</a><br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/sequence_labeling_ner_bert_of_theseus.py" target="_blank" rel="noopener">sequence_labeling_ner_bert_of_theseus</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://github.com/JetRunner/BERT-of-Theseus" target="_blank" rel="noopener">论文原作者配图</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Programming" scheme="https://xv44586.github.io/categories/Programming/"/>
    
    
      <category term="Distillation" scheme="https://xv44586.github.io/tags/Distillation/"/>
    
  </entry>
  
  <entry>
    <title>optimizer of bert</title>
    <link href="https://xv44586.github.io/2020/08/01/optimizer-in-bert/"/>
    <id>https://xv44586.github.io/2020/08/01/optimizer-in-bert/</id>
    <published>2020-08-01T12:48:48.000Z</published>
    <updated>2020-08-09T03:31:24.208Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#zheng-ti-you-hua-fang-an">整体优化方案</a></li><li><a href="#adam-in-bert">Adam in bert</a><ul><li><a href="#weight-decay">weight decay</a><ul><li><a href="#weight-decay-1">weight decay</a></li><li><a href="#l2-regularization">L2 regularization</a></li></ul></li></ul></li><li><a href="#learning-rate">Learning rate</a><ul><li><a href="#learning-rate-decay">Learning rate decay</a></li><li><a href="#warmup">warmup</a></li></ul></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>最近尝试实现了 bert ,在最后 pretraining 是发现 bert 中的优化方法比较有趣，所以记录一下自己的理解。</p><h1><span id="zheng-ti-you-hua-fang-an">整体优化方案</span><a href="#zheng-ti-you-hua-fang-an" class="header-anchor"></a></h1><p>bert中的优化方案可以总结为：线性分段学习率 + weight decay Adam</p><h1><span id="adam-in-bert">Adam in bert</span><a href="#adam-in-bert" class="header-anchor"></a></h1><p>首先简单回忆一下 Adam Optimizer：<br>整体框架：<br>$$<br>g_{t}=\bigtriangledown f(w_{t})<br>$$<br>$$<br>m_{t}=\Phi (g_{1},g_{2},…,g_{t})<br>$$<br>$$<br>v_{t}=\Psi (g_{1},g_{2},…,g_{t})<br>$$<br>$$<br>\eta =\alpha \cdot m_{t}/\sqrt{V_{t}}<br>$$<br>$$<br>\omega_{t+1}=\omega_{t}-\eta_{t}<br>$$</p><p>其中一阶动量 m 与二阶动量 v 的计算方式：<br>$$<br>m_{t}=\beta_{1}m_{t-1} + (1-\beta_{1})\cdot g_{t}<br>$$<br>$$<br>v_{t}=\beta_{2}v_{t-1} + (1-\beta_{2})\cdot g_{t}^{2}<br>$$<br>参数一般取值：ß1=0.9，ß2=0.999<br>而也是这个原因，初期对一阶动量与二阶动量v的估算都偏小，会导致优化方向朝着 0 走，所以，一般会进行一个修正（bias correct），方式是：<br>$$<br>\hat{m_{t}}=m_{t}/1-{\beta_{1}}^{t}<br>$$<br>$$<br>\hat{v_{t}}=v_{t}/1-{\beta_{2}}^{t}<br>$$<br>而 bert 中实现都 Adam 却没有进行这个修正，至于原因，放在下面一起说。</p><h2><span id="weight-decay">weight decay</span><a href="#weight-decay" class="header-anchor"></a></h2><p>在 bert 中对 Adam 进行了weight decay，具体代码上是这一段：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Just adding the square of the weights to the loss function is *not*</span></span><br><span class="line"><span class="comment"># the correct way of using L2 regularization/weight decay with Adam,</span></span><br><span class="line"><span class="comment"># since that will interact with the m and v parameters in strange ways.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Instead we want ot decay the weights in a manner that doesn't interact</span></span><br><span class="line"><span class="comment"># with the m/v parameters. This is equivalent to adding the square</span></span><br><span class="line"><span class="comment"># of the weights to the loss with plain (non-momentum) SGD.</span></span><br><span class="line"><span class="keyword">if</span> self._do_use_weight_decay(param_name):</span><br><span class="line">      update += self.weight_decay_rate * param</span><br></pre></td></tr></table></figure></p><p>这里讲到<code>直接将权重的平方加入到loss 上进行L2 regularization 在 Adam 上是一种错误到方式</code></p><h3><span id="weight-decay">weight decay</span><a href="#weight-decay" class="header-anchor"></a></h3><p>Weight decay是在每次更新的梯度基础上减去一个梯度</p><p>$$\theta_{t+1}=(1-\lambda )\theta_{t} -\alpha \bigtriangledown f_{t}(\theta_{t})$$</p><h3><span id="l2-regularization">L2 regularization</span><a href="#l2-regularization" class="header-anchor"></a></h3><p>L2 regularrization是在参数上加上L2惩罚</p><p>$$ f_{t}^{reg}(\theta)=f_{t}(\theta)+\frac{ {\lambda }’}{2}\left \| \theta\right \| _{2}^{2}$$</p><p>可以看出，在标准SGD下，两者是等价的<br>但是，在Adam下，两者却不是。我们将Adam下的梯度更新完整公式写出来：</p><p>$$ \theta_{t}\leftarrow \theta_{t-1} -\alpha \frac{\beta_{1}m_{t-1}+(1-\beta_{1})(\bigtriangledown f_{t}+\lambda \theta_{t-1})}{\sqrt{\hat{v_{t}}} + \varepsilon  }$$</p><p>而与参数有关的是右上角的部分：<code>$\frac{\lambda \theta_{t-1}}{\sqrt{v_{t}}}$</code> 而这一项表明，在梯度变化越大的方向上，v的值也越大，但对应的权重约束却越小，这显然是不合理的，此外，L2 与 weight decay 都是各个方向同性的，<br>所以针对这一问题，一种调整方式是将梯度更新与weight decay 解偶，<br><img src="/2020/08/01/optimizer-in-bert/de.png" alt><br>具体参考<a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="noopener">DECOUPLED WEIGHT DECAY REGULARIZATION</a><br>而 bert 中也是使用了这种weight decay 方式，来达到与L2正则等效</p><h1><span id="learning-rate">Learning rate</span><a href="#learning-rate" class="header-anchor"></a></h1><h3><span id="learning-rate-decay">Learning rate decay</span><a href="#learning-rate-decay" class="header-anchor"></a></h3><p>通常，为了让模型在后期避免震荡，更加稳定，都会随着训练的进行，将learning rate 进行调整，即越是后期learning rate 越小。</p><h3><span id="warmup">warmup</span><a href="#warmup" class="header-anchor"></a></h3><p>而bert中的learning rate的调整是两段线性调整学习率：前 10% steps 将learning rate 从 0 增长到 init_learning_rate，然后，再一致递减 到0<br>而warmup为何有效？</p><ol><li>可以避免较早的对mini-batch过拟合，即较早的进入不好的局部最优而无法跳出；</li><li>保持模型深层的稳定性<br>具体可以参考<a href="https://www.zhihu.com/question/338066667/answer/771252708" target="_blank" rel="noopener">warmup 为什么有效</a></li></ol><p>此外，由于warmup要求前期保持较小的更新，所以Adam中由于前期会导致更新变小而需要进行的bias correct也可以去掉了。这也就是最初留下到那个问题到答案</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>bert在 pretraining 为了让模型收敛到一个较好的点，不但在优化器 Adam 上使用了与 L2 regularization等效的weight decay，为了避免模型前期过早拟合进入local minimal，使用了warmup 策略。<br>bert作者也建议在进行fine-tuning时，使用与bert源码中相同的优化器，我也做了一些实验，提升有大概不到0.5个点（没有细调），所以在下游任务上可以尝试使用。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于圆明园荷花池</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Math" scheme="https://xv44586.github.io/categories/Math/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>darknet</title>
    <link href="https://xv44586.github.io/2020/05/25/darknet/"/>
    <id>https://xv44586.github.io/2020/05/25/darknet/</id>
    <published>2020-05-25T00:16:52.000Z</published>
    <updated>2020-08-01T16:13:19.156Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#darknet">darknet</a></li><li><a href="#build">build</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>yolov4出来了，打榜上看效果还不错，想自己测试一下，遂有了一次折腾darknet的机会，简单记录一下遇到的问题</p><h1><span id="darknet">darknet</span><a href="#darknet" class="header-anchor"></a></h1><p>首先从<a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">AlexeyAB</a>下载源码，修改 <code> Makefile </code> 中 <code>GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=1 </code>，<br>然后直接<code>make</code>, 此时在当前文件夹下即可运行<code> ./darknet</code></p><p>下载预训练权重<a href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights" target="_blank" rel="noopener">yolov4.weights</a>,放入./build/darknet/x64/<br> cd ./build/darknet/x64<br> ../../../darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights yolo_test.mp4 -out_filename res.mp4</p><h1><span id="build">build</span><a href="#build" class="header-anchor"></a></h1><ol><li>video stream stopped!<br>ffmpeg没有与opencv一起编译，需要重新安装opencv（从源码）<br>卸载之前安装的opencv，进入之前安装的opencv目录<br>&lt;!bash&gt;<br>sudo make uninstall<br>cd ..<br>rm -Rf build<br>cd /usr<br>find . -name “<em>opencv</em>“ | xargs sudo rm -rf<br>yum remove opencv-data python-opencv<br>&lt;&gt;</li></ol><p>cmake -D CMAKE_BUILD_TYPE=RELEASE     -D CMAKE_INSTALL_PREFIX=/usr/local     -D INSTALL_C_EXAMPLES=ON     -D INSTALL_PYTHON_EXAMPLES=ON     -D OPENCV_GENERATE_PKGCONFIG=ON     -D OPENCV_EXTRA_MODULES_PATH=~/root/Download/opencv_contrib/modules     -D BUILD_EXAMPLES=ON -D WITH_FFMPEG=ON ..</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>装机指北</title>
    <link href="https://xv44586.github.io/2020/05/05/make-a-computer/"/>
    <id>https://xv44586.github.io/2020/05/05/make-a-computer/</id>
    <published>2020-05-05T01:20:56.000Z</published>
    <updated>2020-05-05T05:05:15.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#ying-jian-pian">硬件篇</a><ul><li><a href="#xian-qia">显卡</a></li><li><a href="#cpu">CPU</a></li><li><a href="#zhu-ban">主板</a></li><li><a href="#san-re">散热</a></li><li><a href="#cun-chu">存储</a></li><li><a href="#dian-yuan-yu-ji-xiang">电源与机箱</a></li></ul></li><li><a href="#an-zhuang">安装</a></li><li><a href="#ruan-jian-an-zhuang">软件安装</a><ul><li><a href="#xi-tong-an-zhuang">系统安装</a></li><li><a href="#an-zhuang-nvidia-qu-dong">安装Nvidia驱动</a></li><li><a href="#cuda-de-an-zhuang-yu-xie-zai">CUDA的安装与卸载</a><ul><li><a href="#xie-zai-cuda">卸载cuda</a></li><li><a href="#an-zhuang-cuda">安装cuda</a></li></ul></li><li><a href="#an-zhuang-cudnn">安装cudnn</a></li><li><a href="#python-huan-jing">Python环境</a><ul><li><a href="#conda-pei-zhi">conda配置</a></li><li><a href="#an-zhuang-jupyter">安装jupyter</a></li></ul></li><li><a href="#an-zhuang-tensorflow-gpu">安装tensorflow-gpu</a></li></ul></li><li><a href="#zong-jie">总结</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>  今年回来，第一件事就是要打造一个自己的实验环境，而这也是我第一次自己从硬件开始，所以前前后后差不多折腾了大半个月，总算是搞定了。为了纪念这次从0开始打造自己的深度学习实验环境，所以写了这篇指北。</p><h1><span id="ying-jian-pian">硬件篇</span><a href="#ying-jian-pian" class="header-anchor"></a></h1><p>  关于硬件，最主要的就是显卡、CPU、主板和散热了，接下来一个一个介绍经验。</p><h2><span id="xian-qia">显卡</span><a href="#xian-qia" class="header-anchor"></a></h2><p>  显卡的选择主要参考两个维度：1.显存大小；2.浮点计算能力。<br>  显卡主要分AMD与Nvidia系列。因为Nvidia有CUDA加持，加速计算，自然是Nvidia系列了，Nvidia显卡目前针对PC有三个系列：Quadro、GeForce和Tesla。其中Quadro系列是专业绘图，GeoForce是专业游戏显卡（可绘图可计算），而Tesla是专业计算卡。综合考虑价格与 state-of-the-art 的模型对硬件的最低要求，最终选择了2080Ti。关于GPU的选型，可以参考<a href="https://zhuanlan.zhihu.com/p/61411536" target="_blank" rel="noopener">深度学习GPU对比</a><br>  确定了型号，就是出品厂家选择了，主要区别就是公版非公版。公版就是Nvidia自己设计自己出的，而非公就是第三方厂商出的，包括evga，技嘉，微星等等。有一篇2080ti进行深度学习时的显卡性能测评的文章，找不到地址了，结论是evga &gt; Nvidia &gt; others 。evga与Nvidia的主要区别有两个：1.价格上evga略高 10% 左右；2. Nvidia 采用双风扇风冷，evga 采用单风扇风冷-水冷混合。最终选择了公版。</p><h2><span id="cpu">CPU</span><a href="#cpu" class="header-anchor"></a></h2><p>  CPU主要有两个系列：Intel 和 AMD 。也是因为主要用来做计算，所以肯定首选 AMD。目前ADM系列顶配是 3990X，但是价格大概在三万左右，太感人了。第二的是 3700X ，京东上一千三上下，那就是他了。</p><h2><span id="zhu-ban">主板</span><a href="#zhu-ban" class="header-anchor"></a></h2><p>  CPU确定了，主板型号基本就定了。 主要参考如下图：<br>  <img src="/2020/05/05/make-a-computer/board.png" alt="cpu-主板对应型号参考"><br>  看了一些测评，最终选择了微星<a href="https://item.jd.com/8259910.html" target="_blank" rel="noopener">B450</a></p><h2><span id="san-re">散热</span><a href="#san-re" class="header-anchor"></a></h2><p>  散热目前两个方式：风冷和水冷。两者的区别主要参考<a href="https://www.zhihu.com/question/57695465/answer/440467918" target="_blank" rel="noopener">风冷与水冷区别</a><br>  水冷在散热上还是要强一些的（240以上），所以打算试水一款水冷。主要推荐两款：<a href="https://item.m.jd.com/product/100003859323.html?wxa_abtest=o&amp;utm_user=plusmember&amp;ad_od=share&amp;utm_source=androidapp&amp;utm_medium=appshare&amp;utm_campaign=t_335139774&amp;utm_term=Wxfriends&amp;from=singlemessage&amp;isappinstalled=0" target="_blank" rel="noopener">乔思伯光影240</a> <a href="https://item.m.jd.com/product/6454809.html?wxa_abtest=o&amp;utm_user=plusmember&amp;ad_od=share&amp;utm_source=androidapp&amp;utm_medium=appshare&amp;utm_campaign=t_335139774&amp;utm_term=Wxfriends&amp;from=singlemessage&amp;isappinstalled=0" target="_blank" rel="noopener">九州风神水元素240T</a>。最后选择了九州风神水元素，因为买那天乔思伯涨价了～</p><h2><span id="cun-chu">存储</span><a href="#cun-chu" class="header-anchor"></a></h2><p>  存储上打算采用 32G + 1T ssd。内存自然上<a href="https://item.jd.com/8391349.html" target="_blank" rel="noopener">金士顿骇客神条</a>, ssd主要参考性价比，最终选择<a href="https://item.jd.com/100002580230.html" target="_blank" rel="noopener">三星1T SSD</a></p><h2><span id="dian-yuan-yu-ji-xiang">电源与机箱</span><a href="#dian-yuan-yu-ji-xiang" class="header-anchor"></a></h2><p>  由于目前暂时只插一张显卡，所以电源在 600W 以上即可。选一个品牌比较好的，那就是<a href="https://item.jd.com/6828141.html" target="_blank" rel="noopener">安钛克750</a> 了<br>  至于机箱，主要参考能不能够合理安放显卡、主板以及后续可能的散热。对于我当前的配置，只要是中塔的基本都够插显卡。买个安静低调的，那就他了：<a href="https://item.jd.com/100004999668.html" target="_blank" rel="noopener">爱国者M2</a><br>  最终的配置清单如下：<br>  <img src="/2020/05/05/make-a-computer/list.png" alt="list"></p><h1><span id="an-zhuang">安装</span><a href="#an-zhuang" class="header-anchor"></a></h1><p>  安装前，强烈建议多看几期安装教程视频，我主要看的是 B站 的<a href="https://www.bilibili.com/video/BV1vx41187cm" target="_blank" rel="noopener">跟装机猿搞装机</a> 系列。最困难的可能是水冷散热的安装了。由于平台不同，安装方式不同，推荐看对应水冷厂商给的安装视频。我主要参考<a href="https://www.bilibili.com/video/BV1EE411h7R1" target="_blank" rel="noopener">水元素安装教程</a><br>  安装过程大致总结一下：</p><ol><li>拿出主板和CPU，按照说明书将CPU装到主板上。</li><li>将水冷拿出，拿出硅脂，涂抹在CPU上，将冷头上的塑料膜撕掉，安装好支撑后，将水冷头贴在CPU上，安装固定。</li><li>拆开机箱侧板，将主板装上，并将水冷的风扇固定在机箱上。注意：风扇上没有挡板的是出风口，一般原则是从机箱内往外吹，注意风向。</li><li>插入内存条，固定固态硬盘</li><li>将线按说明一个一个插上</li><li>寻找机箱上显卡位置，可能需要扣掉挡板铁片。将显卡插入主板，同时固定在机箱上</li><li>放入电源，插好对应线<br>主要注意的主要是：1、水冷头上的膜一定要撕去，否则CPU上的热不能很好的传导出来。2、装之前大概看一下机箱各个位置，有些时候因为安装顺序会导致没地方下手，需要卸了重组。</li></ol><h1><span id="ruan-jian-an-zhuang">软件安装</span><a href="#ruan-jian-an-zhuang" class="header-anchor"></a></h1><h2><span id="xi-tong-an-zhuang">系统安装</span><a href="#xi-tong-an-zhuang" class="header-anchor"></a></h2><p>  系统选Linux的稳定版，所以装了CentOS 7。<br>  1.首先下载centos的镜像文件，外网可能会慢，选择<a href="http://mirrors.aliyun.com/centos/7/isos/x86_64/" target="_blank" rel="noopener">阿里源</a><br>  2.下载相应的刻录工具，插入u盘，将镜像刻录进u盘（刻录时会先格式化u盘，如果是windows系统，u盘默认是FAT32,这种格式下是没法放大于4G的单文件，所以需要进行格式转换：cmd下执行 convert e: /fs:ntfs)<br>  3.插入u盘，开机，引导开机进入u盘（大部分是自动进来），选择 test &amp; install<br>  4.一般安装时，对应的命令行中的目录是错误的，执行会报错：<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR，could not insert <span class="string">'floppy'</span></span><br></pre></td></tr></table></figure></p><p>  而无法进入安装界面<br>  此时需要查找u盘对应名字，修改命令：<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span>:/dev &amp; ls</span><br></pre></td></tr></table></figure></p><p>  找到一个 <code>s##数字 </code>的串，我的是sdb4 ,然后重启，进入后先按 e 进入编辑模式，修改命令行<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linuxefi/images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS\x207\x20x86_64 xdriver=vesa nomodeset quiet</span><br><span class="line">linuxefi/images/pxeboot/vmlinuz inst.stage2=hd:/dev/sdb4 xdriver=vesa nomodeset quiet</span><br></pre></td></tr></table></figure></p><p>  由于我的显卡是2080Ti,系统自带的nouveau驱动不匹配，需要禁用暂时，所以在后面加上 <code> nouveau.modeset=0 </code><br>  最终修改后完整的命令行为：<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linuxefi/images/pxeboot/vmlinuz inst.stage2=hd:/dev/sdb4 xdriver=vesa nomodeset quiet nouveau.modeset=0</span><br></pre></td></tr></table></figure></p><p>  此时保存后退出，选择test &amp; install 即可进入安装界面<br>  进入后安装，安装时，注意软件选择中选择带网络的，剩下的就是按提示一步一步来即可。</p><h2><span id="an-zhuang-nvidia-qu-dong">安装Nvidia驱动</span><a href="#an-zhuang-nvidia-qu-dong" class="header-anchor"></a></h2><p>  1.检查显卡是否正常<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lspci | grep -i nvidia</span><br></pre></td></tr></table></figure></p><p>  2.检查驱动版本<br>  添加EIRepo源<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</span><br><span class="line">$ rpm -Uvh http://www.elrepo.org/elrepo-release-7.6-5.el7.elrepo.noarch.rpm</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">3.安装显卡驱动检查包</span><br><span class="line">```bash</span><br><span class="line">$ yum install nvidia-detect</span><br></pre></td></tr></table></figure></p><p>  检查驱动版本<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-detect -v</span><br></pre></td></tr></table></figure></p><p>  此时会得到对应的版本信息，注意那个数字<br>  4.安装编译环境<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) dkms</span><br><span class="line">$ yum -y update //注意这是升级系统</span><br><span class="line">$ yum -y install gcc kernel-devel kernel-headers dkms</span><br></pre></td></tr></table></figure></p><p>  5.禁用vouveau<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/modprobe.d/blacklist-nouveau.conf</span><br><span class="line">    blacklist nouveau</span><br><span class="line">    options nouveau modeset=0</span><br></pre></td></tr></table></figure></p><p>  6.重新建立initramfs image文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak</span><br><span class="line">$ dracut /boot/initramfs-$(uname -r).img $(uname -r)</span><br></pre></td></tr></table></figure></p><p>  7.reboot</p><h2><span id="cuda-de-an-zhuang-yu-xie-zai">CUDA的安装与卸载</span><a href="#cuda-de-an-zhuang-yu-xie-zai" class="header-anchor"></a></h2><h3><span id="xie-zai-cuda">卸载cuda</span><a href="#xie-zai-cuda" class="header-anchor"></a></h3>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum remove <span class="string">"*cublas*"</span> <span class="string">"cuda*"</span></span><br></pre></td></tr></table></figure><p>参考：<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#removing-cuda-tk-and-driver" target="_blank" rel="noopener">removing-cuda</a></p><h3><span id="an-zhuang-cuda">安装cuda</span><a href="#an-zhuang-cuda" class="header-anchor"></a></h3>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget path/to/your-version/install.rpm</span><br><span class="line">sudo rpm -i cuda-repo-rhel7-10-1-local-10.1.105-418.39-1.0-1.x86_64.rpm</span><br><span class="line">sudo yum clean all</span><br><span class="line">sudo yum install cuda</span><br></pre></td></tr></table></figure><p>参考：<a href="https://developer.nvidia.com/cuda-10.1-download-archive-base?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=CentOS&amp;target_version=7&amp;target_type=rpmlocal" target="_blank" rel="noopener">cuda-download</a><br>注意：一定不要通过浏览器去下载，因为会非常非常非常慢，但是wget大概几分钟就搞定了</p><h2><span id="an-zhuang-cudnn">安装cudnn</span><a href="#an-zhuang-cudnn" class="header-anchor"></a></h2><p>  1.下载相应的包 libcudnn*.rpm</p><ol start="2"><li><p>安装包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh libcudnn7-*.x86_64.rpm</span><br><span class="line">rpm -ivh libcudnn7-devel-*.x86_64.rpm</span><br><span class="line">rpm -ivh libcudnn7-doc-*.x86_64.rpm</span><br></pre></td></tr></table></figure><p>3.验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span>  <span class="variable">$HOME</span>/cudnn_samples_v7/mnistCUDNN</span><br><span class="line"><span class="variable">$make</span> clean &amp;&amp; make</span><br><span class="line">$ ./mnistCUDNN</span><br></pre></td></tr></table></figure><p>if: Test passed! 则验证通过<br>参考：<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-linux" target="_blank" rel="noopener">cudnn-install</a></p><h2><span id="python-huan-jing">Python环境</span><a href="#python-huan-jing" class="header-anchor"></a></h2><p>python环境采用Anaconda + jupyter notebook</p><h3><span id="conda-pei-zhi">conda配置</span><a href="#conda-pei-zhi" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py3 python=3</span><br><span class="line">activate py3</span><br><span class="line">conda install ipykernel -n py3</span><br><span class="line">python -m ipykernel install --user --name py3 --display-name <span class="string">'py3'</span></span><br></pre></td></tr></table></figure><p>删除环境</p><figure class="highlight bash"><figcaption><span>jupyter kernelspec remove py3 ```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  pip 安装 与修改源</span><br><span class="line">```bash </span><br><span class="line">  conda install pip -n py3</span><br><span class="line">  vim ~/.pip/pip.conf</span><br><span class="line"></span><br><span class="line">  [global]</span><br><span class="line">  index-url=https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><h3><span id="an-zhuang-jupyter">安装jupyter</span><a href="#an-zhuang-jupyter" class="header-anchor"></a></h3><figure class="highlight bash"><figcaption><span>pip install jupyter```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  jupyter 作为后台服务器</span><br><span class="line">  1. 添加密码</span><br><span class="line">```bash </span><br><span class="line">  ipython</span><br><span class="line">  from jupyter.auth import passwd</span><br><span class="line">  passwd()</span><br></pre></td></tr></table></figure><p>此时会让你输入两次密码，输入后得到一串hash码，保存下来， 回到bash, 添加配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ jupyter notebook --generate-config</span><br><span class="line">$ vim ~/.jupyter/jupyter-notebook-config.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># edit</span></span><br><span class="line"></span><br><span class="line">c.NotebookApp.ip=<span class="string">'*'</span>                                  <span class="comment"># * 代表所有iP都能访问 ，也可以指定ip</span></span><br><span class="line">c.NotebookApp.password = u<span class="string">'sha1:ce...'</span>       <span class="comment"># 刚才复制的那个密文</span></span><br><span class="line">c.NotebookApp.open_browser = False       <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line">c.NotebookApp.port =8888                         <span class="comment">#指定一个端口</span></span><br><span class="line">   </span><br><span class="line">c.NotebookApp.notebook_dir = <span class="string">'/home/user/user1'</span>  <span class="comment">#指定工作空间</span></span><br><span class="line">c.PAMAuthenticator.encoding = <span class="string">'utf8'</span>         <span class="comment">#指定utf-8编码，解决读取中文路径或者文件乱码问题</span></span><br></pre></td></tr></table></figure><p>后台运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nohup jupyter notebook --allow-root &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></li></ol><p>  关闭后台运行<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># ps -axu | grep jupyter</span></span><br><span class="line"><span class="comment"># kill -9 pid</span></span><br></pre></td></tr></table></figure></p><h2><span id="an-zhuang-tensorflow-gpu">安装tensorflow-gpu</span><a href="#an-zhuang-tensorflow-gpu" class="header-anchor"></a></h2><p>  tensorflow-gpu对cuda有版本要求，所以在安装cuda前需要提前查看，确定自己版本。参考官网<a href="https://www.tensorflow.org/install/source" target="_blank" rel="noopener">install</a><br>  <img src="/2020/05/05/make-a-computer/gpu.png" alt="tensorflow-gpu vs cuda"><br>  还记得cuda安装时，给出的卸载方法吗？就是因为装tensorflow-gpu后发现gpu不带动的，后来才发现是cuda版本太高了～<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install tensorflow-gpu==2.1</span><br></pre></td></tr></table></figure></p><p>  验证<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ipython</span><br><span class="line">  import tensorflow as tf</span><br><span class="line">  tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br></pre></td></tr></table></figure></p><p>  没有报错则正常</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>  以上就是从零开始搭建搭建一套自己的深度学习实验平台的个人经验，整个过程从硬件到最终的软件适配都走了很多坑，希望能给你一些借鉴。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>  部分硬件盒子</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Life" scheme="https://xv44586.github.io/categories/Life/"/>
    
    
      <category term="tensorflow-gpu" scheme="https://xv44586.github.io/tags/tensorflow-gpu/"/>
    
      <category term="装机" scheme="https://xv44586.github.io/tags/%E8%A3%85%E6%9C%BA/"/>
    
      <category term="CUDA" scheme="https://xv44586.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>记一次npm环境问题</title>
    <link href="https://xv44586.github.io/2020/01/11/npm-environment/"/>
    <id>https://xv44586.github.io/2020/01/11/npm-environment/</id>
    <published>2020-01-11T04:31:20.000Z</published>
    <updated>2020-01-11T06:37:47.709Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#qi-yin">起因</a></li><li><a href="#ddbug">Ddbug</a></li><li><a href="#jie-lun">结论</a></li><li><a href="#guan-yu-tou">关于头</a></li></ul><!-- tocstop --></div><h1><span id="qi-yin">起因</span><a href="#qi-yin" class="header-anchor"></a></h1><p>上周写完博客，本地预览时，突然报错，看了一下是 sharp.js 的问题，以为是个小场面，然后就开始了一周的痛苦修环境经历，哭了😭</p><h1><span id="ddbug">Ddbug</span><a href="#ddbug" class="header-anchor"></a></h1><ol><li>首先根据提示，重新安装  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf node_modules/sharp</span><br><span class="line">npm i sharp</span><br></pre></td></tr></table></figure></li></ol><p>此时报错：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c++: error: unrecognized command line option <span class="string">'-stdlib=libc++'</span></span><br><span class="line">make: *** [Release/.node] Error <span class="number">1</span></span><br><span class="line">gyp ERR! build error</span><br><span class="line">gyp ERR! stack Error: `make` failed <span class="keyword">with</span> exit code: <span class="number">2</span></span><br><span class="line">gyp ERR! stack     at ChildProcess.onExit (/usr/local/Cellar/node@<span class="number">8</span>/<span class="number">8.16</span><span class="number">.2</span>/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:<span class="number">262</span>:<span class="number">23</span>)</span><br><span class="line">gyp ERR! stack     at emitTwo (events.js:<span class="number">126</span>:<span class="number">13</span>)</span><br><span class="line">gyp ERR! stack     at ChildProcess.emit (events.js:<span class="number">214</span>:<span class="number">7</span>)</span><br><span class="line">gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:<span class="number">198</span>:<span class="number">12</span>)</span><br></pre></td></tr></table></figure></p><p>这个问题的本质是当前的包需要通过源码编译，而当前用的是gcc（macOS），而gcc不支持当前命令，之前装环境没有遇到过这个问题，可能是我最近跟新了gcc？<br>查了一下解决这个问题最简单的方式是指定c++:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CXX=clang++ npm i xxx</span><br></pre></td></tr></table></figure></p><p>这次确实装成功了，走起！<br>此时又报新错：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">can <span class="keyword">not</span> find sharp xxx</span><br><span class="line">rm node_modules/sharp <span class="keyword">and</span> rebuild</span><br></pre></td></tr></table></figure></p><p>阿嘞？装上了又找不到？why？<br>又尝试了全局安装，依然找不到，此时，有点上头，我干脆把node_modules全部删掉，重新装吧。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf node_modules</span><br><span class="line">CXX=clang++ npm i</span><br></pre></td></tr></table></figure></p><p>阿嘞？这次装也失败了，错误大致原因是node-gyp rebuild nodejieba失败。<br>开始以为是node-gyp的问题，后来查了一下，node-gyp是用来帮助丛源码编译的工具，所以本质上不是他的问题，还是别的问题。<br>又查了一下nodejieba, 有人说是在lunr.js中用的nodejieba在node高版本中会存在编译失败，建议用 node8.x ,python版本最好是2.7，那我上次没失败？先将node降级到node8, python切到2.7。<br>再装一次，依然失败。但是错误信息不够定位，查一下怎么看更全的日志。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CXX=clang++ npm i --verbose</span><br><span class="line">```  </span><br><span class="line">发现两条重要信息，大概是无法找到lib下某个库，可能是本级的环境出了问题，整理一下吧。</span><br><span class="line"> ```python</span><br><span class="line">brew update</span><br><span class="line">brew cleanup</span><br><span class="line">brew doctor</span><br><span class="line">```  </span><br><span class="line">环境的一堆issue解决掉（主要是link无效），然后把之前无法找到的两个lib重新装了一次（很慢）。</span><br><span class="line">此时还是同样问题，这次又查看了一下当前环境问题，两个包没link，一个是python(其实是python3，之前link的python2)， 一个是swig，后来一想，可能是swig这个工具在源码编译是缺失导致的，</span><br><span class="line">```python</span><br><span class="line">brew link swig</span><br></pre></td></tr></table></figure></p><p>此时在装，搞定！走起，又报新错：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FATAL Something<span class="string">'s wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.html</span></span><br><span class="line"><span class="string">Error: spawn /Users/xuming/Project/blog/node_modules/optipng-bin/vendor/optipng ENOENT</span></span><br><span class="line"><span class="string">    at Process.ChildProcess._handle.onexit (internal/child_process.js:190:19)</span></span><br><span class="line"><span class="string">    at onErrorNT (internal/child_process.js:362:16)</span></span><br><span class="line"><span class="string">    at _combinedTickCallback (internal/process/next_tick.js:139:11)</span></span><br><span class="line"><span class="string">    at process._tickCallback (internal/process/next_tick.js:181:9)</span></span><br></pre></td></tr></table></figure></p><p>这是缺module，但是我是npm i，为什么还缺？此时可能只是当前node_modules的问题，为了验证本机其他环境已经ok了，新建了一个博客，验证后发现本机确实OK了。<br>额，那就缺什么装什么吧。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i optipng-bin</span><br></pre></td></tr></table></figure></p><p>走起！一切正常！此时我的心情就好比火箭发射成功一样。折磨了一周的环境问题，终于搞定了，中间还有许多其他方向的试探，但都记不得了。- -！</p><h1><span id="jie-lun">结论</span><a href="#jie-lun" class="header-anchor"></a></h1><ol><li>npm 失败后，可以加 –verbose 参数查看详细日志，定位问题。</li><li>编译源码可能你还需要装xcode-select --install</li><li>对于 Error: spawn .../node_modules/xxx/vendor/.. ENOENT,单独安装一下对应缺失module即可。</li><li>node-gyp 和 libvips可能也会影响，建议重装一次</li><li>MacOS中，可能会gcc与clang并存，加上系统升级，可能导致相应版本不兼容问题。指定clang ：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CXX=clang++ npm i</span><br></pre></td></tr></table></figure>6. 本机环境问题，可以通过<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew doctor</span><br></pre></td></tr></table></figure></li></ol><h1><span id="guan-yu-tou">关于头</span><a href="#guan-yu-tou" class="header-anchor"></a></h1><p>雪中奥森</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Life" scheme="https://xv44586.github.io/categories/Life/"/>
    
    
      <category term="npm" scheme="https://xv44586.github.io/tags/npm/"/>
    
  </entry>
  
  <entry>
    <title>Backup</title>
    <link href="https://xv44586.github.io/2019/12/27/backup/"/>
    <id>https://xv44586.github.io/2019/12/27/backup/</id>
    <published>2019-12-27T14:24:21.000Z</published>
    <updated>2020-01-11T06:42:03.393Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#pei-zhi">配置</a><ul><li><a href="#cha-jian">插件</a></li></ul></li><li><a href="#zhu-ti">主题</a></li><li><a href="#markdown">MarkDown</a><ul><li><a href="#bu-ju">布局</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id> </span><a href="#" class="header-anchor"></a></h1><p><strong>Just for me！<br>现在使用的博客虽然使用的是开源的，但是自己做了部分修改，加上一些常用语法一段时间不用后又需要重新查，所以在此记录一下当前博客常用的。 </strong> </p><h1><span id="pei-zhi">配置</span><a href="#pei-zhi" class="header-anchor"></a></h1><p>Hexo 部署文档： <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="noopener">http://hexo.io/docs/deployment.html</a><br>Hexo _config.xml 的配置 <a href="https://gist.github.com/btfak/18938572f5df000ebe06fbd1872e4e39" target="_blank" rel="noopener">https://gist.github.com/btfak/18938572f5df000ebe06fbd1872e4e39</a></p><h2><span id="cha-jian">插件</span><a href="#cha-jian" class="header-anchor"></a></h2><ul><li>hexo-toc  <a href="https://github.com/bubkoo/hexo-toc" target="_blank" rel="noopener">Insert a markdown TOC before posts be rendered</a><br>用来生产目录</li><li>hexo-renderer-marked + MathJax<br>整体顺序是先由renderer渲染，然后交给MathJax渲染Math相关，前者在遇见$ $后将escape _ 导致下标失效（_在renderer中认为是黑体，<br>所以产生这种冲突），所以修改了部分escape</li></ul><h1><span id="zhu-ti">主题</span><a href="#zhu-ti" class="header-anchor"></a></h1><ul><li>hexo-theme-skapp <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank" rel="noopener">https://github.com/Mrminfive/hexo-theme-skapp</a><ul><li>主要修改：</li></ul><ol><li>部分页面布局，包括footer和header</li><li>字体样式，包括部分元素样式，如<strong>code</strong></li></ol></li></ul><h1><span id="markdown">MarkDown</span><a href="#markdown" class="header-anchor"></a></h1><h2><span id="bu-ju">布局</span><a href="#bu-ju" class="header-anchor"></a></h2><ul><li><p>添加大纲<br>在正文最开始添加 </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- toc --&gt;</span><br></pre></td></tr></table></figure></li><li><p>标题<br><code> # </code> ~ <code>######</code>，<code>#</code>号的个数表示几级标题，即表示一级标题到六级标题</p></li><li><p>有序列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. **我是一级序列** </span><br><span class="line">2. **我是一级序列** </span><br><span class="line">3. **我是一级序列** </span><br><span class="line"> 1. *我是二级序列* </span><br><span class="line"> 1. *我是二级序列* </span><br><span class="line">  1. *我是二级序列*</span><br></pre></td></tr></table></figure></li></ul>  <blockquote><ol><li><strong>我是一级序列</strong> </li><li><strong>我是一级序列</strong> </li><li><strong>我是一级序列</strong> <ol><li><em>我是二级序列</em> </li><li><em>我是二级序列</em> </li><li><em>我是二级序列</em> </li></ol></li></ol></blockquote><ul><li>无序列表</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">* *列表展示  </span><br><span class="line"> * *列表展示</span><br><span class="line">  * *列表展示</span><br><span class="line">+ +列表展示</span><br><span class="line"> + +列表展示</span><br><span class="line">  + +列表展示</span><br><span class="line">- -列表展示</span><br><span class="line"> - -列表展示</span><br><span class="line">  - -列表展示</span><br></pre></td></tr></table></figure>  <blockquote><ul><li>*列表展示<ul><li>*列表展示<ul><li>*列表展示</li></ul><ul><li>+列表展示</li></ul></li></ul><ul><li>+列表展示<ul><li>+列表展示</li></ul><ul><li>-列表展示</li></ul></li></ul><ul><li>-列表展示<ul><li>-列表展示</li></ul></li></ul></li></ul></blockquote><ul><li>表格</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&amp;nbsp; | l1     | l2     </span><br><span class="line">  ----- | --- | ---- </span><br><span class="line">    w0 | $e_&#123;<span class="number">01</span>&#125;$ | $e_&#123;<span class="number">02</span>&#125;$ </span><br><span class="line">    w1 | $e_&#123;<span class="number">11</span>&#125;$ | $e_&#123;<span class="number">12</span>&#125;$ </span><br><span class="line">    w2 | $e_&#123;<span class="number">21</span>&#125;$ | $e_&#123;<span class="number">22</span>&#125;$</span><br></pre></td></tr></table></figure><p>表头与正文用–来分割，列之间用|来分割。注：列名不能使用空格，如需要列名为空，需要使用 &amp;nbsp；替换</p>  <blockquote><table><thead><tr><th>&nbsp;</th><th>l1</th><th>l2     </th></tr></thead><tbody><tr><td>  w0</td><td>$e_{01}$</td><td>$e_{02}$ </td></tr><tr><td>  w1</td><td>$e_{11}$</td><td>$e_{12}$ </td></tr><tr><td>  w2</td><td>$e_{21}$</td><td>$e_{22}$ </td></tr></tbody></table></blockquote><ul><li><p>强调</p> **加粗** __加粗__ _斜体_  ***加粗并斜体*** ~~删除线～~ <blockquote><p><strong>加粗</strong> <strong>加粗</strong> _斜体_  <strong><em>加粗并斜体</em></strong> ~~删除线～~</p></blockquote></li><li><p>高亮</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用&lt;code&gt; content &lt;/code&gt; 来包裹想要高亮的元素</span><br></pre></td></tr></table></figure>  <blockquote><p>用<code> content </code> 来包裹想要高亮的元素</p></blockquote><ul><li><p>关闭MarkDown</p><blockquote><p>{% raw %} content {% endraw %}</p><p>这种方式将会忽略空格回车等，有些场景也会失效，此时可以用 <strong>代码块</strong> 代替</p></blockquote></li><li><p>引用</p><blockquote><p>{% blockquote 江泽民%}科技是第一生产力 {% endblockquote %} </p></blockquote><blockquote><p>科技是第一生产力</p><footer><strong>江泽民</strong></footer></blockquote></li><li><p>Math</p><blockquote><p>行内公式用 $包裹，多行时用$$包裹<br>使用的是MathJax，语法可参考博客<a href="https://blog.csdn.net/ethmery/article/details/50670297" target="_blank" rel="noopener">MathJax基本语法</a> 和官方文档(<a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference</a>)<br><code>调试参考<a href="https://www.quicklatex.com/" target="_blank" rel="noopener">quicklatex</a></code></p></blockquote></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$</span><br><span class="line"> states = $$\begin&#123;pmatrix&#125;</span><br><span class="line"> e_&#123;<span class="number">01</span>&#125;&amp;e_&#123;<span class="number">01</span>&#125;\\\\</span><br><span class="line"> e_&#123;<span class="number">02</span>&#125;&amp;e_&#123;<span class="number">02</span>&#125;\\\\</span><br><span class="line"> \end&#123;pmatrix&#125;$$</span><br><span class="line">$</span><br></pre></td></tr></table></figure>  <blockquote><p>$<br>states = $$\begin{pmatrix}<br>e_{01}&amp;e_{01}\\<br>e_{02}&amp;e_{02}\\<br>\end{pmatrix}$$<br>$</p><p><strong>注：公式内\ 会被转义，需要用双\，尤其在矩阵中。</strong></p></blockquote><ul><li>代码<blockquote><p>使用三个<code>`</code>包裹，如果需要显示三个 <code>`</code>， 可以用四个。也可以用  {%codeblock%}</p></blockquote></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">do</span><span class="params">(self, job_func, *args, **kwargs)</span>:</span></span><br><span class="line">        print(<span class="string">'hello world'</span>)  </span><br><span class="line">  ```</span><br></pre></td></tr></table></figure>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do</span><span class="params">(self, job_func, *args, **kwargs)</span>:</span></span><br><span class="line">   print(<span class="string">'hello world'</span>)  </span><br><span class="line">   </span><br></pre></td></tr></table></figure><p>$$log(Z_{(0-&gt;1-&gt;2)}) = log(exp(states[0]) + exp(states[1])) \\<br>=log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{21} + t_{11}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{21}+t_{21})) \\+<br>log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{22} + t_{12}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{22}+t_{22})) \\<br>= log(exp(e_{01}+e_{11}+t_{11}+e_{21}+t_{11}) + exp(e_{02}+e_{11}+t_{21}+e_{21}+t_{11}) \\ +<br>exp(e_{01}+e_{12}+t_{12} +e_{21}+t_{21}) + exp(e_{02}+e_{12}+t_{22}+e_{21}+t_{21}) \\+<br>exp(e_{01}+e_{11}+t_{11} +e_{22}+t_{12}) + exp(e_{01}+e_{11}+t_{11}+e_{22}+t_{12}) \\+<br>exp(e_{01}+e_{12}+t_{12} +e_{22}+t_{22}) + exp(e_{01}+e_{12}+t_{12}+e_{22}+t_{21}))$$</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p> 望京SOHO夜景</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Life" scheme="https://xv44586.github.io/categories/Life/"/>
    
    
      <category term="Hexo" scheme="https://xv44586.github.io/tags/Hexo/"/>
    
      <category term="MarkDown" scheme="https://xv44586.github.io/tags/MarkDown/"/>
    
  </entry>
  
  <entry>
    <title>from softmax to crf</title>
    <link href="https://xv44586.github.io/2019/12/26/from-softmax-to-crf/"/>
    <id>https://xv44586.github.io/2019/12/26/from-softmax-to-crf/</id>
    <published>2019-12-26T12:45:56.000Z</published>
    <updated>2020-08-06T15:03:31.471Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#xu-lie-biao-zhu">序列标注</a><ul><li><a href="#mo-xing">模型</a></li><li><a href="#shu-xue-xing-shi">数学形式</a></li></ul></li><li><a href="#yan-shi-jian-zhou-softmax">沿时间轴Softmax</a></li><li><a href="#crf">CRF</a></li><li><a href="#xian-xing-lian-crf">线性链CRF</a></li><li><a href="#qiu-jie">求解</a></li><li><a href="#demo">demo</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>又做NER相关东西， 用到了CRF，所以想给组里人从头一步一步的将CRF讲一遍，希望大家看完能明白CRF的数学模型已经工程上的使用。<br>网上关于CRF大多数都是将他与HMM及概率图模型一起对比着讲，但是我觉得这需要一些背景知识，鉴于上次分享发现大家并没有什么背景知识，<br>所以这次希望能尽量减少背景知识就能让人搞懂CRF。</p><h1><span id="xu-lie-biao-zhu">序列标注</span><a href="#xu-lie-biao-zhu" class="header-anchor"></a></h1><h2><span id="mo-xing">模型</span><a href="#mo-xing" class="header-anchor"></a></h2><p>通常CRF出现在序列标注任务中，所以我们先来看看序列标注主要是做什么的。<br>序列标注是NLP中一个重要的任务，它包括分词、词性标注、命名实体识等。下面用一个分词的例子来简单说明。（<a href="https://spaces.ac.cn/archives/5542/comment-page-1#comments" target="_blank" rel="noopener">原文</a>)<br>假设我们现在用$bmes$四标签来进行分词，其中b 代表begin即词的开头， m代表middle即词内，e代表end即词的结尾，s代表single即单独成词。<br>现在我们有一个字符串序列 “今天天气不错”，如果对应的分词结果为“今天/天气/不/错”，则其标签序列为“bebess”。由于在序列标注中，我们认为正确的标签序列是唯一的，<br>所以我们的目标就是在所有可能的标签序列中（如bbbbbb,ssssss)挑选出真实的标签序列（bebess), 即最大化概率$P(bebess|今天天气不错）$。<br><img src="/2019/12/26/from-softmax-to-crf/seg.png" alt="4tag分词网络示意图"><br>即在上图中，所有从左至右的连线中，选出黄色的那条。</p><h2><span id="shu-xue-xing-shi">数学形式</span><a href="#shu-xue-xing-shi" class="header-anchor"></a></h2><p>我们假设输入序列是$X=[x_1, x_2, x_3, …, x_n]$,对应的输出序列是 $Y = [y_1, y_2, …, y_n]$,<br>label的集合为$L = [l_1, l_2, … , l_k]$. 任务目标是让真实的输出序列的概率最大，即：<br>$$<br>Max(P(y_1,y_2,..y_n|X))<br>$$</p><h1><span id="yan-shi-jian-zhou-softmax">沿时间轴Softmax</span><a href="#yan-shi-jian-zhou-softmax" class="header-anchor"></a></h1><p>直接对上述模型进行求解比较困难，所以我们先将问题简化，然后在对简化后的问题进行求解。<br>首先，我们引入朴素假设：即标签之间独立不相关，对应的目标就简化为：<br>$$<br>Max(P(y_1|X)P(y_2|X)…P(y_n|X))<br>$$<br>为了对$P(y_i|X)$进行建模，通常我们先通过RNNs（LSTM，BiLSTM, etc)来捕获输入X的全局信息，获得隐藏状态序列$\bar{x_1}, \bar{x_2}, …, \bar{x_n}$,<br>此时的$\bar{x_i}$可以看作是$x_i$ 通过X获取的特征，由于RNNs可以捕获全局信息，所以我们认为特征$\bar{x_i}$之间互不相关，对应的<br>$$P(y_i|X) = P(y_i|\bar{x_i})$$<br>我们的目标：<br>$$<br>Max(P(y_1|X)P(y_2|X)…P(y_n|X))<br>= Max(P(y_1|\bar{x_1})P(y_2|\bar{x_2})..P(y_n|\bar{x_n}))<br>= Max(\prod(P(y_i|\bar{x_i})))<br>$$<br>此时只需要$Max(P(y_i|\bar{x_i}))$进行求解，即沿时间轴一步一步的对RNNs的隐藏输出通过softmax来最大化对应目标标签概率。<br><img src="/2019/12/26/from-softmax-to-crf/softmax.png" alt="沿时间轴softmax"></p><h1><span id="crf">CRF</span><a href="#crf" class="header-anchor"></a></h1><p>因为在上一个方案中，我们做了朴素建设，将输出序列看作是相互独立的一元模型，这样会引入一些问题，如在分词中（bmes），m- 标签不能出现在s- 后面，<br>s- 标签不能出现在b- 和m-后面等，所以即使在上述方案中，至少也需要人为的设置一个“转移矩阵”，将不合理的转移方式得分置为0，来避免不合理方案的出现。<br>而上述方案出现错误的原因，本质上是因为我们的朴素假设：标签之间相互独立。为了解决上述方案的问题，我们至少需要在输出端显式考虑标签的关联性，即输出标签与上下文相关。<br><img src="/2019/12/26/from-softmax-to-crf/crf_example.png" alt="显式考虑输出端上下文"><br>显式考虑输出端上下文相关</p><p>现在我们回到原始目标上来，原始目标是$Max(P(y_1,y_2,..y_n|X)) $, 上个方案中我们是因为直接对$P(y_1, y_2,…y_n|x_1, x_2, …,x_n)$直接建模很难，<br>所以才做出了假设，简化目标。现在让我们换个思路，为了求解上述概率，我们还可以穷举出输出序列所有的可能结果$Y_1, Y_2, …, Y_{k^n}$,<br>然后如果能计算出当前输入X对应每种可能的输出序列的“值”， 则我们可以通过Softmax计算出真实输出序列的概率，即得到$P(Y_{true}|X)$。<br><code>假设一：我们可以学习一个打分函数f，通过函数f可以得到输出序列关于输入的得分，即$score_i = f(Y_i, X)$</code><br>此时，我们的目标就转化为 $P(Y|X) = \frac{exp(f(y_1, y_2,…y_n; X))}{Z(X)}$.<br>其中$Z(X) = \sum_{i=1}^{k^n}(exp(f(Y_i, X)))$。即此时的概率P是一个指数分布。<br>此时我们的方案是<code>1</code>个<code>$k^n$</code>多分类问题，即我们是对一个完整的输出序列为单位来计算概率（路径积分），而上一个方案中，是<code>n</code>个<code>k</code>分类问题，<br>这是两个方案的不同点之一。<br>在方案一中我们也说过，直接对完整序列建模比较困难，此时我们直接对$f(Y_i, X)$求解也会面临相同的困难，为了避免方案一的问题，我们不再使用一元模型，改为二元模型。<br><code>引入一阶马尔可夫假设，且其关联性是加性的。即当前输出标签只与前一个输出标签相关，其总得分是对所有得分求和。</code><br>此时的目标就转化为<br>$$<br>P(y_1, y_2, …, y_n|X)<br>= P(y_1|X)P(y_2|y_1;X)…P(y_n|y_{n-1}|X)<br>= P(y_1|X)\frac{P(y_1,y_2|X)}{P(y_1|X)P(y_2|X)} P(y_2|X) … \frac{P(y_{n-1}, y_n|X)}{P(y_{n-1}|X)P(y_n|X)} P(y_n|X)<br>$$<br>假设一中我们假设P是一个指数分布，所以此时我们引入两个函数e 和 t：e对$P(y_i|X)$建模, t 对$\frac{P(y_1,y_2|X)}{P(y_1|X)P(y_2|X)}$建模，即：<br>$$<br>P(y_i|X) = exp(e(y_i, X))<br>\frac{P(y_{i-1},y_1|X)}{P(y_{i-1}|X)P(y_i|X)} = P(y_i|X)exp(t(y_{i-i}, y_i; X))<br>$$</p><p>此时的目标就化简为：<br>$$<br>P(y_1, y_2, …, y_n|X)<br>= \frac{exp(f(y_1, y_2,…y_n; X))}{Z(X)}<br>= \frac{1}{Z(X)} exp(e(y_i,X) + t(y_1, y_2; X) + e(y_2, X) + … + t(y_{n-1}, y_n; X) + e(y_n, X))<br>$$<br>此时我们只需要对每个标签和相邻标签打分，然后将所有打分求和，即可得到总得分，然后对目标进行求解。</p><h1><span id="xian-xing-lian-crf">线性链CRF</span><a href="#xian-xing-lian-crf" class="header-anchor"></a></h1><p>虽然上面已经做了大量简化，但是求解时依然比较困难，主要是在求解t中，因为需要同时对输入X与标签$y_i$, $y_{i-1}$同时考虑，而在e中，<br>已经将输入与输出的关联进行了建模，此时我们引入线性链假设：假设t与输入X无关，则此时$ t(y_{i-1}, y_i;X) = t(y_{i-1}, y_i)$，那打分函数f简化为：<br>$$<br>f(y_1, y_2, …, y_n;X)<br>= e(y_1,X) + t(y_1, y_2) + e(y_2, X) + … + t(y_{n-1},y_n;X) + e(y_n,X)<br>$$<br>此时t就是一个待训练的参数矩阵，而e则可以通过RNNs来建模，概率分布也变为：</p><p>$$<br>P(y_1, y_2,…,y_n|x_1, x_2,…,x_n)<br>= exp(e(y_1, X) + \sum_{i=1}^{n-1}[t(y_i, y_{i+1}) + e(y_{i+1}, X)])) \frac{1}{Z(X)}<br>$$</p><h1><span id="qiu-jie">求解</span><a href="#qiu-jie" class="header-anchor"></a></h1><p>为了求解模型，我们用最大似然法， 即：</p><p>$$loss = -logP(y_1, y_2, …, y_n|x_1, x_2, …, x_n)$$<br>将上式代入：</p><p>$$loss = logZ(X) - (e(y_1, X) + \sum_{i=1}^{n-1}[t(y_i, y_{i+1}) + e( y_{i+1} , X)]) $$</p><p>减号后面的项通过添加一个待训练的参数矩阵循环计算即可得到结果，难算的前面的归一化因子Z(X)。前面我们也说了，我们此时是以路径为单位，<br>则此时Z(X) 需要我们穷举所有可能的路径比对其打分求和，而此时的路径有 $k^n$条，是指数级的，直接算效率太低，几乎不可能。<br>在假设二中，我们引入了一阶马尔可夫假设，当前标签只与前一个标签有联系，因此我们可以递归的计算归一化因子，这使得原来是指数级的计算量降低为线性级别。<a href="https://spaces.ac.cn/archives/5542/comment-page-1#comments" target="_blank" rel="noopener">原文</a><br>（这点是求解归一化因子的关键，最初我在推导时一直卡在这点上）<br>具体的: 将计算到时刻t的归一化因子记为Zt，并将它安装标签分为k个部分，即：</p><p>$$Zt = Zt^1 + Zt^2 + … + Zt^k$$<br>其中$Zt^i$表示以标签i为终点的所有路径的得分指数和，此时，我们写出递归公式：</p><p>$$Z_{t+1}^1 = (Zt^1 T_{11} + Zt^2 T_{21} + … + Zt^k T_{k1})E_{t+1}(1|X)$$</p><p>$$…  $$</p><p>$$Z_{t+1}^k = (Zt^1 T_{1k} + Zt^2 T{2k} + … + Zt^k T_{kk})E_{t+1}(k|X)$$</p><p>其中T是矩阵t的各个元素取指数形式，即$T_{ij} = exp(t_{ij})$, E是e的指数形式，即$E_{ij} = exp(e_{ij})$, 而$e_{ij}$是指时刻i时RNNs对label j的打分。<br><img src="/2019/12/26/from-softmax-to-crf/logz.png" alt="logz"></p><p>上式带有指数形式，我们取对数来简化计算过程。<br>$$<br>log(Z_{t+1}^1) = log((Zt^1 T_{11} + Zt^2 T_{21} + … + Zt^k T_{k1})E_{t+1}(1|X) )  \\<br>=log(exp(log(Zt^1) + t_{11}) + exp(log(Zt^2) + t_{21}) + … + exp(log(Zt^k) + t_{k1}) exp(e_{t+1}(1|X)))  \\<br>= log(exp(log(Zt^1) + t_{11} + e_{t+1}(1|X)) + exp(log(Zt^2) + t_{21} + e_{t+1}(1|X)) + … + exp(log(Zt^k) + t_{k1} + e_{t+1}(1|X)))\\<br>= log(\sum_k(log(Zt^k) + t_{k1} + e_{t+1}(1|X)))<br>$$</p><p>上面的过程比较曲折，对有些同学可能不太好理解，我们用一个简单的例子来帮助理解。<br>我们假设我们现在有一个输入$X=[w_0,w_1, w_2]$, 标签集合为$L=[l_1, l_2]$. RNNs对e完成了建模，即：</p><table><thead><tr><th>&nbsp;</th><th>l1</th><th>l2     </th></tr></thead><tbody><tr><td>w0</td><td>$e_{01}$</td><td>$e_{02}$ </td></tr><tr><td>w1</td><td>$e_{11}$</td><td>$e_{12}$ </td></tr><tr><td>w2</td><td>$e_{21}$</td><td>$e_{22}$ </td></tr></tbody></table><p>其中$e_{ij}$代表第i个字是第j个标签的得分。<br>转移矩阵t:</p><table><thead><tr><th>&nbsp;</th><th>l1</th><th>l2 </th></tr></thead><tbody><tr><td>l1</td><td>t11</td><td>t12  </td></tr><tr><td>l2</td><td>t21</td><td>t22  </td></tr></tbody></table><p>其中$t_{ij}$代表第i个标签转换为第j个标签的得分。<br>接下来我们按从$w_0$到$w_2$的方向一步一步来进行计算。首先，我们引入两个变量: states, cur，其中:  </p><ul><li>states代表上一个时刻计算的最终结果，即对应$log(Z_t^i)$</li><li><p>cur代表当前时刻各个标签的得分，即对应$e_t^i$</p></li><li><p><code>$w_0$:</code></p></li><li>states = None</li><li>$cur = [e_{01}, e_{02}]$<br>此时:<br>$$log(Z) = exp(e_{01}) + exp(e_{02})$$</li></ul><ul><li><code>$w_0$ –&gt; $w_1$:</code></li><li>states = $[e_{01}, e_{02}]$</li><li>cur = $[e_{11}, e{12}]$</li></ul><ol><li>扩展states:<br>$<br>states = $$\begin{pmatrix}<br>e_{01}&amp;e_{01}\\<br>e_{02}&amp;e_{02}\\<br>\end{pmatrix}$$<br>$</li></ol><ol start="2"><li>扩展cur:<br>$<br>cur = $$\begin{pmatrix}<br>e_{11}&amp;e_{12}\\<br>e_{11}&amp;e_{12}\\<br>\end{pmatrix}$$<br>$</li></ol><ol start="3"><li>将cur, states 与转移矩阵t 求和:</li></ol><p>$<br>score = $$\begin{pmatrix}<br>e_{11}&amp;e_{12}\\<br>e_{11}&amp;e_{12}\\<br>\end{pmatrix}$$<br>$+$ $$\begin{pmatrix}<br>e_{01}&amp;e_{01}\\<br>e_{02}&amp;e_{02}\\<br>\end{pmatrix}$$<br>$+$ $$\begin{pmatrix}<br>t_{11}&amp;t_{12}\\<br>t_{21}&amp;t_{22}\\<br>\end{pmatrix}$$<br>$=$ $$\begin{pmatrix}<br>e_{01} + e_{11} + t_{11} &amp; e_{01} + e_{12} + t_{12}\\<br>e_{02} + e_{11} + t_{21} &amp; e_{02} + e_{12} + t_{22}\\<br>\end{pmatrix}$$<br>$</p><ol start="4"><li>对score取指数形式然后求和，得到新的states:<br>$$<br>states = [log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21})), log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))]<br>$$<br>其中，states中的每个元素即对应着式中$logZ_{t}^i$, 此时的$log(Z)= \sum_k(exp(log(Z^k)))$:</li></ol><p>$$<br>log(Z_{0,1}) = log(exp(log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}))) + exp(log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22})))) \\<br>= log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}) + exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))<br>$$</p><p>当序列长度为2，标签有两个时，所有可能的标签序列为$(label_1-&gt;label_1, label_2-&gt;label_1, label_1-&gt;label_2, label_2-&gt;label_2)$,而对应的序列得分，即对应上式中的项，即：</p><ul><li>$ S_1: label_1-&gt;label_1:$<br>$ S_1 = e_{01} + e_{11} + t_{11} $</li><li>$S_2: label_2-&gt;label_1:$<br>$ S_2 = e_{02} + e_{11} + t_{21}$</li><li>$S_3 = label_1-&gt;label_2:$<br>$ S_3 = e_{01} + e_{12} + t_{12}$</li><li>$S_4 = label_2-&gt;label_2:$<br>$ S_4 = e_{02} + e_{12} + t_{22}$</li></ul><ul><li><code>$w_0$ -&gt; $w_1$ -&gt; $w_2$:</code></li><li>$states = [log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21})), log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))] $</li><li>$cur = [e_{21}, e_{22}]$</li></ul><p>与上面的做法一样，也分为4步：</p><ol><li>扩展states:<br>$<br>states = $$\begin{pmatrix}<br>log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}))&amp;log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}))\\<br>log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))&amp;log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))\\<br>\end{pmatrix}$$<br>$</li></ol><ol start="2"><li>cur：<br>$<br>cur = $$\begin{pmatrix}<br>e_{21} &amp;e_{22}\\<br>e_{21} &amp;e_{22}\\<br>\end{pmatrix}$$<br>$</li></ol><ol start="3"><li>将states，cur与转义矩阵t求和：</li></ol><p>$$<br>scores = \begin{pmatrix}<br>log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) &amp; log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) \\<br>log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22})) &amp; log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))\\<br>\end{pmatrix} \\+<br>\begin{pmatrix}<br>e_{21} &amp;e_{22}\\<br>e_{21} &amp;e_{22}\\<br>\end{pmatrix} \\+<br>\begin{pmatrix}<br>t_{11}&amp; t_{12}\\<br>t_{21}&amp;t_{22}\\<br>\end{pmatrix} \\=<br>\begin{pmatrix}<br>log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{21} + t_{11} &amp;log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{22} + t_{12}\\<br>log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22})) + e_{21} + t_{21} &amp;log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+x_{12}+t_{22})) + e_{22} + t_{22}\\<br>\end{pmatrix}<br>$$</p><ol start="4"><li>对score取指数形式然后求和，得到新的states:</li></ol><p>$$ states = [\\<br>log(exp(log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{21} + t_{11})+exp(log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22})) + e_{21} + t_{21})), \\<br>log(exp(log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{22} + t_{12})+ exp(log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+x_{12}+t_{22})) + e_{22} + t_{22})))] \\<br> = [log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{21} + t_{11}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{21}+t_{21})),\\<br>log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{22} + t_{12}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{22}+t_{22}))]<br>$$</p><p>现在，我们用states计算一下$(Z_2)$:</p><p>$$<br>log(Z_{(0-&gt;1-&gt;2)}) = log(exp(states[0]) + exp(states[1])) \\<br>=log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{21} + t_{11}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{21}+t_{21})) \\+<br>log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{22} + t_{12}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{22}+t_{22})) \\<br>= log(exp(e_{01}+e_{11}+t_{11}+e_{21}+t_{11}) + exp(e_{02}+e_{11}+t_{21}+e_{21}+t_{11}) \\ +<br>exp(e_{01}+e_{12}+t_{12} +e_{21}+t_{21}) + exp(e_{02}+e_{12}+t_{22}+e_{21}+t_{21}) \\+<br>exp(e_{01}+e_{11}+t_{11} +e_{22}+t_{12}) + exp(e_{01}+e_{11}+t_{11}+e_{22}+t_{12}) \\+<br>exp(e_{01}+e_{12}+t_{12} +e_{22}+t_{22}) + exp(e_{01}+e_{12}+t_{12}+e_{22}+t_{21}))<br>$$</p><p>上式也就是我们要求的最终结果$log(Z)$,其中指数内对应着所有路径的得分。<br>到此，上例中的整个归一化因子的计算过程也就完成了，而CRF中最难的部分也就解决了。</p><h1><span id="demo">demo</span><a href="#demo" class="header-anchor"></a></h1><p>搞懂了理论部分，下面写一个demo来验证一下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mask_label=False, **kwargs)</span>:</span></span><br><span class="line">        self.mask_label = <span class="number">1</span> <span class="keyword">if</span> mask_label <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        super(CRF, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.num_label = input_shape[<span class="number">-1</span>] - self.mask_label</span><br><span class="line">        self.trans = self.add_weight(name=<span class="string">'crf_trans'</span>,</span><br><span class="line">                                     shape=(self.num_label, self.num_label),</span><br><span class="line">                                     trainable=<span class="literal">True</span>,</span><br><span class="line">                                     initializer=<span class="string">'glorot_uniform'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">path_score</span><span class="params">(self, inputs, labels)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param inputs: (batch_size, timesteps, num_label), obtained from rnn(lstm, bilstm. etc.)</span></span><br><span class="line"><span class="string">        :param labels: one-hot, (batch_size, timesteps, num_label) , real target series</span></span><br><span class="line"><span class="string">        :return:  path score</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        point_score = K.sum(K.sum(inputs * labels, <span class="number">2</span>), <span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        label_pre = K.expand_dims(labels[:, :<span class="number">-1</span>], <span class="number">3</span>)</span><br><span class="line">        label_next = K.expand_dims(labels[:, <span class="number">1</span>:], <span class="number">2</span>)</span><br><span class="line">        label_trans = label_pre * label_next</span><br><span class="line">        trans = K.expand_dims(K.expand_dims(self.trans, <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">        trans_score = K.sum(K.sum(label_trans * trans, [<span class="number">2</span>, <span class="number">3</span>]), <span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> point_score + trans_score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log_norm_pre</span><span class="params">(self, inputs, states)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        expand previous states and inputs, sum with trans</span></span><br><span class="line"><span class="string">        :param inputs: (batch_size, num_label), current word emission scores</span></span><br><span class="line"><span class="string">        :param states: (batch_size, num_label), all paths  score of previous word</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        states = K.expand_dims(states[<span class="number">0</span>], <span class="number">2</span>)</span><br><span class="line">        inputs = K.expand_dims(inputs, <span class="number">1</span>)</span><br><span class="line">        trans = K.expand_dims(self.trans, <span class="number">0</span>)</span><br><span class="line">        scores = states + trans + inputs</span><br><span class="line">        output = K.logsumexp(scores, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, [output]</span><br><span class="line">        <span class="comment"># states = K.expand_dims(states[0], 2)  # (batch_size, output_dim, 1)</span></span><br><span class="line">        <span class="comment"># trans = K.expand_dims(self.trans, 0)  # (1, output_dim, output_dim)</span></span><br><span class="line">        <span class="comment"># output = K.logsumexp(states + trans, 1)  # (batch_size, output_dim)</span></span><br><span class="line">        <span class="comment"># return output + inputs, [output + inputs]</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, y_true, y_pre)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param inputs: (batch_size, timesteps, num_label)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># mask = 1 - y_true[:, 1: -1] if self.mask_label else None</span></span><br><span class="line">        <span class="comment"># # y_true, y_pred = y_true[:, :, :self.num_label], y_pre[:, :, :self.num_label]</span></span><br><span class="line">        <span class="comment"># real_path_score = self.path_score(y_pre, y_true)</span></span><br><span class="line">        <span class="comment"># init_states = [y_pre[:, 0]]</span></span><br><span class="line">        <span class="comment"># log_norm, _ = K.rnn(self.log_norm_pre, initial_states=init_states, inputs=y_pre[:, 1:], mask=mask)  # log(Z)</span></span><br><span class="line">        <span class="comment"># log_norm_score = K.logsumexp(log_norm, 1, keepdims=True)</span></span><br><span class="line">        <span class="comment"># return log_norm_score - real_path_score</span></span><br><span class="line">        mask = <span class="number">1</span> - y_true[:, <span class="number">1</span>:, <span class="number">-1</span>] <span class="keyword">if</span> self.mask_label <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        y_true, y_pre = y_true[:, :, :self.num_label], y_pre[:, :, :self.num_label]</span><br><span class="line">        init_states = [y_pre[:, <span class="number">0</span>]]  <span class="comment"># 初始状态</span></span><br><span class="line">        log_norm, _, _ = K.rnn(self.log_norm_pre, y_pre[:, <span class="number">1</span>:], init_states, mask=mask)  <span class="comment"># 计算Z向量（对数）</span></span><br><span class="line">        log_norm = K.logsumexp(log_norm, <span class="number">1</span>, keepdims=<span class="literal">True</span>)  <span class="comment"># 计算Z（对数）</span></span><br><span class="line">        path_score = self.path_score(y_pre, y_true)  <span class="comment"># 计算分子（对数）</span></span><br><span class="line">        <span class="keyword">return</span> log_norm - path_score  <span class="comment"># 即log(分子/分母)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span>  <span class="comment"># crf 只是loss，不改变inputs</span></span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y_true, y_pred)</span>:</span></span><br><span class="line">        mask = <span class="number">1</span> - y_true[:, :, <span class="number">-1</span>] <span class="keyword">if</span> self.mask_label <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        y_true, y_pred = y_true[:, :, :self.num_label], y_pred[:, :, :self.num_label]</span><br><span class="line">        isequal = K.equal(K.argmax(y_true, <span class="number">2</span>), K.argmax(y_pred, <span class="number">2</span>))</span><br><span class="line">        isequal = K.cast(isequal, <span class="string">'float32'</span>)</span><br><span class="line">        <span class="keyword">if</span> mask == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> K.mean(isequal)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> K.sum(isequal * mask) / K.sum(mask)</span><br></pre></td></tr></table></figure></p><p>结果：<br><img src="/2019/12/26/from-softmax-to-crf/result.png" alt="result"></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>初雪下的红果果</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
      <category term="CRF" scheme="https://xv44586.github.io/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>分享一个有趣的游戏</title>
    <link href="https://xv44586.github.io/2019/12/09/longton/"/>
    <id>https://xv44586.github.io/2019/12/09/longton/</id>
    <published>2019-12-09T14:35:44.000Z</published>
    <updated>2019-12-13T13:29:33.341Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>今天第一次知道了一个有趣的游戏，Langton的蚂蚁，动手自己画了一个后，决定分享一下。这个游戏的有趣体现在两个方面：<br><em>1：</em> 这个游戏是个零玩家游戏，整个过程也十分的简单：有一个像围棋盘一样画满方格的画布，初始时，整个画布都是空白，<br>一只蚂蚁在画布的某一个地方，如果当前空格为白色，则将当前空格去反，然后左转90度并前进一格；如果当前空格为黑色，则将当前空格颜色去反，<br>然后右转90度并前进一格，如此往复。<br><em>2：</em> 最后的结果非常有意思，开始时，整个画布是复杂无规律的复杂图像，很难想象是这么简单的规则产生的，但当蚂蚁走了一万步以后，整个运动过程<br>进入了循环，而图像也开始变为有规律的图像。</p><p>开始的前一百步如上图，到一万步还有点时间，所以为直接跳过中间一万步，给出一万步后的一百步。感兴趣有耐心的可以跑下代码观察一下这个有点神奇的游戏。Have fun！</p><p><img src="/2019/12/09/longton/ant.gif" alt><br>    <strong>前一百步</strong></p><p><img src="/2019/12/09/longton/ant_r.gif" alt><br>    <strong>规律出现后的一百步</strong></p><p>Langlon的蚂蚁游戏代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Date    : 2019/12/9</span></span><br><span class="line"><span class="comment"># @Author  : mingming.xu</span></span><br><span class="line"><span class="comment"># @File    : test.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> animation</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> plticker</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Direction</span><span class="params">(object)</span>:</span></span><br><span class="line">    D = [<span class="string">'E'</span>, <span class="string">'N'</span>, <span class="string">'W'</span>, <span class="string">'S'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, position, direct)</span>:</span></span><br><span class="line">        self.direct = direct</span><br><span class="line">        self.position = position</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">go_one_step</span><span class="params">(self, left=True)</span>:</span></span><br><span class="line">        lr = <span class="number">1</span> <span class="keyword">if</span> left <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">        direct = self.D[(self.D.index(self.direct) + <span class="number">1</span> * lr) % len(self.D)]</span><br><span class="line">        next_position = &#123;</span><br><span class="line">            <span class="string">'E'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>], p[<span class="number">1</span>] + <span class="number">1</span> * lr],</span><br><span class="line">            <span class="string">'N'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>] - <span class="number">1</span> * lr, p[<span class="number">1</span>]],</span><br><span class="line">            <span class="string">'W'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>], p[<span class="number">1</span>] - <span class="number">1</span> * lr],</span><br><span class="line">            <span class="string">'S'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>] + <span class="number">1</span> * lr, p[<span class="number">1</span>]]</span><br><span class="line">        &#125;</span><br><span class="line">        position = next_position[self.direct](self.position)</span><br><span class="line">        <span class="keyword">return</span> Direction(position=position, direct=direct)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N = <span class="number">100</span>  <span class="comment"># length of matrix</span></span><br><span class="line">lc, hc = <span class="number">1</span>, <span class="number">-1</span>  <span class="comment"># color for negative and positive</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">50</span>, <span class="number">50</span>))</span><br><span class="line">data = np.zeros((N, N)) + lc</span><br><span class="line">data[int(N/<span class="number">2</span>), int(N/<span class="number">2</span>)] *= <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># remove axis</span></span><br><span class="line">ax.get_yaxis().set_visible(<span class="literal">False</span>) <span class="comment">#不显示y轴</span></span><br><span class="line">ax.get_xaxis().set_visible(<span class="literal">False</span>) <span class="comment">#不显示x轴</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add text</span></span><br><span class="line">time_template = <span class="string">'step = %d'</span></span><br><span class="line">time_text = plt.text(<span class="number">0.5</span>, N+<span class="number">0.1</span>, <span class="string">''</span>, fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>: <span class="string">'red'</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#add grid</span></span><br><span class="line"><span class="comment"># myInterval = 1</span></span><br><span class="line"><span class="comment"># loc = plticker.MultipleLocator(base=myInterval)</span></span><br><span class="line"><span class="comment"># ax.xaxis.set_major_locator(loc)</span></span><br><span class="line"><span class="comment"># ax.yaxis.set_major_locator(loc)</span></span><br><span class="line"><span class="comment"># ax.grid(which='major', axis='both', linestyle='-')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># imshow</span></span><br><span class="line">ln = plt.imshow(data, animated=<span class="literal">True</span>, cmap=<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dct = Direction(position=[int(N / <span class="number">2</span>), int(N / <span class="number">2</span>)], direct=<span class="string">'N'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">()</span>:</span></span><br><span class="line">    ax.set_xlim(<span class="number">0</span>, N)</span><br><span class="line">    ax.set_ylim(<span class="number">0</span>, N+<span class="number">5</span>)</span><br><span class="line">    time_text.set_text(<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> ln, time_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move</span><span class="params">(dct, data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    当前为白色方格，则对当前方格取反，左转前进一格；若当前为黑色方格，取反后右转前进一格</span></span><br><span class="line"><span class="string">    :param dct:</span></span><br><span class="line"><span class="string">    :param mat:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    pos = dct.position</span><br><span class="line">    <span class="keyword">if</span> data[pos[<span class="number">0</span>], pos[<span class="number">1</span>]] &gt; <span class="number">0</span>:</span><br><span class="line">        data[pos[<span class="number">0</span>], pos[<span class="number">1</span>]] *= <span class="number">-1</span></span><br><span class="line">        dct_ = dct.go_one_step(left=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data[pos[<span class="number">0</span>], pos[<span class="number">1</span>]] *= <span class="number">-1</span></span><br><span class="line">        dct_ = dct.go_one_step(left=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dct_, data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(f)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> dct</span><br><span class="line">    <span class="keyword">global</span> data</span><br><span class="line">    dct, data = move(dct, data)</span><br><span class="line">    ln.set_data(data)</span><br><span class="line">    time_text.set_text(time_template % f)</span><br><span class="line">    <span class="keyword">return</span> ln, time_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">()</span>:</span></span><br><span class="line">    frame = <span class="number">0</span></span><br><span class="line">    max_step = <span class="number">11000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> frame &lt; max_step:</span><br><span class="line">        frame += <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> frame</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">anim = animation.FuncAnimation(fig, update, frames=data_gen, interval=<span class="number">10</span>,</span><br><span class="line">                               init_func=init, blit=<span class="literal">True</span>, repeat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">anim.save(<span class="string">'ant.gif'</span>, writer=<span class="string">'imagemagick'</span>, fps=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>游戏的前一百步</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Math" scheme="https://xv44586.github.io/categories/Math/"/>
    
    
      <category term="Game" scheme="https://xv44586.github.io/tags/Game/"/>
    
  </entry>
  
  <entry>
    <title>有趣的概率统计题</title>
    <link href="https://xv44586.github.io/2019/11/30/statistics/"/>
    <id>https://xv44586.github.io/2019/11/30/statistics/</id>
    <published>2019-11-30T02:28:23.000Z</published>
    <updated>2019-11-30T07:15:58.387Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#fen-xiang-yi-ge-you-qu-de-gai-lu-ti">分享一个有趣的概率题</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><p>昨晚下雪了，开心😄！</p><h1><span id="fen-xiang-yi-ge-you-qu-de-gai-lu-ti">分享一个有趣的概率题</span><a href="#fen-xiang-yi-ge-you-qu-de-gai-lu-ti" class="header-anchor"></a></h1><p>一段线段上，任意取两不重合的点，将这条线段切分成三段，问，这三条线段组成三角形点概率是多少？</p><p>现在我们将问题转化一下，假设原始线段长度为1，即$(0，1)$表示原始线段，此时，随机在$(0，1)$范围内选两个点a，b，组成$(0，a)$，$(a，b)$，$(b，1)$三个线段，<br>此时原始问题等价于现在的三个线段组成三角形的概率。<br>现在我们来考虑一下，三条线段分别为$A$， $B$，$C$，其中$A &gt;= B &gt;= C$,则A的长度必定在$[1/3, 1)$,而要组成三角形，则需要 $B + C &gt; A$,所以 B + C 的长度为$(1/2, 1)$,<br>而对应的A的长度也就在[1/3, 1/2)内，所以，最终能构成三角形的概率即为 $A_{triangle} / A_{all} = (1/2 - 1/3) / (1 - 1/3) = 1/4$</p><p>看似非常复杂的问题，从最基本的数学知识就能解答，还真是有趣！ </p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>2019年的第一场雪，摄于望京地铁站</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Math" scheme="https://xv44586.github.io/categories/Math/"/>
    
    
      <category term="Statistics" scheme="https://xv44586.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>带约束的领域词挖掘</title>
    <link href="https://xv44586.github.io/2019/11/28/domain-words/"/>
    <id>https://xv44586.github.io/2019/11/28/domain-words/</id>
    <published>2019-11-28T07:31:41.000Z</published>
    <updated>2019-12-04T01:47:13.812Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#bei-jing">背景</a></li><li><a href="#fang-an">方案</a><ul><li><a href="#shi-yan">实验</a></li><li><a href="#shi-yan-jie-guo">实验结果</a></li></ul></li><li><a href="#zong-jie">总结</a><ul><li><a href="#you-dian">优点</a></li><li><a href="#que-dian">缺点</a></li><li><a href="#fen-xiang">分享</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="bei-jing">背景</span><a href="#bei-jing" class="header-anchor"></a></h1><p>分享一个最近做的项目方案，背景是当前的内容标签是最细粒度到一级大类，而产品希望出一些三级小类（当前最细分类）的标签，而且这些标签应该是有别于其他三级小类的。</p><h1><span id="fang-an">方案</span><a href="#fang-an" class="header-anchor"></a></h1><p>我将其看作是领域词挖掘任务，只是这个任务带有一些约束，及这些领域词是个性化的，与其他小类内的领域词是不同的。<br>所以，需要做的是抽词（新词发现）+领域词挖掘+个性化判断。<br>由于之前做过新词发现，主要参考<a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="noopener">互联网时代的社会语言学：基于SNS的文本数据挖掘</a>,<br>对应的java实现<a href="https://github.com/xv44586/dict_build" target="_blank" rel="noopener">dict_build</a>。<br>除了无监督抽词外，matrix67的博文后面半部分也很有意思：通过对两份语料进行抽词，然后对词进行词频统计，通过对比词在两份语料内的词频差异，来发现“热词”。<br>回到现在的任务，个性化的词，必定在当前小类内相对于其他小类更“热”，我们可以将当前小类内的词与非当前小类进行对比，就能得到当前小类的“热词”，<br>有了热词，我们在判断这些“热词”是不是领域词，即可得到想要的结果。而实际上，由于一级大类之间的词已经有很大差异，所以，我们在抽当前小类“热词”时，不必对全量非当前小类语料进行统计，只需要对其一级大类内非当前小类语料进行统计对比，即可得到“热词”。<br>而对于领域词判断，由于我们已经有了一级大类的标签，我们可以利用这部分信息来进行领域词判断。<br>所以，最终的方案是先抽词，然后对词进行领域词判断，对词进行是否是“热词”判断，对领域词与热词进行求交运算，得到个性化的领域词。</p><h2><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h2><p>实际处理时，由于我们挖掘的“词”其实更像是一个短语，其长度也比词稍长，而“词”的组合上限是${CharCount}^{WordLength}$，而字符大概有12000+， 词平均长度假如是7，这个量也是相当大的。<br>所以抽词对内存要求很高，而且非常耗时（10h+).在思考如何优化抽词程序时，看到了<a href="https://kexue.fm/archives/6540" target="_blank" rel="noopener">分享一次专业领域词汇的无监督挖掘</a>，作者的思路与我的思路一样，不过不同的是，<br>作者在抽词时，并没有沿用matrix67的博文中根据凝合度与信息熵来进行是否成词判断，而且用信息熵是否低于某个阈值，来判断是否切开，即为两个词，这个思路还是非常巧妙的，这样，不光计算量减小了很多，而且不在需要设置超参数中ngram的值，<br>挖掘出的词也更符号“语义完整性”；而领域词判断，作者采用的是通过语料训练词向量，再由种子词来扩展领域词，也不失为一个好思路。</p><h2><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h2><p>从结果上看，效果还是很不错的。<br><img src="/2019/11/28/domain-words/words.png" alt="热词结果"><br>热词结果<br><img src="/2019/11/28/domain-words/keywords.png" alt="领域词结果"><br>领域词结果<br><img src="/2019/11/28/domain-words/result.png" alt="最终结果"><br>最终挖掘的个性化领域词</p><h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><h2><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h2><p>挖掘领域词任务中，首先是如何对”词”进行判断，然后才是”领域词”判断，所以影响最终结果好坏的主要因素也是”词”划分的质量与”领域词”判断的准确性。<br>上述方案中，通过用信息熵的最小阈值来判断相邻两个字是否属于同一个词内来进行分词，优点是速度快，能切分出高频的短语；<br>通过种子词在词向量空间来一层一层挖掘领域词，主要思路是近邻的近邻，虽然与你”直线距离”稍远，但通过你的近邻点跳转后”距离”近，这样来扩大召回，所以通过少量种子点即可召回出大量”近邻”词。</p><h2><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h2><p>缺点也很明显，首先分词时，容易将常用搭配错误切分，如html容易切出 *h/tml,的xxx容易切分为一个词，所以需要写一些规则过滤掉明显错误切分的词；种子词在召回时，由于词向量用的是word2vec这种静态词向量，近邻容易跳出当前domain，如用”华为”做种子词，直接聚类出”小米”/“苹果”，而苹果的近邻就可能跳出手机品牌，<br>转向水果，如”西瓜”/“桃子”，所以需要控制扩散的层次深度。</p><h2><span id="fen-xiang">分享</span><a href="#fen-xiang" class="header-anchor"></a></h2><p>机器学习中最重要的一个思路就是寻找差异，找到合理的差异往往是解决问题的关键。本次分享了一个通过对比差异来进行领域词挖掘的例子，希望给做类似任务的同学一些思路。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于奥森</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Programming" scheme="https://xv44586.github.io/categories/Programming/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
      <category term="新词发现" scheme="https://xv44586.github.io/tags/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/"/>
    
      <category term="领域词挖掘" scheme="https://xv44586.github.io/tags/%E9%A2%86%E5%9F%9F%E8%AF%8D%E6%8C%96%E6%8E%98/"/>
    
      <category term="信息熵" scheme="https://xv44586.github.io/tags/%E4%BF%A1%E6%81%AF%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>天猫双十一销售额相关思考</title>
    <link href="https://xv44586.github.io/2019/11/24/tm-1111/"/>
    <id>https://xv44586.github.io/2019/11/24/tm-1111/</id>
    <published>2019-11-24T01:49:06.000Z</published>
    <updated>2019-12-13T13:36:39.118Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#qi-yin">起因</a></li><li><a href="#shi-yan">实验</a></li><li><a href="#shi-yan-xiao-jie">实验小结</a></li><li><a href="#shi-me-shi-guo-ni-he">什么是过拟合</a></li><li><a href="#tong-su-de-jie-shi">通俗的解释</a></li><li><a href="#zhi-shi-zu-zhou">知识诅咒</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="qi-yin">起因</span><a href="#qi-yin" class="header-anchor"></a></h1><p>最近双十一，各大电商平台造势，宣传自己当天平台销售额，而有个网友爆料天猫销售额造假，因为自己在几个月前就已经成功预测了今年双十一的销售额，并给出了自己的模型参数（公式）。<br><a href="/2019/11/24/tm-1111/wb.jpeg">2019年11月12日网民认为尹立庆神推算的微博</a></p><h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>首先，我们按照楼主的思路做一个的实验。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tm_2009_2019 =  [<span class="number">0.50</span>, <span class="number">9.36</span>, <span class="number">52.00</span>, <span class="number">191.00</span>, <span class="number">350.00</span>, <span class="number">571.00</span>, <span class="number">912.00</span>, <span class="number">1207.00</span>, <span class="number">1682.69</span>, <span class="number">2135.00</span>]</span><br><span class="line">year = list(range(<span class="number">2009</span>, <span class="number">2019</span>))</span><br><span class="line">year = list(map(<span class="keyword">lambda</span> x: x<span class="number">-2009</span>, year))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sse</span><span class="params">(y_pre, y_true)</span>:</span></span><br><span class="line">    se = (y_pre - y_true) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> se.sum()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssr</span><span class="params">(y_pre, y_true)</span>:</span></span><br><span class="line">    sr = (y_pre - y_true.mean()) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> sr.sum()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">R</span><span class="params">(y_pre, y_true)</span>:</span></span><br><span class="line">    se = sse(y_pre, y_true) </span><br><span class="line">    sr = ssr(y_pre, y_true)</span><br><span class="line">    <span class="keyword">return</span> sr/(se+sr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(x, y, degree=<span class="number">3</span>)</span>:</span></span><br><span class="line">    x = np.array(x).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    y = np.array(y).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    pf = PolynomialFeatures(degree=degree)</span><br><span class="line">    X = pf.fit_transform(x)</span><br><span class="line">    linear_reg = LinearRegression()</span><br><span class="line">    linear_reg.fit(X, y)</span><br><span class="line">    y_pre = linear_reg.predict(X)</span><br><span class="line">    print(<span class="string">'R is: '</span>, R(y_pre, y))</span><br><span class="line">    <span class="keyword">return</span> linear_reg, pf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, pf, test_x)</span>:</span></span><br><span class="line">    test_X = pf.transform(np.array(test_x).reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> model.predict(test_X)</span><br></pre></td></tr></table></figure></p><p><img src="/2019/11/24/tm-1111/r0.png" alt="模型结果"><br>可以看到，此时的拟合优度R高达0.999，而预测今年的值为2689，确实也与今年的实际值2684基本吻合，所以网友认定自己成功的发现了天猫双十一的销售额”模型”。<br>接下来，我们做几个不一样的实验来看看其结果。<br><strong>实验1</strong><br>这次我们将数据修改一下：将第三年和倒数第三年的数据都减少10%，其他参数不变，看看模型效果。<br><img src="/2019/11/24/tm-1111/lower_9.png" alt="修改数据后模型结果"><br>拟合优度0.996，预测值为2642，与今年实际值也基本吻合。</p><p><strong>实验2</strong><br>这次我们不再使用全部数据，而只用2013-2018这六年的数据，其他不变，来看看我们的模型效果如何。<br><img src="/2019/11/24/tm-1111/r6.png" alt="六年数据模型结果"><br>拟合优度也高达0.997，而且预测值2672也与今年的实际值基本吻合。</p><p><strong>实验3</strong><br>这次我们不再使用三次，而换为二次线性方程来拟合，仍然使用原始的十年数据。<br><img src="/2019/11/24/tm-1111/degree_2.png" alt="修改模型参数后结果"><br>拟合优度0.999，预测值为2675，与今年的实际值也基本吻合。</p><h1><span id="shi-yan-xiao-jie">实验小结</span><a href="#shi-yan-xiao-jie" class="header-anchor"></a></h1><p>实验1与实验2说明，对样本做一定处理后，对模型最终的拟合影响不大，原因是在求解模型时，通常我们使用的loss是欧式距离（最小二乘估计），整体的<br>loss是对所有样本拟合loss的和，所以模型在拟合时会更”关注”值大的样本，而前几年的值与后几年相比差距一个数量级，所以不使用值很小的样本对模型<br>影响不大。而实际建模时，对于样本范围跨度很大的特征，通常我们都需要平滑而不是直接使用原始值。<br>实验3我们使用了一个参数更小的模型，$f(x) = A + Bx + Cx^2$, 而原始模型使用三次拟合，$F(x) = A + bx + Cx^2 + Dx^3$, 两个模型明显是<br>不同的，但是却都能拟合数据，这是因为这些模型都过拟合了，也就是对于一批离散点，总是能找到一个函数F，可以非常好的拟合他，这也是冯诺伊曼的那<br>个笑话：四个参数画大象，五个参数鼻子晃。</p><h1><span id="shi-me-shi-guo-ni-he">什么是过拟合</span><a href="#shi-me-shi-guo-ni-he" class="header-anchor"></a></h1><p>在这个问题上，一些网友说，这个模型肯定是有效的而不是过拟合，因为他”完美预测”了今年的真实值，模型如果只是单纯的在之前的数据上很好的拟合，而不能成功预测今年的真实值，这才是过拟合，现在模型成功预测了今年的值，所以现在的模型是有效的，不存在过拟合。<br>为了说明这个问题，我们来简单讨论一下，什么是过拟合，是不是成功预测了就不存在过拟合。<br>首先，我们来简单说明一下，一个模型的误差分为两部分，一部分是因为模型对训练数据拟合的不好带来的误差，这部分我们称为偏差；而由于抽样过程中，抽出的训练样本与整体样本分布不同引起的误差，我们称为方差。而当我们抽样时是有偏的，即抽样与整体样本分布不同，而我们的模型又对训练数据拟合的非常好，此时的模型我们就称之为过拟合，即他虽然对样本拟合的很好，但是由于样本与整体分布不同，导致模型在泛化时性能并不好，通常比在训练集上要差很多，因为毕竟模型拟合的是一个与整体不同分布的数据集，拟合的越好，泛化越差。<br>那当模型在新的样本上预测对时，是不是就不能说他过拟合呢？其实也不尽然，本质上过拟合只与你的整个过程有关，至于最终的模型在新样本上预测的准不准，并不能说明这个模型是否过拟合，毕竟瞎猫还能碰上死耗子，模型拟合的数据集虽然与整体分布不同，但是毕竟也是整体数据的一部分，预测准<br>一部分数据也正常，毕竟我们说过拟合的泛化性能低，但不是说他泛化低到全错。<br>所以对应天猫的这个案例，总的来说天猫的销售额是一个复杂的业务场景最终带来的结果，而仅仅用最终的销售额来进行模型，忽略各种业务策略的影响因素，显然与业务的真实分布相去甚远，不然也不用投入这么多人力物力，直接等着新的一年到来就好了，因为你不管怎么调整，最终的结果都是”模型”定好的。</p><h1><span id="tong-su-de-jie-shi">通俗的解释</span><a href="#tong-su-de-jie-shi" class="header-anchor"></a></h1><p>当我用上面这些论述来解释时，同事告诉我：”我不懂数学不懂机器学习，我就知道他的模型预测对了，都预测对了，你怎么能说他不准呢，还过拟合，过拟合不是预测不对才叫过拟合吗？” 这时我会换一个场景，”假如现在有个人，通过之前的十期双色球，预测对了这期对双色球，那你要不要用他的模型，重金投入买下一期的双色球呢？”</p><h1><span id="zhi-shi-zu-zhou">知识诅咒</span><a href="#zhi-shi-zu-zhou" class="header-anchor"></a></h1><p>最近学到了一个新的概念：知识的诅咒，其含义是当你知道了一件事后，你就无法想象自己是不知道这件事的。而造成这种现象的根本原因是信息的不对等。在我最初给人解释天猫销售额事件时，对面总是听不太懂我在说什么，我也搞不清楚为什么我说的这么简单直白，他会听不懂，当看到知识诅咒这个概念后，这个问题就有了答案了。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于故宫</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="机器学习" scheme="https://xv44586.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>文档查重之SimHash算法</title>
    <link href="https://xv44586.github.io/2019/10/26/simhash/"/>
    <id>https://xv44586.github.io/2019/10/26/simhash/</id>
    <published>2019-10-26T02:13:06.000Z</published>
    <updated>2019-10-26T02:31:56.413Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#simhash">SimHash</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="simhash">SimHash</span><a href="#simhash" class="header-anchor"></a></h1><p>不同网站间相互转载内容的情况非常常见，即使同一网站，不同的URL地址也可能对应相同内容，只是以不同的形式显示出来（不同的UI），而我们在爬取大量内容时，除了靠URL去重外，还需按文档内容排重<br>指纹可以判断人的身份，比如侦探把从犯罪现场采集的指纹与指纹库中的指纹做个对比，就能确定犯罪嫌疑人的身份。类似的，我们用一个文档的语义指纹来代表文档的语义，如采用一个二进制数组来代表。从而判断文档之间的相似性转化为判断两个语义指纹之间的相似性。<br>SimHash是Google在2007年发表的论文《Detecting Near-Duplicates for Web Crawling 》中提到的一种指纹生成算法或者叫指纹提取算法，被Google广泛应用在亿级的网页去重的Job中，作为locality sensitive hash（局部敏感哈希）的一种，其主要思想是降维，即将一篇若干数量的文本内容用一个长度较短的数组来表示，而这个数组与这篇文档的主要的特征所对应。如在没有犯罪嫌疑人的身份证和指纹时，一个人的特征有无数多个，而我们可以通过调查犯罪嫌疑人的姓名，性别，出生日期，身高，体重，当天穿的衣服，外貌等一些主要特征来甄别嫌疑人的身份。simhash也是将复杂的特征，降维来简化。<br>SimHash计算过程：<br><img src="/2019/10/26/simhash/sim.png" alt="simhash计算流程"></p><ul><li>1.对文档提取特征及特征对应的权重</li><li>2.对特征进行hash，生成对应的hash值</li><li>3.hash值加权：对特征hash值的每一位做循环处理：如果该位值为1，则用weight代替，否则，用-weight代替</li><li>4.求和：将特征hash加权后的结果，按位求和，然后将结果按位二值化：大于0则为1，否则为0，即得到最后的SimHash值。</li></ul><p>SimHash的计算依据是要比较的对象的特征，对于结构化的记录，可以按列提取特征；对于非结构化的文档，特征可以用全文提取topk关键词、标题、最长的几句话、每段的首句、尾句等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> analyse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimHash</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, content, topK=<span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.topK = topK</span><br><span class="line"></span><br><span class="line">        self.simhash = self.getSimHash(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getSimHash</span><span class="params">(self, content)</span>:</span></span><br><span class="line"></span><br><span class="line">        seg = jieba.cut(content)</span><br><span class="line"></span><br><span class="line"><span class="comment">#        jieba.analyse.set_stop_words('stopword.txt')</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#topk words and it's tf/idf</span></span><br><span class="line"></span><br><span class="line">        keyWords = jieba.analyse.extract_tags(</span><br><span class="line"></span><br><span class="line">            <span class="string">'|'</span>.join(seg), topK=self.topK, withWeight=<span class="literal">True</span>, allowPOS=())</span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(keyWords)</span></span><br><span class="line"></span><br><span class="line">        word_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> feature, weight <span class="keyword">in</span> keyWords:</span><br><span class="line"></span><br><span class="line">            feature = self.string_hash(feature)</span><br><span class="line"></span><br><span class="line">            temp = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> feature:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> i == <span class="string">'1'</span>:</span><br><span class="line"></span><br><span class="line">                    temp.append(weight)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">                    temp.append(-weight)</span><br><span class="line"></span><br><span class="line">            word_list.append(temp)</span><br><span class="line"></span><br><span class="line">        hashSum = np.sum(np.array(word_list), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        simhash = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> code <span class="keyword">in</span> hashSum:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> code &gt; <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">                simhash += <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">                simhash += <span class="string">'0'</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> simhash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">string_hash</span><span class="params">(self,source)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> source == <span class="string">""</span>:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">            x = ord(source[<span class="number">0</span>]) &lt;&lt; <span class="number">7</span></span><br><span class="line"></span><br><span class="line">            m = <span class="number">1000003</span></span><br><span class="line"></span><br><span class="line">            mask = <span class="number">2</span> ** <span class="number">128</span> - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> source:</span><br><span class="line"></span><br><span class="line">                x = ((x * m) ^ ord(c)) &amp; mask</span><br><span class="line"></span><br><span class="line">            x ^= len(source)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> x == <span class="number">-1</span>:</span><br><span class="line"></span><br><span class="line">                x = <span class="number">-2</span></span><br><span class="line"></span><br><span class="line">            x = bin(x).replace(<span class="string">'0b'</span>, <span class="string">''</span>).zfill(<span class="number">64</span>)[<span class="number">-64</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#            print(source,x)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>海明距离</strong></p><p>得到文档的SimHash值后，我们还需要判断两个文档是否相似。对相同长度的数字序列，我们采用海明距离来衡量其相似性。海明距离是指两个码字对应比特位（数字序列对应位置）不同的比特位个数。如1011101和1001001的第三位和第五位有差别，所以对应的海明距离为2。<br>计算两个数的海明距离时，我们先把两个数按位异或（XOR），然后计算结果中1的个数，结果就是海明距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hamDis</span><span class="params">(l1, l2)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#异或</span></span><br><span class="line"></span><br><span class="line">    lxor = int(l1,<span class="number">2</span>) ^ int(l2,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    c = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算异或结果1的个数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(lxor):</span><br><span class="line"></span><br><span class="line">        lxor &amp;= lxor<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        c += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><p>把文档转化成SimHash后，文档的排重就变成了海明距离计算问题：给出一个f位的语义指纹集合F和一个语义指纹fg，找出F中是否存在与fg只有k位差异的语义指纹。<br>当k值很小而要找的语义指纹集合中的元素不多时，可以用逐次探查法：先把所有和当前指纹差k位的指纹扩展出来，然后用折半查找法在排好序的指纹集合中查找；<br>如果是面对的是海量的数据，且动态的增加，逐次探查法的效率将越来越慢。当k值较小，如不大于3时，我们使用一种快速方法。首先，我们将64位分成4份，当k为3时，则有一份中两者相等。<br><img src="/2019/10/26/simhash/match.png" alt><br>所以我们在存储时，将数据扩展为4份，每份以其中16位为k，剩余的部分为v，查找时精确匹配这16位。<br><img src="/2019/10/26/simhash/search.png" alt><br>除此之外，对于一个已经排序的容量为$2^d$的f位指纹集合，由于指纹集合中有很多的位组合存在，所以高d位只有少量重复存在，所以在搜索时，也可以找出高d位与当前指纹相同的集合f‘，缩小查找份范围。</p><p>Simhash算法对长文本500字+比较适用，短文本可能偏差较大，最后使用海明距离，求相似，在google的论文给出的数据中，64位的签名，在海明距离为3的情况下，可认为两篇文档是相似的或者是重复的，当然这个值只是参考值，针对自己的应用可以自测取值。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于河南老家冬雪</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>分词算法综述</title>
    <link href="https://xv44586.github.io/2019/10/22/cutwords/"/>
    <id>https://xv44586.github.io/2019/10/22/cutwords/</id>
    <published>2019-10-22T14:06:05.000Z</published>
    <updated>2019-10-23T00:57:58.725Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#fen-ci-xian-zhuang">分词现状</a></li><li><a href="#ji-yu-ci-dian-de-fen-ci">基于词典的分词</a><ul><li><a href="#zui-da-pi-pei-fen-ci-suan-fa">最大匹配分词算法</a></li><li><a href="#zui-duan-lu-jing-fen-ci-suan-fa">最短路径分词算法</a><ul><li><a href="#dijkstra-suan-fa">Dijkstra算法</a></li></ul></li><li><a href="#n-zui-duan-lu-jing-fen-ci-suan-fa">N-最短路径分词算法</a></li><li><a href="#ji-yu-n-gram-model-de-fen-ci-suan-fa">基于n-gram model的分词算法</a></li></ul></li><li><a href="#ji-yu-zi-de-fen-ci">基于字的分词</a><ul><li><a href="#hmm-mo-xing">HMM模型</a><ul><li><a href="#wei-te-bi-suan-fa">维特比算法</a></li></ul></li><li><a href="#crf">CRF</a><ul><li><a href="#hmm-yu-crf-de-guan-xi-shi-shi-me">HMM与CRF的关系是什么？</a></li><li><a href="#wei-te-bi-suan-fa-yu-liang-zhe-hmm-crf-de-guan-xi-shi-shi-me">维特比算法与两者（HMM/CRF）的关系是什么？</a></li></ul></li><li><a href="#shen-jing-wang-luo-fen-ci-suan-fa">神经网络分词算法</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="fen-ci-xian-zhuang">分词现状</span><a href="#fen-ci-xian-zhuang" class="header-anchor"></a></h1><p>NLP的底层任务由易到难大致可以分为词法分析、句法分析和语义分析。分词是词法分析（还包括词性标注和命名实体识别）中最基本的任务，可以说既简单又复杂。说简单是因为分词的算法研究已经很成熟了，大部分的准确率都可以达到95%以上，说复杂是因为剩下的5%很难有突破，主要因为三点：</p><ul><li><ol><li>粒度，不同应用对粒度的要求不一样，比如“苹果手机”可以是一个词也可以是两个词。</li></ol></li><li><ol start="2"><li>歧义，比如“下雨天留人天留我不留”。</li></ol></li><li><ol start="3"><li>未登录词，比如“skrrr”、“打call”等新兴词语。<br>分词算法根据其核心思想主要分为两种，第一种是基于字典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。归根结底，上述两种方法都可以归结为在图或者概率图上寻找最短路径的问题。</li></ol></li></ul><h1><span id="ji-yu-ci-dian-de-fen-ci">基于词典的分词</span><a href="#ji-yu-ci-dian-de-fen-ci" class="header-anchor"></a></h1><h2><span id="zui-da-pi-pei-fen-ci-suan-fa">最大匹配分词算法</span><a href="#zui-da-pi-pei-fen-ci-suan-fa" class="header-anchor"></a></h2><p>最大匹配分词寻找最优组合的方式是将匹配到的最长词组合在一起。主要的思路是先将词典构造成一棵Trie树，也称为字典树。<br><img src="/2019/10/22/cutwords/max.jpeg" alt="image.jpeg"> </p><p>Trie树由词的公共前缀构成节点，降低存储空间同时提升查找效率。最大匹配分词将句子与Trie树进行匹配，在匹配到根节点后由下一个字重新开始查找。最大匹配又分为前向匹配和后向匹配算法，如上图中，对语句“他说的确实在理”，前向最大匹配的结果“他/说/的确/实在/理，而反向最大匹配结果”他/说/的/确实/在理“。通过词典最大匹配时间效率虽然高，但是效果很差，实际一般很少使用这种方法。</p><h2><span id="zui-duan-lu-jing-fen-ci-suan-fa">最短路径分词算法</span><a href="#zui-duan-lu-jing-fen-ci-suan-fa" class="header-anchor"></a></h2><p>最短路径分词算法首先将一句话中的所有词匹配出来（基于词典），构成词图（有向无环图DAG），然后寻找从起始点到终点的最短路径作为最佳组合方式。<br><img src="/2019/10/22/cutwords/min.jpeg" alt="image.jpeg"> </p><p>图中所有词的权重都是相等的，所以每条边的权重都是1.<br>DAG图的最短路径问题，可以描述为 在无向图 $G=(V,E)$ 中，假设每条边 $E[i]$ 的长度为 $w[i]$，找到由顶点 V0 到其余各点的最短路径（单源最短路径）。假设源点为S，节点集合为V，终点为E。对于最短路径P(S,E)中的中间节点，其源点到其的最短路径也在P（SE）内。即：假如S-&gt;A-&gt;B-&gt;C-&gt;E是最短路径，那S-&gt;A-&gt;B一定是S到B的最短路径，否则，将会存在一点F，使得d(S-&gt;F-&gt;B)&lt;d(S-&gt;A-&gt;B),而最短路径P1(S,E)=S-&gt;F-&gt;B-&gt;C-&gt;E将比P(S,E)短，从而与假设矛盾。因此，求解DAG可以利用最优子结构，通过贪心或者动态规划来求解。</p><h3><span id="dijkstra-suan-fa">Dijkstra算法</span><a href="#dijkstra-suan-fa" class="header-anchor"></a></h3><p>Dijkstra算法本质是贪心算法，每一步求解最短路径节点，然后递推更新源节点到其他节点的距离。<br>算法步骤：</p><ul><li>a.初始时，S只包含源点，即S＝{v}，v的距离为0。U包含除v外的其他顶点，即:U={其余顶点}，若v与U中顶点u有边，则&lt;u,v&gt;正常有权值，若u不是v的出边邻接点，则&lt;u,v&gt;权值为∞。</li><li>b.从U中选取一个距离v最小的顶点k，把k，加入S中（该选定的距离就是v到k的最短路径长度）。</li><li>c.以k为新考虑的中间点，修改U中各顶点的距离；若从源点v到顶点u的距离（经过顶点k）比原来距离（不经过顶点k）短，则修改顶点u的距离值，修改后的距离值的顶点k的距离加上边上的权。</li><li>d.重复步骤b和c直到所有顶点都包含在S中。<br><img src="/2019/10/22/cutwords/dij.gif" alt="image.gif"> </li></ul><p>Dijkstra算法的结果为”他/说/的/确实/在理“，可见最短路径分词可以解决大部分问题，但是当最短路径存在多条时，Dijkstra只保存一条，这种策略即缺乏理论依据也对其他路径不公平。</p><h2><span id="n-zui-duan-lu-jing-fen-ci-suan-fa">N-最短路径分词算法</span><a href="#n-zui-duan-lu-jing-fen-ci-suan-fa" class="header-anchor"></a></h2><p>N-最短路径分词是对Dijkstra算法的扩展，他在每一步都保存最短的N条路径（beam search），同时记录当前节点的前驱，最后求得最优解时回溯得到最短路径，该算法结果优于Dijkstra，但是时间与空间复杂度上更大。<br><img src="/2019/10/22/cutwords/nshort.png" alt="image.png"></p><h2><span id="ji-yu-n-gram-model-de-fen-ci-suan-fa">基于n-gram model的分词算法</span><a href="#ji-yu-n-gram-model-de-fen-ci-suan-fa" class="header-anchor"></a></h2><p>前文中边的权重都是1，但实际中不同的词出现的频率/概率不同，其成词的概率也就相应不同，因此将求解词图最短路径的问题转化为求解最大概率路径问题。即将句子切分为”最有可能的词的组合“。而计算词出现的概率，就需要语料对”语言“进行统计建模。<br>语言模型是对一句话出现的概率进行建模，根据条件概率：<br>p(他说的确实在理) = p(他)p(说|他)p(的|他说)…p(理|他说的确实在)<br>上述计算过于庞大，一般我们采用n-gram来近似，如2-gram。<br>然后我们将语言模型得到的概率分布应用到词图，可以得到词图的概率图 ：</p><p><img src="/2019/10/22/cutwords/dag.jpeg" alt="image.jpeg"></p><p>利用上面两种求解DAG最短路径的方法进行求解即可。</p><h1><span id="ji-yu-zi-de-fen-ci">基于字的分词</span><a href="#ji-yu-zi-de-fen-ci" class="header-anchor"></a></h1><p>与基于词典的分词不同，基于字的分词事先不对句子进行词匹配，而是将分词看成是给句子中的每个字打上标签的序列标注问题，可以看成是对每个字的分类问题。 比如通过4标签来进行标注（single，单字成词；begin，多字词的开头；middle，三字以上词语的中间部分；end，多字词的结尾。均只取第一个字母。），这样，“为人民服务”就可以标注为“sbebe”了。 4标注不是唯一的标注方式，类似地还有6标注，理论上来说，标注越多会越精细，理论上来说效果也越好，但标注太多也可能存在样本不足的问题，一般常用的就是4标注和6标注。</p><h2><span id="hmm-mo-xing">HMM模型</span><a href="#hmm-mo-xing" class="header-anchor"></a></h2><p>HMM模型认为在解决序列标注问题时，存在两种序列，一种是观测序列，是人显性观察到的句子，而标签是隐状态序列，即观测状态为X，隐藏状态序列是Y，因果关系是Y-&gt;X.<br>我们用λ=λ1λ2…λn表示输入的句子，用o= o1o2…on表示对应的label，那最优的输出是什么呢？从概率的角度，我们希望下面的条件概率最大：</p><p>$max P(o∣λ)=max P(o1o2…on∣λ1λ2…λn)$<br>也就是，o有很多种可能，但是最优的是o应该是概率最大的o。而上式的概率是关于2n个变量的条件概率，而且不同的输入有不同的n，精确计算基本不太可能，所以，我们为了简化问题，先提出第一个假设：输出只与对应的输入有关。于是，上式就可以简化为：<br>$maxP(o∣λ)=max P(o1∣λ1)P(o2∣λ2)…P(on∣λn)$<br>现在，上式的求解就简单多了，只要让每个<strong>P(ok|λk)</strong>最大，即可得到最大的$P(o|λ)$.<br>上面的方案是一个简化方案，但是完全没有考虑上下文，这样会出现很多不合理的结果，比如按照我们4标注方法，b后只能接m或者e，而上述方案不考虑上下文，就可能出现bb\bs等错误结果。这些都是不合理的结果。而产生这种结果的原因是我们没有考虑label之间的关系，所以，我们需要换个角度，反过来考虑这个问题。<br>首先，利用贝叶斯公式：<br><img src="/2019/10/22/cutwords/bys.png" alt="image.png"></p><p>由于λ是给定的输入，那么P(λ)就是常数，我们忽略他。现在，最大化P(o|λ)~P(o)P(λ|o).此时，我们的模型是考虑了输出之间的关系P(o),就可以避免一些不合理的情况的出现。</p><p>$P(o)P(λ|o)=P(o1o2…on)P(λ1λ2…λn|o1o2…on)$<br>根据我们第一个假设，我们得到：<br>$P(λ|o)=P(λ1|o1)P(λ2|o2)…P(λn|on)$<br>对于$P(o)$,有<br>$P(o) = P(o1)P(o2|o1)…P(on|o1o2..on-1)$<br>此时，我们提出第二个假设：输出状态只与前一个状态有关。<br>$P(o) = P(o1)P(o2|o1)…P(on|on-1)$<br>此时，<br>$P(o|λ)~P(λ1|o1)P(o2|o1)P(λ2|o2)P(o3|o2)…P(on|on-1)P(λn|on)$<br>其中，$P(λk|ok)$称为发射概率，$P(on|on-1)$称为转移概率。对于不合理的输出序列，如bb，我们可以通过设置其状态转移概率为0来避开。<br>接下来的HMM模型的训练，就是如何求解发射概率与转移概率了。<br>学习训练主要有极大似然估计和 Baum-Welch(前向后向)两种算法，通常用极大似然估计。如果有一批标注好的语料，这俩个概率的估计就非常简单了，但通常我们没有大批标注好的语料，我们可以通过词典和未标注的语料来估计，如果没有语料，我们可以通过词典来估计发射概率，状态转移概率人为给一个。<br>得到了发射概率和转移概率，此时就是计算给定输入的最大概率的输出序列。设输入序列长度为L, 状态（label)有n种，则有 $n^L$种可能的排列方式，此时计算最大概率的复杂度为$O(Ln^L)$，这个量太大了，不可取，于是，维特比算法登场了。</p><h3><span id="wei-te-bi-suan-fa">维特比算法</span><a href="#wei-te-bi-suan-fa" class="header-anchor"></a></h3><p>维特比算法是HMM，CRF中的经典解码算法，其核心是通过DP思想，在每个时间点的每个label上存储到该位置的最大概率，将计算复杂度降低为 $O(L^2n)$。其流程图如下:<br><img src="/2019/10/22/cutwords/vtb.jpeg" alt="image.jpeg"></p><p>算法流可能不够直观，下面来通过一个例子来更好的理解。<br><img src="/2019/10/22/cutwords/vtb_demo.gif" alt="image.gif"></p><p>图中X是观察序列（输入序列），Y是标签矩阵。T1为每个时间点每个label的最大概率，</p><p><img src="/2019/10/22/cutwords/ti.png" alt="image.png"></p><p>T2为到达该时间点该label最大概率的前驱label（上个时间点上）</p><p><img src="/2019/10/22/cutwords/t2.png" alt="image.png"></p><p>到达最后一个时间点后，求解最大概率对应的label，然后通过T2回溯，将对应前驱节点加入path直到到达起始时间点，得到的path即为最优解的逆序，反转即可得到最优路径（最大概率的输出标签序列）。</p><h2><span id="crf">CRF</span><a href="#crf" class="header-anchor"></a></h2><p>CRF可以看作一个无向图模型，对于给定的标注序列Y和观测序列X，对条件概率P(Y|X)进行建模。首先，我们讨论一下有向图与无向图概率模型。</p><p>有向图<br><img src="/2019/10/22/cutwords/dg.png" alt="image.png"> </p><p>上图是一个广义的有向图，他们的联合概率数学表示为：<br><img src="/2019/10/22/cutwords/px.png" alt="image.png"> </p><p>无向图<br><img src="/2019/10/22/cutwords/ug.png" alt="image.png"> </p><p>上图是一个广义的无向图，而求解时，需要将其分解为若干个“小团”的联合概率的乘积，而每个小团必须是“最大团”（子图里任意两个节点是相连的，且没有其他节点能加入使他成为跟大的团），则有：</p><p><img src="/2019/10/22/cutwords/py.png" alt="image.png"> </p><p>其中<br><img src="/2019/10/22/cutwords/zx.png" alt><br>用来归一化，将结果转化为概率。<br>上图的无向图的联合概率表示为：<br><img src="/2019/10/22/cutwords/pyu.png" alt="image.png"> </p><p>其中<img src="/2019/10/22/cutwords/c.png" alt="image.png"></p><p>是最大团C上随机变量的联合概率，形式一般取指数函数：<br><img src="/2019/10/22/cutwords/yc.png" alt="image.png"> </p><p>上式也叫势函数。</p><p>无向图的联合概率分布在因子分解下表示为：<br><img src="/2019/10/22/cutwords/pyz.png" alt="image.png"> </p><p>上式中化简为各个团的联合概率的乘积由 Hammersly-Clifford law保证。</p><p>广义的CRF的定义是： 满足 <img src="/2019/10/22/cutwords/pyc.png" alt><br> 的马尔科夫随机场叫做条件随机场（CRF），一般我们说的是线性链条件随机场，定义为：<br>$P(Y|X) = P(Yv|X, Yv-1, Yv+1)$<br>下面来看看我们在做序列建模时，对应的图结构。<br><img src="/2019/10/22/cutwords/arc.png" alt="image.png"> </p><p>上图是一个序列建模(线性链CRF)时，对应结构示意图，为了求解上图中的概率，我们需要对上图进行因子分解，分解成若干个“最大团”，而上图结构中，每个“最大团”都是由一对（Ii~Oi)组成，即每个“最大团”C与位置I对应，c=i，而且线性链CRF满足<br><img src="/2019/10/22/cutwords/pio.png" alt="image.png"><br>，我们带入无向图的联合概率分布的因子分解，对应的CRF建模公式为：<br><img src="/2019/10/22/cutwords/pioz.png" alt="image.png"> </p><p>公式内i是对应当前节点对应位置；k表示第几个特征函数，每个特征函数对应一个特征权重λk, Z(o)的作用是归一化为概率。</p><p>对于特征函数，我们可以定义两类特征函数：转移特征和状态特征，我们可以将上式展开为：<br><img src="/2019/10/22/cutwords/unroll.png" alt="image.png"> </p><p>其中tj为转移特征，对应权重为λj, sl为状态特征，对应权重为ul。一般我们不分开看，将对应特征函数统一起来。</p><p>CRF学习过程中，需要提前定义特征函数，然后根据样本来学习对应特征的权重，所以CRF中比较困难的点在于需要定义大量的特征函数，特征函数的质量直接影响最后的模型质量。<br>下面借一个列子（CRF++）来说明一下：<br>首先，我们会定义相应的标签数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"tags"</span>: [</span><br><span class="line"><span class="string">"B"</span>,</span><br><span class="line"><span class="string">"E"</span>,</span><br><span class="line"><span class="string">"M"</span>,</span><br><span class="line"><span class="string">"O"</span>,</span><br><span class="line"><span class="string">"S"</span>]</span><br></pre></td></tr></table></figure></p><p>然后我们会定义特征模版，通过特征模版，会帮助我们产生相应的特征函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Unigram</span><br><span class="line">U00:%x[-2,0]</span><br><span class="line">U01:%x[-1,0]</span><br><span class="line">U02:%x[0,0]</span><br><span class="line">U03:%x[1,0]</span><br><span class="line">U04:%x[2,0]</span><br><span class="line">U05:%x[-1,0]/%x[0,0]</span><br><span class="line">U06:%x[0,0]/%x[1,0]</span><br><span class="line"># Bigram</span><br><span class="line">B</span><br></pre></td></tr></table></figure></p><p>其中，我们定义了7个U系列模板（Unigram）和一个B系列（Bigram）模板，其中U00:%x[-2,0]代表U系列，索引为00，对应特征是当前位置的左侧第二个位置(-2)token和当前位置的token，最后的B代表只有一个B系列特征，即只有当前位置的前一个位置的输出token(标签)与当前位置的token（标签）组成的Bigram一个特征（转移概率）。接下来看看对应学习后输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&quot;feature_func_weight&quot;:&#123;</span><br><span class="line">&quot;U006: 等&quot;:[</span><br><span class="line">0.901,</span><br><span class="line">-0.003,</span><br><span class="line">0.311,</span><br><span class="line">-0.01,</span><br><span class="line">-0.006</span><br><span class="line">],</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>上面这个代表的是什么呢？就是对应第“006”的“U”系列特征是“等”字时，对应五个（BEMOS)对应得分，通过模板，会对每个字都生成对应的6个特征，每个特征都有对应的特征函数权重。</p><h3><span id="hmm-yu-crf-de-guan-xi-shi-shi-me">HMM与CRF的关系是什么？</span><a href="#hmm-yu-crf-de-guan-xi-shi-shi-me" class="header-anchor"></a></h3><p>在介绍两者区别之前，我们先来讨论一下机器学习中的两种不同的模型：生成式模型与判别式模型。<br>在监督学习下，模型可以分为判别模型和生成模型。判别模型通过features对labels刻画边界，即他直接对P(X|Y)进行建模；而生成模型通过训练样本，确定数据的整体分布情况，即他直接对P(X,Y).</p><p><img src="/2019/10/22/cutwords/dis.png" alt="image.png"></p><p>我们的目标是对P(o|λ)进行求解，在HMM中，我们通过贝叶斯公式，将问题转化为：<br><img src="byshmm.png" alt="image.png"></p><p>分母是常数，被我们忽略了，本质上是对分子进行建模，即对P(o,λ)进行建模，所以是一个生成模型。而CRF，直接对P(o|λ)进行建模，是一个判别模型。<br>除此之外，CRF可以解决HMM能解决的一切问题，还能解决许多HMM解决不了的问题。为了说明这一点，我们来看看他们的数学表达形式。HMM模型的数学表达为：<br><img src="/2019/10/22/cutwords/pls.png" alt="image.png"></p><p>如果我们将HMM模型取对数，可以得到：<br><img src="/2019/10/22/cutwords/logp.png" alt="image.png"></p><p>而CRF的未归一化的log形式（特征函数）的数学表达为：<br><img src="/2019/10/22/cutwords/score.png" alt="image.png"><br>如果将HMM中的log形式看作是CRF中的特征权重，则两者具有相同的形式，即每一个HMM都对应某一个形式的CRF。</p><h3><span id="wei-te-bi-suan-fa-yu-liang-zhe-hmm-crf-de-guan-xi-shi-shi-me">维特比算法与两者（HMM/CRF）的关系是什么？</span><a href="#wei-te-bi-suan-fa-yu-liang-zhe-hmm-crf-de-guan-xi-shi-shi-me" class="header-anchor"></a></h3><p>维特比算法是一类问题的解法，而不是固定只与HMM\CRF搭配。CRF与HMM都有两个过程，一个是学习阶段，一个是解码阶段。学习阶段确定模型参数，解码阶段求解最大概率路径。解码过程中，HMM与CRF（线性链CRF）从模型定义上看，每个位置的得分都是依赖前一个位置的信息（标签信息），所以整体是一个递推过程，而这个过程在求解最大概率路径时，如果是穷举，在上文也讨论了，复杂度为O(L<em>n<strong>L)，而维特比算法将复杂度降低为O(L</strong>2</em>n)。所以维特比算法是刚好适用于HMM/CRF解码过程的一种算法。</p><h2><span id="shen-jing-wang-luo-fen-ci-suan-fa">神经网络分词算法</span><a href="#shen-jing-wang-luo-fen-ci-suan-fa" class="header-anchor"></a></h2><p>对于序列建模这类问题，RNN有着天然的优势，可以很方便的处理变长输入和序列输入，目前对于序列标注问题，公认效果最好的模型是BiLSTM+CRF,对应的模型结构为：<br><img src="/2019/10/22/cutwords/lstm.png" alt="image.png"></p><p>相比于其他模型，BiLSTM可以学习到上下文相关信息，可以很好的解决序列输入问题，但BiLSTM只学习到了输入的上下文信息，对应标注的上下文信息却完全没有用到，所以正如HMM中我们讨论的一样，会出现很多不合理的标签序列，如b\b\b这种，所以，在后面接一个CRF来避免这种情况的出现。</p><p><strong>参考</strong><br><a href="https://www.cnblogs.com/biyeymyhjob/archive/2012/07/31/2615833.html" target="_blank" rel="noopener">https://www.cnblogs.com/biyeymyhjob/archive/2012/07/31/2615833.html</a><br><a href="https://baike.baidu.com/item/%E8%BF%AA%E6%9D%B0%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95/4049057?fr=aladdin#reference-[1]-1712262-wrap" target="_blank" rel="noopener">https://baike.baidu.com/item/%E8%BF%AA%E6%9D%B0%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95/4049057?fr=aladdin#reference-[1]-1712262-wrap</a><br><a href="https://wenku.baidu.com/view/da6e0921af45b307e8719702.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/da6e0921af45b307e8719702.html</a><br><a href="https://zhuanlan.zhihu.com/p/63087935" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63087935</a><br><a href="https://www.cnblogs.com/pinard/p/7048333.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/7048333.html</a><br><a href="https://www.zhihu.com/question/35866596/answer/236886066" target="_blank" rel="noopener">https://www.zhihu.com/question/35866596/answer/236886066</a><br><a href="https://www.cnblogs.com/pinard/p/7048333.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/7048333.html</a><br><a href="https://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/" target="_blank" rel="noopener">https://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/</a><br><a href="https://www.zhihu.com/question/20279019" target="_blank" rel="noopener">https://www.zhihu.com/question/20279019</a><br><a href="http://taku910.github.io/crfpp/" target="_blank" rel="noopener">http://taku910.github.io/crfpp/</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于北京欢乐谷</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>词向量总结</title>
    <link href="https://xv44586.github.io/2019/10/22/w2v-summary/"/>
    <id>https://xv44586.github.io/2019/10/22/w2v-summary/</id>
    <published>2019-10-22T01:07:15.000Z</published>
    <updated>2019-11-27T01:44:41.318Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#yi-tong">异同</a><ul><li><a href="#xiang-tong-dian">相同点</a></li><li><a href="#bu-tong-dian">不同点</a></li></ul></li><li><a href="#xing-zhi">性质</a><ul><li><a href="#pmi-jiao-du">PMI角度</a></li><li><a href="#ke-jia-xing">可加性</a></li><li><a href="#mo-chang">模长</a></li><li><a href="#xiang-guan-xing">相关性</a></li></ul></li><li><a href="#ying-yong">应用</a><ul><li><a href="#liang-ge-ju-zi-de-xiang-guan-xing">两个句子的相关性</a></li><li><a href="#zhong-xin-ci-guan-jian-ci-ti-qu">中心词（关键词）提取</a></li><li><a href="#ju-xiang-liang">句向量</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="yi-tong">异同</span><a href="#yi-tong" class="header-anchor"></a></h1><p>本文主要讨论Glove和word2vec两种模型对应词向量。</p><h2><span id="xiang-tong-dian">相同点</span><a href="#xiang-tong-dian" class="header-anchor"></a></h2><ul><li>两种模型都是在对词对的PMI做分解，所以他们具有相同的性质（向量可加性，点积，余弦距离，模长等）。</li><li>模型的基本形式都是向量的点积，且都有两套词向量空间（单词向量空间与上下文词向量空间）。</li></ul><h2><span id="bu-tong-dian">不同点</span><a href="#bu-tong-dian" class="header-anchor"></a></h2><ul><li>1.通常我们都是根据模型来推导其对应性质，而Glove是通过其性质来反推模型，这种方式还是给人眼前一亮的。</li><li>2.除Glove以外的词向量模型都是对条件概率$P(w|context)$进行建模，如word2vec的SkipGram对$P(w2|w1)$进行建模，但是这个信息是有缺点的，首先，他不是一个严格对称的模型，即P(w2|w1)  与 P(w1|w2) 并不一定相等，所以，在建模时需要把上下文与中心词向量区分开，不能放到同一个向量空间；其次，这个概率是有界的、归一化的量，所以在模型里需要用softmax等对结果进行归一化，这个也会造成优化上的困难。而Glove可以看作是对PMI进行建模，而PMI是比概率更对称也更重要的一个量。</li><li>3.如Glove论文中所述，就整体目标函数而言，可以看作是两种模型采用了不同的损失函数，其基本形式是一致的。</li><li>4.两种模型都是词袋模型（一元模型）。</li><li>5.对应词向量内积含义不同：<br><img src="/2019/10/22/w2v-summary/dot.png" alt="内积含义"></li></ul><h1><span id="xing-zhi">性质</span><a href="#xing-zhi" class="header-anchor"></a></h1><h2><span id="pmi-jiao-du">PMI角度</span><a href="#pmi-jiao-du" class="header-anchor"></a></h2><p>上文说了两种模型都是对词对的PMI做分解，所以我们在解释其性质时直接从PMI的角度来解释。<br>首先来看一下PMI<br><img src="/2019/10/22/w2v-summary/pmi.png" alt="PMI"><br>对于任意两个词序列Q和A，其中$Q=(q1,q2…qk)$, $A = (a1, a2…al)$，我们模型都是采用的词袋模型，即满足朴素假设：每个特征之间相互独立。<br><img src="/2019/10/22/w2v-summary/pqa.png" alt><br>带入朴素假设<br><img src="/2019/10/22/w2v-summary/naive.png" alt><br>用贝叶斯公式变换<br><img src="/2019/10/22/w2v-summary/change.png" alt><br>再用一次朴素假设<br><img src="/2019/10/22/w2v-summary/naive-change.png" alt></p><p>最后得到：<br><img src="/2019/10/22/w2v-summary/pmiqa.png" alt><br>即在朴素假设下，两个序列的互信息等于两个序列中各个项的互信息的总和。</p><h2><span id="ke-jia-xing">可加性</span><a href="#ke-jia-xing" class="header-anchor"></a></h2><p>在Glove和word2vec中，两个词之间的相关性是通过对应词向量的内积来表达的，即对于词Wi, Wj, 其相关性等于$<vi ,vj>$, 带入上面，即：<br><img src="/2019/10/22/w2v-summary/pmiin.png" alt><br>即两个词序列的相关性可以通过将两个序列内的词向量求和后再进行点积计算。<br>如我们求两个句子的相关度时，可以先将句子内的词对应的词向量进行求和，然后再进行相似性计算。</vi></p><h2><span id="mo-chang">模长</span><a href="#mo-chang" class="header-anchor"></a></h2><p>词向量w的模长正比与其内积&lt;w,w&gt;，即正比PMI(w,w)，而在一个滑动窗口内，上下文中的词与中心词相等的概率极低，所以可以认为P(w,w) ~ P(w),推出<br><img src="/2019/10/22/w2v-summary/pww.png" alt><br>即，模长正比与词频的倒数，词频越高（停用词，虚词等），其对应的模长越短，这样就表面模长能在一定程度上代表词本身的重要性。<br>从模型学习的角度来看，词向量的内积等于其模长的乘积乘以余弦值，即<br><img src="/2019/10/22/w2v-summary/cos.png" alt><br>对于高频的几乎没有什么固定搭配的词，其所含语义也相对非常少，即这些词与其他任意词的互信息都非常低，约等于0，而为了让上式等于0，与其不停的调节两个向量的方向，不如让其中一个的模长像0靠近，这样经过多次迭代后，高频的语义少的词的模长就越来越短，逐渐接近0.<br>实验结果中，也能看到按模长排序后，前面的都是高频的语义含量极低的词。<br><img src="/2019/10/22/w2v-summary/sort.png" alt="模长排序结果"><br>可以看到，排在前面的都是高频的语义极少的词（’UNK’，’以及’,’三’，符号等)</p><h2><span id="xiang-guan-xing">相关性</span><a href="#xiang-guan-xing" class="header-anchor"></a></h2><p>两个词的互信息正比于词对应向量的内积，即两个词互信息越大，两个词成对出现的几率越高，其对应词向量的内积也就越大，因此，可以通过内积来对词的相关性进行排序。而上面也说了，模长代表了词的重要程度，如果我们不考虑词本身的重要程度，只考虑其词义，可以用向量范数将其归一化后在进行内积计算，这样更稳定.<br><img src="/2019/10/22/w2v-summary/cosij.png" alt><br>即词的相关性可以用词向量之间的余弦距离来计算，这样比只使用内积更稳定。<br>在统计上，互信息为0，则表面这两个词无关，对应到模型，即两个词的词向量的内积为0，而根据向量的知识，两个向量的内积为0，则表明两个向量相互垂直，即两个向量无关。两个词在统计上的无关正好对应其在词向量空间上的几何无关！</p><h1><span id="ying-yong">应用</span><a href="#ying-yong" class="header-anchor"></a></h1><h2><span id="liang-ge-ju-zi-de-xiang-guan-xing">两个句子的相关性</span><a href="#liang-ge-ju-zi-de-xiang-guan-xing" class="header-anchor"></a></h2><p>计算两个句子或短语之间的相关性时，我们可以借鉴上面PMI在朴素假设下的性质，将两个句子中的词向量进行求和，再计算两个结果向量之间的相关性，如点积或余弦。<br>而如果一个句子内的词向量的和与某一个词的词向量相关性非常高，可以认为这个句子与这个词表达了相同的语义，或者，在词向量空间内，词对应的向量在句子的聚类中心附近。</p><h2><span id="zhong-xin-ci-guan-jian-ci-ti-qu">中心词（关键词）提取</span><a href="#zhong-xin-ci-guan-jian-ci-ti-qu" class="header-anchor"></a></h2><p>所谓中心词（关键词），即能概况句子的意思，通过这些词，我就能大概猜到整个句子的整体内容。即这些词（相对句子内其他词）对整个句子的相关性更高。这样就能将问题转化成词与句子相关性排序问题。<br>通过语言模型的角度来看，语言模型本身就是一个通过上（下）文来预测下一个词概率的模型，即最大化$P(w1,w2,w3..|W)$.而关键词的含义是什么呢？用数学的方式表达就是：<br>对于$S=(w1,w2, w3..wk)$,求解%P(S|wk)$值最高的$wk$,其中$wk$属于$S$。这其实与语言模型的含义是一致的。<br>最终，将问题转化成词与句子之间的相关性排序问题，而上面提到求解两个句子相关性时，可以将句子对应词向量先求和再计算相关性，最后关键词提取就变成：<br>先将句子对应词向量求和，得到sen_vec，然后计算单个词与sen_vec的相关性，然后排序即可。<br><img src="/2019/10/22/w2v-summary/keywords.png" alt><br>可以看到结果还是相当不错的。</p><h2><span id="ju-xiang-liang">句向量</span><a href="#ju-xiang-liang" class="header-anchor"></a></h2><p>上面在做句子相关性时，都是为了将计算从$O(n^2)$降低到O(n)而将句子内的词向量进行求和，然后再计算。其实也就是用词向量的求和来得到句向量，来作为句子在相同词向量空间的语义。其实这是一种简单又快捷的得到句向量的方式，在很多任务中都可以尝试使用。</p><p><strong>refer</strong><br><a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" target="_blank" rel="noopener">neural-word-embedding-as-implicit-matrix-factorization</a><br><a href="https://aclweb.org/anthology/P17-1007" target="_blank" rel="noopener">https://aclweb.org/anthology/P17-1007</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于秦皇岛</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
      <category term="Thinking" scheme="https://xv44586.github.io/tags/Thinking/"/>
    
  </entry>
  
  <entry>
    <title>深入谈谈word2vec</title>
    <link href="https://xv44586.github.io/2019/10/21/deep-w2v/"/>
    <id>https://xv44586.github.io/2019/10/21/deep-w2v/</id>
    <published>2019-10-21T01:02:56.000Z</published>
    <updated>2019-10-22T01:24:14.513Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#nnlm-fu-za-du">NNLM复杂度</a></li><li><a href="#you-hua-fang-an">优化方案</a><ul><li><a href="#1-hierarchical-softmax">1.Hierarchical softmax</a></li><li><a href="#2-negative-sampling">2.negative sampling</a></li></ul></li><li><a href="#si-chong-xun-lian-fang-an">四种训练方案</a></li><li><a href="#si-kao">思考</a></li><li><a href="#hui-da">回答</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="nnlm-fu-za-du">NNLM复杂度</span><a href="#nnlm-fu-za-du" class="header-anchor"></a></h1><p>原始的NNLM在训练词向量时非常耗时，尤其是大规模语料上，作者在论文后也提出了可能的优化方案，所以word2vec的关注点就是如果更加有效的在大规模语料上训练词向量。<br>每个训练样本的计算复杂度：<br>Q = N <em> D + N </em> D <em>H +H </em> V<br>其中V是词典大小，每个词编码为1-of-V，N是当前序列中当前词的前N个词，D是词向量大小，H是神经网络层中隐层神经元个数。<br>这个Q中的主要部分是最后的H <em> V 部分，但通过一些优化方法可以降低（hs/ng), 所以此时的主要的复杂度来着 N </em> D * H, 所以作者直接将神经网络层去掉，来提高计算效率。<br>作者之前的工作发现成功的训练一个神经网络语言模型可以通过两步进行：1，首先通过一个简单模型训练词向量，2，然后在这之上训练N-gram NNLM。同时，增加当前词后续的词（下文信息）可以得到更好的结果。基于此，提出了两种结构的模型：<br><img src="/2019/10/21/deep-w2v/model.png" alt="CBOW vs  Skip-gram"></p><p>1.CBOW：与NNLM类似，但是将网络层去掉，同时使用当前词的下文，即通过将上下文窗口内的词投影得到统一向量，然后预测当前词。此时模型内词的顺序不再影响投影结果，计算复杂度为：Q = N <em> D + D </em> log(V)<br>2.Skip-gram: 与CBOW类似，不过是通过当前此来预测上下文内的词，为了提高效率，在实际工程上对上下文内的词进行了采样，此时的计算复杂度为：Q = C <em> (D + D </em> log(V))</p><h1><span id="you-hua-fang-an">优化方案</span><a href="#you-hua-fang-an" class="header-anchor"></a></h1><p>之前提到原始NNLM中的主要计算复杂度是输出层，即 H * V，主要的优化思路是避免全量计算V的概率，作者实现了两种方案，即 Hierarchical Softmax 和negative sampling</p><h2><span id="1-hierarchical-softmax">1.Hierarchical softmax</span><a href="#1-hierarchical-softmax" class="header-anchor"></a></h2><p>通过词频构建霍夫曼树，然后将输出层用霍夫曼树替换，上一层结果与每个节点做二分类，判断属于词类，叶子节点为对应的词，判断属于该词的概率</p><p><img src="/2019/10/21/deep-w2v/hs.png" alt></p><p>特点：高频词的位置更靠近根节点，所需的计算进一步降低。但对于低频词，其对应位置远离根，对应路径长，所需计算量依然很大，效率不高</p><h2><span id="2-negative-sampling">2.negative  sampling</span><a href="#2-negative-sampling" class="header-anchor"></a></h2><p>在输出层避免对全量字典进行判断，而通过先验知识来圈出最容易混淆的一部分，然后组成负样本（相对于当前词）。作者提出通过词频来归一化后的比例来组成一定比例的候选集，随机的在候选集选取一定数量的负样本(n &lt;&lt; V)来组成负样本集，最后的softmax多分类层变成多个sigmod二分类层，来提高计算效率及词向量的质量。</p><h1><span id="si-chong-xun-lian-fang-an">四种训练方案</span><a href="#si-chong-xun-lian-fang-an" class="header-anchor"></a></h1><ul><li>1.基于hs的CBOW</li></ul><p><img src="/2019/10/21/deep-w2v/hs-cbow.png" alt><br>其中</p><p><img src="/2019/10/21/deep-w2v/xw.png" alt></p><p><img src="/2019/10/21/deep-w2v/pw.png" alt></p><p><img src="/2019/10/21/deep-w2v/gd.png" alt></p><p>对应的伪代码：</p><p><img src="/2019/10/21/deep-w2v/code.png" alt></p><ul><li>2.基于hs的Skip-gram<br><img src="/2019/10/21/deep-w2v/sk.png" alt></li></ul><p>对应的伪代码：<br><img src="/2019/10/21/deep-w2v/sk-code.png" alt></p><ul><li>3.基于ng的CBOW</li></ul><p><img src="/2019/10/21/deep-w2v/ng-cbow.png" alt><br><img src="/2019/10/21/deep-w2v/G.png" alt="image.png"></p><p>对应伪代码：</p><p><img src="/2019/10/21/deep-w2v/ng-code.png" alt></p><ul><li>4.基于hs的skip-gram<br><img src="/2019/10/21/deep-w2v/opt.png" alt></li></ul><p>G中表达式与基于hs的CBOW一样，只是在最外面多了一层求和，后面的过程与CBOW一样。</p><h1><span id="si-kao">思考</span><a href="#si-kao" class="header-anchor"></a></h1><ul><li>1.词向量的训练过程是一个fake task，我们的目标不是最后的语言模型，而是在这个过程中产生的feature vector，用一个real task 来训练是不是更好？</li><li>2.因为是个fake task，那我们如何评估这个task，又如何评估得到的词向量的质量？论文中使用了近似词对及线性平移的特性，有没有更好的方式？</li><li>3.词向量的“similarity”具体是什么含义？</li></ul><h1><span id="hui-da">回答</span><a href="#hui-da" class="header-anchor"></a></h1><ul><li>1.用real task来训练一般会得到更好的词向量，但一般下游任务都是在词向量之上构建，所以一般情况是训练一个词向量，然后作为embedding层的初始参数进行下游任务的训练。</li><li>2.除了相似词及线性平移性，其他情况下可以通过下游任务的效率来评估。</li><li>3.词向量的“similarity”跟通常意义的近义词或相似词有本质上的区别，词向量更多的含义是“同位词”，即上下文相近的词。换个角度，我们将模型的连接函数形式写出来：</li></ul><p><img src="/2019/10/21/deep-w2v/pwk.png" alt></p><p>上式中v分别对应左右两个词向量空间的词向量，由于模型是对称的，所以实际使用时左右两个词向量可以任选一个。</p><p>其中<img src="/2019/10/21/deep-w2v/pwi.png" alt></p><p>分母是归一化项，暂时忽略，最终最大化P(wk|wi)的同时，即让 Vwk与 Vwi的内积更大。即模型内隐式的用词向量的内积（方向）来表示词向量直接的距离远近（语义距离），所以可以利用词向量的cosine来寻找语义更接近的词。进一步的，左右两个词向量分属不同的向量空间，最小化两个词的语义距离被转化为最小化两个词在不同语义空间的距离，而不是在同一个向量空间，为什么这种方案可行？原文里提到是因为放在同一个向量空间（同一个矩阵），两个词向量正交在一起，不好优化，，分开放在两个向量空间更利于优化。两个词向量空间为什么可行？我认为主要是因为模型是对称的，虽然两个向量空间不同，但是可以认为是只是经过了旋转缩放，词在向量空间的相对位置没有发生改变（词向量之间的角度）。</p><p>优点：</p><ul><li>1.没有神经网络层，所以没有耗时的矩阵相乘，只保留了一个softmax层，计算效率高。</li><li>2.优化时使用的是随机梯度下降，罕见词不会主导优化目标<br>demo：<a href="https://github.com/xv44586/Papers/blob/master/NLP/WordVector/word2vecDemo.ipynb" target="_blank" rel="noopener">https://github.com/xv44586/Papers/blob/master/NLP/WordVector/word2vecDemo.ipynb</a></li></ul><p><strong>论文</strong><br><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1301.3781.pdf</a><br><a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">https://arxiv.org/abs/1310.4546</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于苏州</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Dropout--深度神经网络中的Bagging</title>
    <link href="https://xv44586.github.io/2019/10/17/dropout/"/>
    <id>https://xv44586.github.io/2019/10/17/dropout/</id>
    <published>2019-10-17T14:28:12.000Z</published>
    <updated>2019-10-17T15:01:46.580Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#shen-du-shen-jing-wang-luo-guo-ni-he-wen-ti">深度神经网络过拟合问题</a></li><li><a href="#dropout">Dropout</a></li><li><a href="#bu-tong-dropout-fang-an-dui-bi-shi-yan">不同dropout方案对比实验</a></li><li><a href="#updating">updating…</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="shen-du-shen-jing-wang-luo-guo-ni-he-wen-ti">深度神经网络过拟合问题</span><a href="#shen-du-shen-jing-wang-luo-guo-ni-he-wen-ti" class="header-anchor"></a></h1><p>深度神经网络由于其巨大的参数量，可以很方便的拟合非常复杂的非线性关系，同时，巨大的参数量也给模型带来了过拟合的问题。为了解决这个问题，也有人提出了早停和加入正则项等手段。而在传统机器学习中，除了这些手段，还有一种手段来解决这个问题，即bagging（参考我之前的文章<a href="https://xv44586.github.io/2019/10/16/bagging/">Bagging为什么能降低过拟合</a>），最典型的就是随机森林：对样本和特征进行抽样，训练多个模型，然后进行集成（投票/求平均）。那如何将这种思路引入到深度神经网络中呢？<br>首先,由于训练单个深度神经网络就已经非常耗时，通常样本量也不够多，不适合采样，所以像随机森林一样抽样训练多个模型的方案不可取。<br>剩下的思路就是用“一个”模型来模拟多个sub-model，那如何来模拟呢？</p><h1><span id="dropout">Dropout</span><a href="#dropout" class="header-anchor"></a></h1><p>对于深度神经网络，其最重要的部分就是其隐藏单元（神经元），对于一个有n个神经元的层，我们可以通过设置神经元是否激活，来模拟$2^n$ 种结构，即sub-model，而在evaluate阶段，我们将这些所有的sub-mdoel的结果进行求平均。那对于现在的结构，不可能对 $2^n$中结构都去做计算然后再求平均，一种近似的做法是用当前这“一个”模型来近似模拟：设置所有神经元都处于激活状态，同时，对每个神经元的输出乘以其激活概率keep_prob.这就是Dropout的背后思想。<br><img src="/2019/10/17/dropout/dropout.png" alt="Dropout"><br>简单总结Dropout的具体做法：训练阶段，在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作；预测阶段，对每个神经元的输出乘以1-p (keep_prob)。<br>接下来思考一下，为什么这么做work:首先，训练阶段部分神经元失活，对应的结果是部分features不参与计算，本质上是对features进行采样；其次，从整个训练过程中看，每次训练（batch-data),都对应不同的失活神经元（sub-model)，对应的每个sub-model在单个epoch内，都是在对样本进行无放回的抽样，本质上是在bagging。最后，在预测阶段，为什么可以用“一个”完整的模型来模拟sub-model的求平均过程呢？从sub-model的角度看，每个sub-model被training的概率为(1-p), 而神经元对所有sub-model是共享的，唯一的区别是是否激活，所以归一到每个神经元上，单个神经元被training（激活）的概率为(1-p),而sub-model的总数是$2^n$，每个神经元求平均的过程即Out_i <em> (1-p) </em> $2^n$/ $2^n$ = Out_i * (1 -p).<br>具体实现时，我们的目的是在训练阶段对神经元进行随机（概率p)失活,而在test和evaluate时，对神经元的输出乘以(1-p),所以在实现时，可以采用一个trick：dropout时，对样本进行mask的同时，将其除以(1-p),这样就可以一次计算完成所有逻辑，同时，把所有逻辑保留在整个层中。<br><strong>tf的实现：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, keep_prob, noise_shape=None, seed=None, name=None)</span>:</span>  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">  <span class="string">"""Computes dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  With probability `keep_prob`, outputs the input element scaled up by</span></span><br><span class="line"><span class="string">  `1 / keep_prob`, otherwise outputs `0`.  The scaling is so that the expected</span></span><br><span class="line"><span class="string">  sum is unchanged.</span></span><br><span class="line"><span class="string">    ...</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line">  <span class="keyword">with</span> ops.name_scope(name, <span class="string">"dropout"</span>, [x]) <span class="keyword">as</span> name:</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># Do nothing if we know keep_prob == 1</span></span><br><span class="line">    <span class="keyword">if</span> tensor_util.constant_value(keep_prob) == <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    noise_shape = noise_shape <span class="keyword">if</span> noise_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> array_ops.shape(x)</span><br><span class="line">    <span class="comment"># uniform [keep_prob, 1.0 + keep_prob)</span></span><br><span class="line">    random_tensor = keep_prob</span><br><span class="line">    random_tensor += random_ops.random_uniform(noise_shape,</span><br><span class="line">                                               seed=seed,</span><br><span class="line">                                               dtype=x.dtype)</span><br><span class="line">    <span class="comment"># 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)</span></span><br><span class="line">    binary_tensor = math_ops.floor(random_tensor)</span><br><span class="line">    ret = math_ops.div(x, keep_prob) * binary_tensor</span><br><span class="line">    <span class="keyword">if</span> context.in_graph_mode():</span><br><span class="line">      ret.set_shape(x.get_shape())</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></p><h1><span id="bu-tong-dropout-fang-an-dui-bi-shi-yan">不同dropout方案对比实验</span><a href="#bu-tong-dropout-fang-an-dui-bi-shi-yan" class="header-anchor"></a></h1><p>如果在非dropout阶段不进行scaled会如何？<br>scaled<br><img src="/2019/10/17/dropout/scaled.png" alt="scaled.png"><br>without_scaled<br><img src="/2019/10/17/dropout/without-scaled.png" alt="without_scaled.png"><br>实验也说明非dropout阶段如果没有进行scaled（求平均），对应的loss会比train阶段高，同时acc也会降低。</p><p><strong>桥豆麻袋，到这里好像出现了一点问题：</strong><br>按照我们上面的思路，在test和evaluate阶段，我们从对sub-model求平均转化为对每个神经元的output进行scaled down，即 activation(x <em> W ) </em> (1 - p)， 而我们在实现时，只是对x进行scaled up操作，如果后面接的层的激活函数是线性的，这样处理没有什么问题，但是，后面的层不总是线性激活函数，那此时，output = activation(x <em> (1-p) </em> W)  != activation(x<em>W) </em> (1 - p),即我们得到的输出与我们想要的并不一样，按照以上的理解，我们对output进行scaled-down，验证一下两者的区别。<br>scaled_input<br><img src="/2019/10/17/dropout/scaled-input.png" alt="scaled_input.png"><br>scaled_output<br><img src="/2019/10/17/dropout/scaled-output.png" alt="scaled_output.png"></p><p>看上去对output进行scaled-down结果稍微好一点，但是并不显著，此时的激活函数是relu，而且是在倒数第二层，换个激活函数试试。<br>scaled_input_tanh<br><img src="/2019/10/17/dropout/scaled_input_tanh.png" alt="scaled_input_tanh.png"><br>scaled_output_tanh<br><img src="/2019/10/17/dropout/scaled_output_tanh.png" alt="scaled_output_tanh.png"><br>结果看上去对output进行scaled-down效果稍微差一些，但差距不大。<br>代码地址<a href="https://github.com/xv44586/Papers/tree/master/DeepLearning/Dropout" target="_blank" rel="noopener">https://github.com/xv44586/Papers/tree/master/DeepLearning/Dropout</a>，感兴趣的可以试试其他方式。<br>既然两种方式在结果上看，效果差不多，而对output进行scaled-down需要添加一个AfterDropLayer，逻辑会在不同的层中，而对inputs直接进行scaled-up，所有逻辑都保存在一个layer中，更清晰。<br><strong>But，Why？</strong>为什么两种方式的结果效果差异不大？真让人头秃啊！</p><p><strong>论文地址</strong><br><a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="noopener">http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</a></p><p>================================</p><h1><span id="updating">updating…</span><a href="#updating" class="header-anchor"></a></h1><p><img src="/2019/10/17/dropout/linear.png" alt="image.png"></p><p>上图是sigmoid函数在[-8,8]区间的图像，其中linear是由[-8,-4,-2,2,4,8]截断的直线，看图可以看出，sigmoid在区间内都非常的接近”linear”，梯度变化较大的部分只在几个拐点周围，大部分都是近似”linear”,所以也就解释了为什么两种方式有差异，但是差异并不大。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>大四竞赛作品截图</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="DeepLearning" scheme="https://xv44586.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Glove模型</title>
    <link href="https://xv44586.github.io/2019/10/17/glove/"/>
    <id>https://xv44586.github.io/2019/10/17/glove/</id>
    <published>2019-10-17T01:15:58.000Z</published>
    <updated>2019-10-17T13:58:39.300Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc"><!-- toc --><ul><li><a href="#yi-zheng-ti-si-lu">一、整体思路</a></li><li><a href="#er-ji-ben-jia-she">二、基本假设</a></li><li><a href="#san-mo-xing">三、模型</a></li><li><a href="#si-dui-bi">四、对比</a></li><li><a href="#wu-si-kao">五、思考</a></li><li><a href="#zai-si-kao">再思考：</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul><!-- tocstop --></div><h1><span id="yi-zheng-ti-si-lu">一、整体思路</span><a href="#yi-zheng-ti-si-lu" class="header-anchor"></a></h1><p>获取词向量基本上有两种思路：</p><ul><li>1.利用全局统计信息，进行矩阵分解（如LSA）来获取词向量，这样获得的词向量往往在词相似性任务上表现不好，表明这是一个次优的向量空间结构；</li><li>2.利用局部上下文窗口单独训练，但是统计信息作为有用的先验知识，没有很好的利用到。<br>Glove：结合两种训练方式，获取更好的词向量</li></ul><h1><span id="er-ji-ben-jia-she">二、基本假设</span><a href="#er-ji-ben-jia-she" class="header-anchor"></a></h1><p>词的共现次数与其语义的相关性往往不是严格成比例，所以直接用共线性来表征词之间相关性效果不好，因此，作者通过引入第三个词，通过词之间的差异来刻画相关性。差异选择用两个词与同一个词的共现概率的次数来更好的判断词之间的相关性。比率：$ratio_{i,j,k}=\frac{Pi,k}{Pj,k} $<br>看下面这个例子：<br><img src="/2019/10/17/glove/table1.png" alt="image.png"><br>ice 与solid相关性高，而steam与solid相关性弱，对应比例大于1；ice与gas相关性弱，steam与gas相关性高，对应比例小于1，ice与steam都与water相关，对应比例约等于1，ice与steam与fashion都不相关，对应比例也是约等于1.<br>相关性的规律：<br><img src="/2019/10/17/glove/ratio.png" alt="image.png"></p><h1><span id="san-mo-xing">三、模型</span><a href="#san-mo-xing" class="header-anchor"></a></h1><p>模型的数学形式为：<br><img src="/2019/10/17/glove/1.png" alt><br>其中wi, wj 与wk分属不同的两个词向量空间（参考skipgram），对于F函数，我们希望他能够在向量空间内预测pPik/Pjk这个比率，由于向量空间的线性结构，最自然的方式就是用向量的差，即：<br><img src="/2019/10/17/glove/2.png" alt><br>等式的右侧是一个标量，左侧F函数可以是一个复杂函数，而我们上面提到我们希望捕捉向量的线性结构，所以避免使用复杂函数，首先将参数做内积：<br><img src="/2019/10/17/glove/3.png" alt><br>在窗口滑动的过程中，中心词与上下文词的角色会相互转化，但是当词的位置互换后，其相关性应该是保持一致的，所以，F函数需要对“和”操作与“商”操作上同态（这里同态的意思是F函数在左右两侧应该是一致的，也就是F((wi - wj)）=F(wi) / F(wj)：<br><img src="/2019/10/17/glove/4.png" alt><br>其中：<br><img src="/2019/10/17/glove/5.png" alt></p><p>为了解决 上式4，F函数的形式就是exp(指数形式），最终求解后：<br><img src="/2019/10/17/glove/6.png" alt><br>上式中，等式左侧是对称的，即WiT<em>Wj = WjT</em>Wi, 而右侧是不对称的，即log(pij) != log(Pji). 如果上式的右侧没有log(xi)则等式的左右就对称了，考虑到与k无关，所以把这一项并入到i的偏差项中，即：<br><img src="/2019/10/17/glove/7.png" alt><br>由于上式中有log，所以需要处理0值，同时，对于低频与高频的共线词都不能过度训练，于是，优化目标就变成了：<br><img src="/2019/10/17/glove/8.png" alt><br>其中，权重函数f(x)需要满足：</p><ul><li>1， f(0)=0</li><li>2, 非减以避免低频共现过度训练</li><li>3，抑制高频共现避免过度训练<br>最后采用的f(x) 形式为：<br><img src="/2019/10/17/glove/9.png" alt><br>实验中他们采用的是xmax=100, a=3/4<br><img src="/2019/10/17/glove/scratch.png" alt="完整过程"></li></ul><h1><span id="si-dui-bi">四、对比</span><a href="#si-dui-bi" class="header-anchor"></a></h1><p>与局部窗口方式对比：<br><img src="/2019/10/17/glove/comp.png" alt="与local window对比"><br>优化目标使用不同的损失函数，并带有调和函数来降低高频词的影响。<br>语义相似性结果对比：<br><img src="/2019/10/17/glove/result.png" alt="不同模型对比"></p><h1><span id="wu-si-kao">五、思考</span><a href="#wu-si-kao" class="header-anchor"></a></h1><ul><li>1.相对与word2vec, Glove引入了词频统计信息，这是很重要的全局信息。</li><li>2.word2vec的训练次数与词频相关，Glove的训练中词频是loss的weight，高频低频词的overweight的情况更低。</li><li>3.将基于局部窗口的模型中，相同词进行合并，修改对应object：<img src="/2019/10/17/glove/13.png" alt><br>其中H（）为交叉熵，相对Glove的object：<br><img src="/2019/10/17/glove/16.png" alt><br>loss由交叉熵改为最小二乘，Xi改为f(Xi)函数进行调和。</li><li>4.Glove中的左右词向量也是两个不同的词向量空间，与word2vec一样，虽然Glove模型上看上去可以使用同一个词向量空间做，但是作者说是因为更好优化且模型更稳定，不同的时，最后的结果是左右词向量求和（虽然word2vec也可以这么做）</li></ul><p>Demo:<a href="https://github.com/xv44586/Papers/blob/master/NLP/WordVector/GloveDemo.ipynb" target="_blank" rel="noopener">https://github.com/xv44586/Papers/blob/master/NLP/WordVector/GloveDemo.ipynb</a></p><hr><h1><span id="zai-si-kao">再思考：</span><a href="#zai-si-kao" class="header-anchor"></a></h1><ul><li>1.通常我们都是根据模型来推导其对应的性质，而Glove是因为其应该具有的性质，来反推模型，这种方式也给人提供了一种新思路。</li><li>2.为什么两种模型都有两套词向量空间（中心词向量和上下文词向量）？虽然两个作者都说是因为更好优化且模型更稳定，那有没有更合理的理论上的解释呢？我的理解是：对于word2vec，模型直接对概率p(w|context),如skipgram中，直接对P(w2|w1)进行建模，而P(w2|w1)与P(w1|w2)并不一定相等，所以需要针对词的位置区分，也就是需要两套不一样的词向量空间；而Glove中，如上文中公式（6）所示，模型右侧有一个与位置有关的参数，虽然通过引入两个bias可以一定程度上消除这个位置相关的参数，但是这个参数并不是均匀分布，所以仅通过bias不能完全解决这个问题，而引入两个不同的词向量空间，相当于是引入了位置信息，这样能更好的解决这个问题。其最本质的原因是在窗口滑动过程中，词位置变化的同时信息可能是不对称的，即以a为中心词的窗口中的b在以b为窗口时，a可能丢失。</li><li>3.对于上式8，存在一个比较严重的问题，模型为了消去位置相关参数，将其吸收进bias内，而这个bias的引入，就导致了一个严重的问题，即模型不适定。<br><img src="/2019/10/17/glove/ret.png" alt><br>即当你求得一组解后，你可以给这组解加上一个常数向量，其还是一组解。那这个问题就很严重了，你无法评估你得到的解是哪组解。如果加上的是非常大的常数向量，那这组词向量在很多度量上就失去了意义（如余弦距离）<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>摄于北京某水库</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="Reading" scheme="https://xv44586.github.io/categories/Reading/"/>
    
    
      <category term="NLP" scheme="https://xv44586.github.io/tags/NLP/"/>
    
  </entry>
  
</feed>
