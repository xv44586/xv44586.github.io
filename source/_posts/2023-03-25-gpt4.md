---
author:
  nick: 小蛋子
  link: 'https://github.com/xv44586'
title: GPT-4 yes!! but
date: 2023-03-25 21:50:00
categories: NLP
tags:
  - GPT-4
  - LLM
cover: /2023/03/25/gpt4/bg.jpeg
---
<!-- toc -->

这篇博客简单讨论下在GPT-4 如此强大的技术冲击下，我们NLPer该何去何从。
首先说下我的结论：GPT-4 非常强大，但是还没有到完全取代我们工作的地步，我们依然有很多能做的方向。

# GPT-4 yes
1. 更可靠了（胡说八道进一步降低）
2. 性能更好：比GPT-3.5 又提升了一大截
3. reverse inverse scaling prize:一些随着模型变大性能下降的任务在GPT-4上不再出现类似现象（曾经没法通过增大模型规模提升性能的任务现在也解决了）
4. 能够用图像做prompt：增加图像信息能进一步提升性能（看图说话，类似BLIP2，这个对盲人太友好了）
5. 进一步closeAI

# GPT-4存在的问题
## 不开源
由于GPT-4 完全不公布任何技术细节，所以他为什么有如此强大的能力，我们只能猜，想要研究它变得困难重重。
## 数据安全
ChatGPT 的火爆让大家突然忘了曾经非常看重的数据安全问题，preview 版是有可能会参与下次迭代的；而商用api 即使强调不会用于模型训练，敏感业务数据你敢用吗？
## 资源消耗大
即使是GPT-3 也有175B 参数，训练/推理都是极其消耗资源的，从GPT-4 的价格上涨了50% 来看，GPT-4 的推理消耗资源也上升了50% 左右。

# NLP可做的方向
这也是最近讨论比较热烈的一个问题，回答这个问题前，不妨先思考一下理想的NLP 模型应该具有哪些特征。我认为比较理想的模型是：安全可靠/支持长文本/小/快/私有化部署。所以我仅从个人出发，给出一些我比较关注的方向。

## hallucination
目前LLM 最大的问题就是hallucination(一本正经的胡说八道)。目前主流两种思路：alignment/多模态。
- alignment
至于如何做alignment ，学术界主要是instruction-tuning为主，OpenAI 的路线是RLHF，然而普通玩家我是完全不推荐做RL的，只要仔细阅读InstructGPT/GPT-4 paper中关于reward model 部分就能劝退了。所以对于我们普通玩家，是否有别的路径？
- 多模态
GPT4 的paper 上看效果是不错的，我没做过，不多说了。

## 复现GPT-4/ChatGPT/GPT-3.5/InstructGPT
不开源只能复现，目前主要有[facebookresearch/llama](https://github.com/facebookresearch/llama)/[bigscience/bloom](https://huggingface.co/bigscience/bloom),此外还有不开源但是可以使用API 访问的百度文心一言/ChatGLM 等。

## 如何评估LLM
我们说百度文心一言性能不行时，到底如何不行？这里就牵扯到如何量化的评估LLM 的性能。曾经自动化的方案及benchmark 的参考意义随着LLM 的能力提升显得越来越弱，现在急需新的数据集/评估方案。目前的工作有[openai/evals](https://github.com/openai/evals)/[stanford-crfm/HELM](https://github.com/stanford-crfm/helm)

## 支持长文本
更长的输入对某些任务是有利的，如何让模型支持更长的输入呢？主要的思路有两个：
- 训练时使用较短文本，推理时外推更长的位置信息，使模型获得处理长文本的能力，如bloom 中使用的[ALiBI](https://arxiv.org/pdf/2108.12409.pdf)
- 调整模型结构，如最近的工作：[CoLT5:Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf)
PS: GPT-4 的输入从GPT-3.5 的4K(or 8K?) 提升到了30K，如何做的呢？

## 变小变快
相同架构的模型通常变小就会变快，让模型变小的方法主要是蒸馏/量化/train 小模型，这个方向目前工作有[stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)/[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)，中文上也有[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)/[BELLE](https://github.com/LianjiaTech/BELLE)等。

## 低成本inference
如何在低成本设备上使用这些模型？如单张GPU 上跑大模型或普通CPU 上跑模型。这个方向的工作也有[FlexGen](https://github.com/FMInference/FlexGen)/[llama.cpp](https://github.com/ggerganov/llama.cpp) 等。

## 低成本优化
低成本fine-tuning 主要有两个方向：parameter-efficient / sample-efficient.
- parameter-efficient  的思路目前主要有prompt-tuning/prefix-tuning/LoRA/Adapter 等，参考[huggingcae/peft](https://github.com/huggingface/peft)
- sample-efficient 可以帮助我们如何更有效的构造训练集，最近的工作有[Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs](http://arxiv.org/abs/2303.08114)

## 优化器
优化器决定了我们训练时需要的资源。虽然我们通常使用Adam 优化器，但是其需要2倍额外显存，而google 好像用Adafactor 更多一点，最近他们又出了一个新工作[Lion](https://arxiv.org/abs/2302.06675).

## 更可控
如从可控生成角度看，目前可控主要通过control token（prompt）来实现，有没有更好的办法来实现更“精细”的控制，就如controlnet 之于stable diffusion。

## 识别AIGC
如何判别内容是人写的还是模型生成的呢？随着模型的性能越来越强，识别AIGC 也越来越困难。目前的工作也有watermark/[GPTZero](https://gptzero.me/) 等。不过我感觉还没什么特别有效的方案目前。
对此我有个简单的思路：将AI 生成的与非AI 生成的看作是两种不同的语言，如code 与英语一样，虽然都是相同符合构成，但是对应不同语言。使用大量的AI 生成的内容（或人机交互数据）pretrain 一个”AI 语言模型“，再来进行识别。

## 单一任务/领域刷榜
我认为在某个任务/领域上通过小模型挑战大模型依然有意义，LLM 虽然强大，但是依然有太多我们不知道的能力，通过小模型刷榜也许能提供一些思路，就像PET 本意是调战GPT-3，却打开了LLM 的新思路。

# 何去何从
## 普通工程师
这种新的革命性的技术我们普通工程师通常都不是第一线的，我第一次真正使用bert 也是在其出来两年后了。即使今天，也有很多场景/公司不使用bert这个技术。
换个角度，即使我们想参与，我想能参与训练/fine-tuning 一个10B 规模模型的工程师都相当少，更别提更大的了。所以到底是“左右逢源”还是“举步维艰”，让子弹飞一会儿吧。

## 普通用户
普通用户我觉得应该就是多读书，提高自己的鉴别能力了。”生活中不缺少美，而是缺少发现美的眼睛。”

# 番外
## 通过prompt 构建技术壁垒/申请prompt 专利
随着alignment 的进一步优化，LLM 通常越来越理解自然语言，所以我认为prompt-trick 越来越不重要，而清晰准备的用prompt 描述你的需求越来越重要。所谓技术壁垒也许就是如何更清晰有效的描述需求了，但也很难形成技术壁垒。
至于专利，软件著作权保护的是制作软件这个技术本身，而非你使用软件时的姿势，所以我想单独的prompt 应该也不会形成专利，但是作为你某个技术的一部分，还是有可能的。

## 会不会失业
我认为不会失业，但是会转变一部分人的工作方式。在计算这件事上，人类早已被计算机远远的甩在后面，而计算机的出现也带来了大量的新工作。尤其是LLM 现阶段的表现是“懂开车的人才能开车”，所以需要更多更懂某个业务，更熟练使用LLM 工具的人。

# 关于头图
放张动漫图，据说能缓解焦虑

