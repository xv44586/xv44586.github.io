<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Xgboost原理 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2019/10/14/xgb/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2019/10/14/xgb/cover.jpeg" alt="Xgboost原理"></div><header class="post__info"><h1 class="post__title">Xgboost原理</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2019-10-14</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/Xgboost/">Xgboost</a></li><li class="mark__item"><a href="/tags/Boosting/">Boosting</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#xu-lun">绪论</a></li><li><a href="#suan-fa-yuan-li">算法原理</a><ul><li><a href="#xue-xi-mu-biao">学习目标</a><ul><li><a href="#jie-dian-hua-fen">节点划分</a></li><li><a href="#suo-jian-yu-lie-cai-yang">缩减与列采样</a></li><li><a href="#xun-zhao-zui-jia-fen-ge-dian-suan-fa">寻找最佳分割点算法</a></li><li><a href="#dai-quan-chong-de-fen-wei-shu-lue-tu-weighted-quantile-sketch-suan-fa">带权重的分位数略图（weighted quantile sketch）算法</a></li><li><a href="#xi-shu-zi-gua-ying-fen-ge-ce-lue">稀疏自适应分割策略</a></li><li><a href="#xgboost-de-you-que-dian">XGBoost的优缺点</a></li></ul></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><h1><span id="xu-lun">绪论</span><a href="#xu-lun" class="header-anchor"></a></h1><p>在实际应用的机器学习方法里，<code>GradientTree Boosting （GBDT）</code>是一个在很多应用里都很出彩的技术。XGBoost是一套提升树可扩展的机器学习系统。2015年Kaggle发布的29个获胜方法里有17个用了XGBoost。在这些方案里，有8个仅用了XGBoost，另外的大多数用它结合了神经网络。对比来看，第二流行的方法，深度神经网络，只被用了11次。这个系统的成功性也被KDDCup2015所见证了，前十的队伍都用了XGBoost。此外，据胜出的队伍说，很少有别的集成学习方法效果能超过调好参的XGBoost。<br>主要创新点：</p><ul><li>设计和构建高度可扩展的端到端提升树系统。</li><li>提出了一个理论上合理的加权分位数略图（weighted quantile sketch ）来计算候选集。</li><li>引入了一种新颖的稀疏感知算法用于并行树学习。 令缺失值有默认方向。</li><li>提出了一个有效的用于核外树形学习的缓存感知块结构。 用缓存加速寻找排序后被打乱的索引的列数据的过程。</li></ul><h1><span id="suan-fa-yuan-li">算法原理</span><a href="#suan-fa-yuan-li" class="header-anchor"></a></h1><h2><span id="xue-xi-mu-biao">学习目标</span><a href="#xue-xi-mu-biao" class="header-anchor"></a></h2><p>首先来看下我们是如何预测的：<br>XGBoost是一个树集成模型，他将K（树的个数）个树的结果进行求和，作为最终的预测值。即：<br><img src="/2019/10/14/xgb/target.png" alt><br>假设给定的样本集有n个样本，m个特征，则<br><img src="/2019/10/14/xgb/feature.png" alt><br>其中 xi 表示第i个样本，yi 表示第i个类别标签，回归树（CART树）的空间F为<br><img src="/2019/10/14/xgb/F.png" alt><br>其中q代表每棵树的结构，他将样本映射到对应的叶节点；T是对应树的叶节点个数；f(x)对应树的结构q和叶节点权重w。所以XGBoost的预测值是每棵树对应的叶节点的值的和。<br>我们的目标是学习这k个树，所以我们最小化下面这个带正则项的目标函数：<br><img src="/2019/10/14/xgb/target_n.png" alt><br><img src="/2019/10/14/xgb/text.png" alt><br>上式的第一项是损失误差，如MSE和logistic等，第二项是正则项，控制树的复杂度，防止过拟合。<br>式子2中目标函数的优化参数是模型（functions），不能使用传统的优化方法在欧氏空间优化，但是模型在训练时，是一种加法的方式（additive manner），所以在第t轮，我们将f（t）加入模型，最小化下面的目标函数：<br><img src="/2019/10/14/xgb/min.png" alt><br>训练时，新的一轮加入一个新的f函数，来最大化的降低目标函数，在第t轮，我们的目标函数为 ：<br><img src="/2019/10/14/xgb/lt.png" alt><br>接下来我们将目标函数进行泰勒展开，取前三项，移除高阶小无穷小项，最后我们的目标函数转化为：<br><img src="/2019/10/14/xgb/lt_merge.png" alt><br>其中：<br><img src="/2019/10/14/xgb/gi.png" alt><br>接下来我们求解这个目标函数<br><img src="/2019/10/14/xgb/scratch.png" alt></p><p>最终我们将关于树模型的迭代转化为关于树的叶子节点的迭代，并求出最优的叶节点分数。将叶节点的最优值带入目标函数，最终目标函数的形式为：<br><img src="/2019/10/14/xgb/ltq.png" alt><br>上式可以作为得分函数用来测量树结构q的质量，他类似与决策树的不纯度得分，只是他通过更广泛的目标函数得到<br><img src="/2019/10/14/xgb/arc.png" alt><br>通过上式我们可以看到，当树结构确定时，树的结构得分只与其一阶倒数和二阶倒数有关，得分越小，说明结构越好。</p><h3><span id="jie-dian-hua-fen">节点划分</span><a href="#jie-dian-hua-fen" class="header-anchor"></a></h3><p>而通常情况下，我们无法枚举所有可能的树结构然后选取最优的，所以我们选择用一种贪婪算法来代替：我们从单个叶节点开始，迭代分裂来给树添加节点。节点切分后的损失函数：<br><img src="/2019/10/14/xgb/split.png" alt><br>上式用来评估切分后的损失函数，我们的目标是寻找一个特征及对应的值，使得切分后的loss reduction最大。γ除了控制树的复杂度，另一个作用是作为阈值，只有当分裂后的增益大于γ时，才选择分裂，起到了预剪枝的作用。</p><h3><span id="suo-jian-yu-lie-cai-yang">缩减与列采样</span><a href="#suo-jian-yu-lie-cai-yang" class="header-anchor"></a></h3><p>除了在目标函数中引入正则项，为了防止过拟合，XGBoost还引入了缩减(shrinkage)和列抽样（column subsampling），通过在每一步的boosting中引入缩减系数，降低每个树和叶子对结果的影响；列采样是借鉴随机森林中的思想，根据反馈，列采样有时甚至比行抽样效果更好，同时，通过列采样能加速计算。</p><h3><span id="xun-zhao-zui-jia-fen-ge-dian-suan-fa">寻找最佳分割点算法</span><a href="#xun-zhao-zui-jia-fen-ge-dian-suan-fa" class="header-anchor"></a></h3><p>树模型学习的一个关键问题是如何寻找最优分割点。第一种方法称为基本精确贪心算法（exact greedy algorithm）：枚举所有特征的所有可能划分，寻找最优分割点。该算法要求为连续特征枚举所有可能的切分，这个对计算机要求很高，为了有效做到这一点，XGBoost首先对特征进行排序，然后顺序访问数据，累计loss reduction中的梯度统计量（式6）。<br><img src="/2019/10/14/xgb/max.png" alt><br>上述方法是一个非常精确的分割点算法，但是当数据无法完全加载进内存或分布式的情况下，该算法就不是特别有效了。为了支持这两种场景，提出了一种近似算法：根据特征分布的百分位数，提出n个候选切分点，然后将样本映射到对应的两个相邻的切分点组成的桶中，聚会统计值，通过聚会后的统计值及推荐分割点，计算最佳分割点。该算法有两种形式：全局近似和局部近似，其差别是全局近似是在生成一棵树之前，对各个特征计算其分位点并划分样本；局部近似是在每个节点进行分裂时采用近似算法。近似算法的流程：<br><img src="/2019/10/14/xgb/appr.png" alt></p><h3><span id="dai-quan-chong-de-fen-wei-shu-lue-tu-weighted-quantile-sketch-suan-fa">带权重的分位数略图（weighted quantile sketch）算法</span><a href="#dai-quan-chong-de-fen-wei-shu-lue-tu-weighted-quantile-sketch-suan-fa" class="header-anchor"></a></h3><p>在近似算法中重要的一步是寻找候选分割点，通常情况下，特征的百分位数使数据均匀的分布在数据上。现在我们定义一个数据集Dk = {(x1k, h1), (x2k, h2) … }代表样本的第k个特征及其对应的二阶梯度，现在我们定义一个函数rk：<br><img src="/2019/10/14/xgb/rk.png" alt><br>上式代表特征k小于特征z的样本比例，我们的目标是寻找候选分割点{sk1, sk2,…}，使它满足：<br><img src="/2019/10/14/xgb/rksk.png" alt><br>其中e是候选因子，即切分的百分位数，所以最后有大约1/e个候选分割点。那为什么可以用二阶倒数h来代替权重呢？我们将目标函数变形为<br><img src="/2019/10/14/xgb/new_target.png" alt><br>上式可以看成是label是gi/hi，权重是hi的平方损失，这对于大数据下寻找划分点非常重要。在以往的分位法中，没有考虑权值，许多存在的近似方法中，或者通过排序或者通过启发式方法（没有理论保证）划分。文章的贡献是提供了理论保证的分布式加权分位法。<br>为什么要对目标函数进行类似的变形？思考一下我们划分分割点的目标是什么？是希望均匀的划分loss，而不同样本对loss的权重不同，不考虑权重直接按样本划分时，loss的分布是不均匀的，对应的分位点就会有偏差。<br>PS：文章中的近似变形感觉不太近似，更近似的变形可能是<img src="/2019/10/14/xgb/trans.png" alt><br>即label是-gi/hi的带权重平方损失。不知道文章内为啥是另一种形式</p><h3><span id="xi-shu-zi-gua-ying-fen-ge-ce-lue">稀疏自适应分割策略</span><a href="#xi-shu-zi-gua-ying-fen-ge-ce-lue" class="header-anchor"></a></h3><p>实际情况下避免不了数据稀疏，产生数据稀疏的原因主要有三个：1，数据缺失，2，统计上为0，3，one-hot编码。而适应稀疏数据非常重要。XGBoost提出的是在计算分割后的分数时，遇到缺失值，分别将缺失值带入左右两个分割节点，然后取最大值的方向为其默认方向。<br><img src="/2019/10/14/xgb/gain.png" alt><br>以上就是XGBoost所涉及的算法原理。</p><h3><span id="xgboost-de-you-que-dian">XGBoost的优缺点</span><a href="#xgboost-de-you-que-dian" class="header-anchor"></a></h3><p><strong>与GBDT对比</strong></p><ul><li>1.GBDT的基分类器只支持CART树，而XGBoost支持线性分类器，此时相当于带有L1和L2正则项的逻辑回归（分类问题）和线性回归（回归问题）。</li><li>2.GBDT在优化时只使用了一阶倒数，而XGBoost对目标函数进行二阶泰勒展开，此外，XGBoost支持自定义损失函数，只要损失函数二阶可导</li><li>3.XGBoost借鉴随机森林算法，支持列抽样和行抽样，这样即能降低过拟合风险，又能降低计算。</li><li>4.XGBoost在目标函数中引入了正则项，正则项包括叶节点的个数及叶节点的输出值的L2范数。通过约束树结构，降低模型方差，防止过拟合。</li><li>5.XGBoost对缺失值不敏感，能自动学习其分裂方向</li><li>6.XGBoost在每一步中引入缩减因子，降低单颗树对结果的影响，让后续模型有更大的优化空间，进一步防止过拟合。</li><li>7.XGBoost在训练之前，对数据预先进行排序并保存为block，后续迭代中重复使用，减少计算，同时在计算分割点时，可以并行计算</li><li>8.可并行的近似直方图算法，树结点在进行分裂时，需要计算每个节点的增益，若数据量较大，对所有节点的特征进行排序，遍历的得到最优分割点，这种贪心法异常耗时，这时引进近似直方图算法，用于生成高效的分割点，即用分裂后的某种值减去分裂前的某种值，获得增益，为了限制树的增长，引入阈值，当增益大于阈值时，进行分裂；</li></ul><p><strong>与LightGBM对比</strong></p><ul><li>1.XGBoost采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，贪心法耗时，LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低，但是不能找到最精确的数据分割点。同时，不精确的分割点可以认为是降低过拟合的一种手段。</li><li>2.LightGBM借鉴Adaboost的思想，对样本基于梯度采样，然后计算增益，降低了计算</li><li>3.LightGBM对列进行合并，降低了计算</li><li>4.XGBoost采样level-wise策略进行决策树的生成，同时分裂同一层的节点，采用多线程优化，不容易过拟合，但有些节点分裂增益非常小，没必要进行分割，这就带来了一些不必要的计算；LightGBM采样leaf-wise策略进行树的生成，每次都选择在当前叶子节点中增益最大的节点进行分裂，如此迭代，但是这样容易产生深度很深的树，产生过拟合，所以增加了最大深度的限制，来保证高效的同时防止过拟合。</li></ul><hr><p>参考：<a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.02754.pdf</a><br><a href="https://www.zhihu.com/question/41354392/answer/98658997" target="_blank" rel="noopener">https://www.zhihu.com/question/41354392/answer/98658997</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>风格迁移模型效果图</p><div class="post__prevs"><div class="post__prev"><a href="/2019/10/13/nnlm/" title="神经网络语言模型(NNLM)"><i class="iconfont icon-prev"></i>神经网络语言模型(NNLM)</a></div><div class="post__prev post__prev--right"><a href="/2019/10/16/bagging/" title="Bagging为什么能降低过拟合">Bagging为什么能降低过拟合<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">22</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2021/02/19/happy-new-year/" title="辞旧迎新"><div class="item__cover"><img src="/2021/02/19/happy-new-year/white-whale.jpeg" alt="辞旧迎新"></div><div class="item__info"><h3 class="item__title">辞旧迎新</h3><span class="item__text">2021-02-19</span></div></a></li><li class="latest-post-item"><a href="/2021/01/20/ccf-qa-2/" title="ccf问答匹配比赛（下）：如何只用“bert”夺冠"><div class="item__cover"><img src="/2021/01/20/ccf-qa-2/head.png" alt="ccf问答匹配比赛（下）：如何只用“bert”夺冠"></div><div class="item__info"><h3 class="item__title">ccf问答匹配比赛（下）：如何只用“bert”夺冠</h3><span class="item__text">2021-01-20</span></div></a></li><li class="latest-post-item"><a href="/2021/01/12/matrix/" title="重新认识矩阵"><div class="item__cover"><img src="/2021/01/12/matrix/det.png" alt="重新认识矩阵"></div><div class="item__info"><h3 class="item__title">重新认识矩阵</h3><span class="item__text">2021-01-12</span></div></a></li><li class="latest-post-item"><a href="/2020/11/24/fine-tune/" title="如何提升bert在下游任务中的性能"><div class="item__cover"><img src="/2020/08/31/bert-01/bert.jpg" alt="如何提升bert在下游任务中的性能"></div><div class="item__info"><h3 class="item__title">如何提升bert在下游任务中的性能</h3><span class="item__text">2020-11-24</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA-match/">QA match</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>