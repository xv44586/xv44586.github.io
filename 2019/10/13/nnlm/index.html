<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>神经网络语言模型(NNLM) | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2019/10/13/nnlm/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2019/10/13/nnlm/cover.jpeg" alt="神经网络语言模型(NNLM)"></div><header class="post__info"><h1 class="post__title">神经网络语言模型(NNLM)</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2019-10-13</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/NLP/">NLP</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#tong-ji-yu-yan-mo-xing">统计语言模型</a></li><li><a href="#nnlm">NNLM</a><ul><li><a href="#why-it-works">why it works?</a></li><li><a href="#bing-xing">并行</a></li><li><a href="#out-of-vocabulary-word">out-of-vocabulary word</a></li><li><a href="#hou-xu-gong-zuo">后续工作</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><h1><span id="tong-ji-yu-yan-mo-xing">统计语言模型</span><a href="#tong-ji-yu-yan-mo-xing" class="header-anchor"></a></h1><p>首先看一个例子：<br><strong>ztc/ 上下/ 齐/ 拼搏/ ，誓为/ 春战/ 做/ 贡献</strong><br>这句话呢通顺，意思明白，那如果换一下词的位置：<br><strong>上下/ 齐/ 拼搏/ ztc/ ，春站/ 做/ 贡献/ 誓为</strong><br>意思含糊了，但是大概意思还是能猜到，那如果在变换一下：<br><strong>拼搏/ 齐/ ztc/ 上下/ ，贡献/ 誓为/ 做/ 春战</strong><br>现在这句话已经不知所云了，如何判断这个由词序组成的序列是否符合文法、含义是否正确？</p><p><strong>统计语言模型：一个句子是否合理，就看他的可能性的大小，即他的概率大小。</strong></p><p>假设一个句子S，由一连串特定顺序的词W1, W2,…WT 组成，T是句子中词的个数，则S出现的概率P(S) = P(w1, w2,…wT)<br>利用条件概率公式展开：<br>$P(w1,w2,..wT) = P(w1)<em>P(w2|w1)*P(w3|w1,w2)</em>…*P(wT|w1,w2,..wT-1)$<br>即：<br><img src="/2019/10/13/nnlm/p.png" alt><br>当语料中词典大小为100,000，句子平均长度为5时，需要学习的参数大概100000 * 5 -1 个，为了降低计算复杂度，并考虑到词序列中离的更近的词通常在语义上也更相关，所以在计算时可以通过只使用前面n-1个词来近似计算，即n-grams：<br><img src="/2019/10/13/nnlm/ngram.png" alt></p><p>n-grams存在的问题：1.泛化时常常有训练语料中没有出现过的词序列；2.没有考虑词之间的相似性。</p><h1><span id="nnlm">NNLM</span><a href="#nnlm" class="header-anchor"></a></h1><p><img src="/2019/10/13/nnlm/arc.png" alt="Neural architecture"></p><ul><li>1.对词库里的每个词指定一个分布的词向量</li><li>2.定义联合概率（通过序列中词对应的词向量</li><li>3.学习词向量和概率函数的参数</li></ul><h2><span id="why-it-works">why it works?</span><a href="#why-it-works" class="header-anchor"></a></h2><p>如果我们已知 “走” 和 “跑” 是相似词，那很容易通过 ”猫在屋里跑“ 推出 “猫在屋里走“，因为相似的词会有相似的词向量，而且概率函数是特征的平滑函数，所以特征的微小变化，只会对概率值产生一个很小的影响。即：1.相似词在特征空间距离更接近；2.概率函数是一个相对平滑的函数，对特征值的变化不是非常敏感。<br>所以训练语料中句子的出现不光增加了自身的概率，也增加了他与周围句子的概率（句子向量空间）<br>目标：f(wt ,··· ,wt−n+1) = Pˆ(wt |w1,w2,..wt-1 )<br>约束：</p><ul><li><ol><li>$∑ |V| i=1 f(i,wt−1,··· ,wt−n+1) = 1$</li></ol></li><li>2.$f&gt;0$</li></ul><p>通过得到的条件概率进行相乘，得到词序列的联合概率.<br>模型被分成二部分：<br>1.<strong>特征映射：通过映射矩阵 C∈R ∣V∣×m</strong><br>将输入的每个词映射为一个特征向量，C(i)∈Rm 表示词典中第 i 个词对应的特征向量，其中 m 表示特征向量的维度。<br>2.<strong>概率函数g</strong><br>通过context中词的词向量来映射下一个词的条件概率。g的输出是一个向量，其中第i个元素表示了字典中第i个词的概率。完整的模型表达如下：<br>$f(i,wt−1,··· ,wt−n+1) = g(i,C(wt−1),··· ,C(wt−n+1))$<br>函数f由两个映射（g and c)组成，其中c由所有的上下文共享。<br>训练过程中的参数就由两个映射组成，设 g 对应参数为w，c映射的参数就是自身，则 θ=（c, w)<br>训练过程就是学习θ的最大似然：<br><img src="/2019/10/13/nnlm/l.ong" alt><br>其中R(θ) 是正则项。<br>模型中参数与字典大小V成线性关系，且与n（n-grams)成线性关系，不过可以通过共享结构降低参数数量，如延时神经网络或循环神经网络。<br>实验中，神经网络层只有一个隐层，有一个可选的词向量到输出的直连层，实际上就有两个隐层，一个共享的词向量C 层，该层没有激活函数，还有一个tanh激活函数的隐层；最后的输出层是一个softmax层，来保证所有结果的和为1：<br><img src="/2019/10/13/nnlm/pnew.png" alt></p><p>注意：第一层是没有非线性激活函数的，因为非线性激活函数会带来其他信息（联想神经网络中非线性激活函数），而正是这种直接的线性变换，才能让第一层的参数来作为词向量<br>用yi表示每个输出词的对数概率，则<br>$y = b+Wx+U tanh(d +Hx)$<br>其中x是词向量的拼接，x = (c(wt-1),c(wt-2),c(wt-n+1))</p><h2><span id="bing-xing">并行</span><a href="#bing-xing" class="header-anchor"></a></h2><p>参数与输入的窗口大小和字典的大小成线性，但是计算量却比n-grams 要大很多，首先n-grams中不需要每次都计算所有词的概率，只需要相关词频的线性组合，另外神经网络中主要瓶颈是输出层的激活计算。</p><h2><span id="out-of-vocabulary-word">out-of-vocabulary word</span><a href="#out-of-vocabulary-word" class="header-anchor"></a></h2><p>首先根据窗口上下文可能出现的词，进行加权求和初始化新词的词向量，然后将新词 j 加入字典，然后利用这部分数据集重新训练，进行retune.</p><h2><span id="hou-xu-gong-zuo">后续工作</span><a href="#hou-xu-gong-zuo" class="header-anchor"></a></h2><ul><li>1，分解网络到子网络，如使用词聚类，构建许多小的子网络可能更快更简单</li><li>2，用树结构来表达条件概率：神经网络作用在每一个节点上，每个节点代表根据上下问得到该词类的可能性，叶子节点代表词的可能性，这种结构可以将计算复杂度从|v| 降低到 log|v|</li><li>3，梯度传播时可以只在部分输出词上进行，如某些条件下最相似的（如三元模型）。如果用在语音识别，可以只计算听觉上相似的词。</li><li>4，引入先验知识，如语义信息和语法信息。通过在神经网络结构中共享更多的结构与参数，可以捕获长期的上下文信息，</li><li>5，如何解释神经网络得到的词向量</li><li>6，上述模型对每个单词分配一个在语义空间的点，所以无法解决一词多义问题。如何扩展当前模型，在语义空间中为词分配多个点来代表词的不同语义。</li></ul><p>作者提出的后续工作中，目前是很多人的研究方向，一些已经被证明有效。</p><ul><li>第一个，优化网络结构，提到了从数据方向，构建更多的子网络，还可以直接对网络结构本身进行优化，如word2vec，将神经网络层去掉；</li><li>第二个，由于计算瓶颈在计算output的概率（对每个词计算概率，需要softmax归一化）,所以提出可以通过树结构，来避免直接对所有词进行计算，如 Hierarchical Softmax</li><li>第三个也是在计算输出时，只通过一部分词来进行梯度传播，如负采样</li><li>第四个是通过共享结构，来捕获更多上下文信息，如GPT，Bert</li><li>第五个是如何解释，也是目前很多人的研究方向</li><li>第六个是一次多义的解决方法，如ELMO</li></ul><p>参考：<br><a href="http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf" target="_blank" rel="noopener">http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于望京soho</p><div class="post__prevs"><div class="post__prev"><a href="/2019/10/13/skipgram/" title="Word2Vec之skip-gram"><i class="iconfont icon-prev"></i>Word2Vec之skip-gram</a></div><div class="post__prev post__prev--right"></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Reading/">Reading</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Programing/">Programing</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">1</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2019/10/13/nnlm/" title="神经网络语言模型(NNLM)"><div class="item__cover"><img src="/2019/10/13/nnlm/cover.jpeg" alt="神经网络语言模型(NNLM)"></div><div class="item__info"><h3 class="item__title">神经网络语言模型(NNLM)</h3><span class="item__text">2019-10-13</span></div></a></li><li class="latest-post-item"><a href="/2019/10/13/skipgram/" title="Word2Vec之skip-gram"><div class="item__cover"><img src="/2019/10/13/skipgram/cover.jpeg" alt="Word2Vec之skip-gram"></div><div class="item__info"><h3 class="item__title">Word2Vec之skip-gram</h3><span class="item__text">2019-10-13</span></div></a></li><li class="latest-post-item"><a href="/2019/10/13/gil/" title="Python中的GIL"><div class="item__cover"><img src="/2019/10/13/gil/cover.jpeg" alt="Python中的GIL"></div><div class="item__info"><h3 class="item__title">Python中的GIL</h3><span class="item__text">2019-10-13</span></div></a></li><li class="latest-post-item"><a href="/2019/10/13/multiThreads/" title="python中的多线程"><div class="item__cover"><img src="/2019/10/13/multiThreads/cover.jpeg" alt="python中的多线程"></div><div class="item__info"><h3 class="item__title">python中的多线程</h3><span class="item__text">2019-10-13</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/MachineLearning/">MachineLearning</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLP/">NLP</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="https://www.jianshu.com/u/83d2d8f89199" title="jian shu" target="_blank">简书</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>