<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>逻辑回归与最大熵模型 | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2019/10/13/logistic/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="ptcwuAdCk1Y2V7IiDN-hTKI4jk1ZXkzkk6nUUrxjhYM"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2019/10/13/logistic/olimpic2.jpeg" alt="逻辑回归与最大熵模型"></div><header class="post__info"><h1 class="post__title">逻辑回归与最大熵模型</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2019-10-13</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/MachineLearning/">MachineLearning</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><p><strong>最大熵是概论模型学习的一个准则。</strong></p><div class="toc"><ul><li><a href="#luo-ji-hui-gui">逻辑回归</a><ul><li><a href="#luo-ji-hui-gui-fen-bu">逻辑回归分布：</a></li><li><a href="#er-xiang-luo-ji-hui-gui-mo-xing">二项逻辑回归模型：</a></li><li><a href="#luo-ji-hui-gui-mo-xing-te-dian">逻辑回归模型特点：</a></li></ul></li><li><a href="#zui-da-shang-mo-xing">最大熵模型</a><ul><li><a href="#shang">熵</a></li><li><a href="#zui-da-shang-mo-xing">最大熵模型：</a></li><li><a href="#mo-xing-xue-xi">模型学习：</a></li></ul></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><h1><span id="luo-ji-hui-gui">逻辑回归</span><a href="#luo-ji-hui-gui" class="header-anchor"></a></h1><h2><span id="luo-ji-hui-gui-fen-bu">逻辑回归分布：</span><a href="#luo-ji-hui-gui-fen-bu" class="header-anchor"></a></h2><p>对于连续随机变量X，X服从逻辑回归分布是指X具有以下分布函数和密度函数：<br><img src="/2019/10/13/logistic/func.jpeg" alt="分布函数与密度函数"></p><h2><span id="er-xiang-luo-ji-hui-gui-mo-xing">二项逻辑回归模型：</span><a href="#er-xiang-luo-ji-hui-gui-mo-xing" class="header-anchor"></a></h2><p>二项逻辑回归是一种分类模型，由条件概论分布P(Y|X)表示，其中Y取值为0或1，其条件概论分布为：<br><img src="/2019/10/13/logistic/regression.jpeg" alt="二项逻辑回归条件概率"><br>对于给定的输入x，可以求出P(Y=1|x)和 P(Y=0|x)，逻辑回归通过比较两个条件概率的大小，将x分到概率值大的类中。</p><h2><span id="luo-ji-hui-gui-mo-xing-te-dian">逻辑回归模型特点：</span><a href="#luo-ji-hui-gui-mo-xing-te-dian" class="header-anchor"></a></h2><p>一个事件的几率（odds）是指该事件发生与不发生的概率比值，如果事件发生的概率是p，则几率为p/(1-p)，其对数几率或logit函数是logit(p) = log(p/1-p)，对逻辑回归而言:<br>$log(P(Y=1|x)/(1-P(Y=1|x)) = wx$<br>即输出Y=1的对数几率是x的线性函数。</p><h1><span id="zui-da-shang-mo-xing">最大熵模型</span><a href="#zui-da-shang-mo-xing" class="header-anchor"></a></h1><h2><span id="shang">熵</span><a href="#shang" class="header-anchor"></a></h2><p>假设离散随机变量X的概率分布是P(X),其熵为：<br><img src="/2019/10/13/logistic/entropy.jpeg" alt="熵"><br>熵满足下列不等式：<br>$0&lt;=H(P)&lt;=log|X|$<br>其中|X|指X的取值个数，当且仅当X当分布是均匀分布时，右边等号成立，即均匀分布时，熵最大。<br>最大熵原理是概率模型学习的一个准则，其认为在学习概率模型时，熵最大的模型是最好的。直观地说，即在选择概率模型时，首先要满足已有的事实，在没有更多信息的情况下，那些不确定的部分是“等可能的”，“等可能”不容易操作，而熵则是一个可以优化的数值指标，通过最大化熵来表示等可能。<br>举例说明，当没有给任何多余信息时，我们猜测抛掷硬币时，每面朝上的概率都是0.5，仍骰子时，每面朝上的概率都是1/6，即没有额外信息时，认为都是等可能是“最合理”。假如有一枚骰子，扔出1和4的概率之和是1/2，则此时我们认为1和4朝上的概率是1/4，而其余四面朝上的概率是1/8，即首先要满足已知信息，没有额外信息时，均匀分布“最合理”。</p><h2><span id="zui-da-shang-mo-xing">最大熵模型：</span><a href="#zui-da-shang-mo-xing" class="header-anchor"></a></h2><p><img src="/2019/10/13/logistic/p_entropy.jpeg" alt="条件熵"><br>其中条件熵最大的模型称为最大熵模型</p><h2><span id="mo-xing-xue-xi">模型学习：</span><a href="#mo-xing-xue-xi" class="header-anchor"></a></h2><p>最大熵模型的学习可以形式化为约束最优化问题。通过引入拉格朗日乘子将约束最优化问题转化为无约束最优化的对偶问题。</p><p>PS：<br>西瓜书上认为对于任意单调可微函数g(),令<br>$(y) = wx + b$<br>这样的模型称为广义线性模型，其中函数g为联系函数。对于二分类任务，其输出为0或1，而线性模型的输出为实数域，需要一个函数将实数域的输出转化到0/1值，最理想的函数是单位阶跃函数，即大于0输出1，小于0输出0，等于0输出0.5，但是该函数不连续，不能用作g(),所以使用sigmoid函数替代。<br>逻辑回归有很多优点，如直接对分类可能性进行建模，无需事先假设数据分布，不仅预测出类别，而是得到近似概率预测。</p><p>统计学习方法中给出逻辑回归分布函数，通过引入最大熵模型，求解模型；西瓜书中通过广义线性模型，将sigmoid函数作为连续函数g给出罗辑回归模型，并说明罗辑回归不需要事先假设数据分布。两者都没有明确说明为什么是sigmoid函数，而答案在<a href="http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf" target="_blank" rel="noopener">http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf</a> 可以看到，感兴趣的可以读读。</p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于奥森</p><div class="post__prevs"><div class="post__prev"><a href="/2019/10/10/hello-world/" title="hello-world"><i class="iconfont icon-prev"></i>hello-world</a></div><div class="post__prev post__prev--right"><a href="/2019/10/13/singleton/" title="python中实现单例模式">python中实现单例模式<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Reading/">Reading</a><span class="block-list-count">14</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">2</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">3</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2020/01/11/npm-environment/" title="记一次npm环境问题"><div class="item__cover"><img src="/2020/01/11/npm-environment/cover.jpeg" alt="记一次npm环境问题"></div><div class="item__info"><h3 class="item__title">记一次npm环境问题</h3><span class="item__text">2020-01-11</span></div></a></li><li class="latest-post-item"><a href="/2019/12/27/backup/" title="Backup"><div class="item__cover"><img src="/2019/12/27/backup/cover.jpeg" alt="Backup"></div><div class="item__info"><h3 class="item__title">Backup</h3><span class="item__text">2019-12-27</span></div></a></li><li class="latest-post-item"><a href="/2019/12/26/from-softmax-to-crf/" title="from softmax to crf"><div class="item__cover"><img src="/2019/12/26/from-softmax-to-crf/cover.jpeg" alt="from softmax to crf"></div><div class="item__info"><h3 class="item__title">from softmax to crf</h3><span class="item__text">2019-12-26</span></div></a></li><li class="latest-post-item"><a href="/2019/12/09/longton/" title="分享一个有趣的游戏"><div class="item__cover"><img src="/2019/12/09/longton/ant.gif" alt="分享一个有趣的游戏"></div><div class="item__info"><h3 class="item__title">分享一个有趣的游戏</h3><span class="item__text">2019-12-09</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/DeepLearning/">DeepLearning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/MachineLearning/">MachineLearning</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLP/">NLP</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Thinking/">Thinking</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>