<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>深入谈谈word2vec | 小蛋子</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="NLP, 机器学习, 深度学习, Python, Backend"><meta name="description" content="NLP | Machine Learning | Developer"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://xv44586.github.io/2019/10/21/deep-w2v/index.html"><link rel="icon" type="image/png" href="/img/favicon.ico" sizes="32x32"><meta name="google-site-verification" content="tbK2z0UTHcAWdqNCgEwykaDA9vvvXnN4ZSp_LFbAbDc"><meta name="baidu-site-verification" content="NBO0j1DAOy"><meta name="baidu-site-verification" content="ulZR80nUkv"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="小蛋子"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?b1d2a1d2b250300950a8ffb5caa20818";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="stylesheet" href="/scss/views/page/post.css"><link rel="alternate" href="/atom.xml" title="小蛋子" type="application/atom+xml"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(/other/loading.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="小蛋子" alt="小蛋子"><img src="/img/lg.png" alt="小蛋子"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/2019/10/21/deep-w2v/flower.jpeg" alt="深入谈谈word2vec"></div><header class="post__info"><h1 class="post__title">深入谈谈word2vec</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/xv44586">小蛋子</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2019-10-21</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/word2vec/">Word2vec</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#nnlm-fu-za-du">NNLM复杂度</a></li><li><a href="#you-hua-fang-an">优化方案</a><ul><li><a href="#1-hierarchical-softmax">1.Hierarchical softmax</a></li><li><a href="#2-negative-sampling">2.negative sampling</a></li></ul></li><li><a href="#si-chong-xun-lian-fang-an">四种训练方案</a></li><li><a href="#si-kao">思考</a></li><li><a href="#hui-da">回答</a></li><li><a href="#guan-yu-tou-tu">关于头图</a></li></ul></div><h1><span id="nnlm-fu-za-du">NNLM复杂度</span><a href="#nnlm-fu-za-du" class="header-anchor"></a></h1><p>原始的NNLM在训练词向量时非常耗时，尤其是大规模语料上，作者在论文后也提出了可能的优化方案，所以word2vec的关注点就是如果更加有效的在大规模语料上训练词向量。<br>每个训练样本的计算复杂度：<br>$$<br>Q = N * D + N * D * H + H * V<br>$$<br>其中V是词典大小，每个词编码为1-of-V，N是当前序列中当前词的前N个词，D是词向量大小，H是神经网络层中隐层神经元个数。<br>这个$Q$中的主要部分是最后的$H * V$ 部分，但通过一些优化方法可以降低（hs/ng), 所以此时的主要的复杂度来着 $N * D * H$, 所以作者直接将神经网络层去掉，来提高计算效率。<br>作者之前的工作发现成功的训练一个神经网络语言模型可以通过两步进行：1，首先通过一个简单模型训练词向量，2，然后在这之上训练N-gram NNLM。同时，增加当前词后续的词（下文信息）可以得到更好的结果。基于此，提出了两种结构的模型：<br><img src="/2019/10/21/deep-w2v/model.png" alt="CBOW vs  Skip-gram"></p><p>1.CBOW：与NNLM类似，但是将网络层去掉，同时使用当前词的下文，即通过将上下文窗口内的词投影得到统一向量，然后预测当前词。此时模型内词的顺序不再影响投影结果，计算复杂度为：$Q = N * D + D * log(V)$<br>2.Skip-gram: 与CBOW类似，不过是通过当前此来预测上下文内的词，为了提高效率，在实际工程上对上下文内的词进行了采样，此时的计算复杂度为：$Q = C * (D + D * log(V))$</p><h1><span id="you-hua-fang-an">优化方案</span><a href="#you-hua-fang-an" class="header-anchor"></a></h1><p>之前提到原始NNLM中的主要计算复杂度是输出层，即 $H * V$，主要的优化思路是避免全量计算V的概率，作者实现了两种方案，即 Hierarchical Softmax 和negative sampling</p><h2><span id="1-hierarchical-softmax">1.Hierarchical softmax</span><a href="#1-hierarchical-softmax" class="header-anchor"></a></h2><p>通过词频构建霍夫曼树，然后将输出层用霍夫曼树替换，上一层结果与每个节点做二分类，判断属于词类，叶子节点为对应的词，判断属于该词的概率</p><p><img src="/2019/10/21/deep-w2v/hs.png" alt></p><p>特点：高频词的位置更靠近根节点，所需的计算进一步降低。但对于低频词，其对应位置远离根，对应路径长，所需计算量依然很大，效率不高</p><h2><span id="2-negative-sampling">2.negative sampling</span><a href="#2-negative-sampling" class="header-anchor"></a></h2><p>在输出层避免对全量字典进行判断，而通过先验知识来圈出最容易混淆的一部分，然后组成负样本（相对于当前词）。作者提出通过词频来归一化后的比例来组成一定比例的候选集，随机的在候选集选取一定数量的负样本(n &lt;&lt; V)来组成负样本集，最后的softmax多分类层变成多个sigmod二分类层，来提高计算效率及词向量的质量。</p><h1><span id="si-chong-xun-lian-fang-an">四种训练方案</span><a href="#si-chong-xun-lian-fang-an" class="header-anchor"></a></h1><ul><li>1.基于hs的CBOW</li></ul><p><img src="/2019/10/21/deep-w2v/hs-cbow.png" alt><br>其中</p><p><img src="/2019/10/21/deep-w2v/xw.png" alt></p><p><img src="/2019/10/21/deep-w2v/pw.png" alt></p><p><img src="/2019/10/21/deep-w2v/gd.png" alt></p><p>对应的伪代码：</p><p><img src="/2019/10/21/deep-w2v/code.png" alt></p><ul><li>2.基于hs的Skip-gram<br><img src="/2019/10/21/deep-w2v/sk.png" alt></li></ul><p>对应的伪代码：<br><img src="/2019/10/21/deep-w2v/sk-code.png" alt></p><ul><li>3.基于ng的CBOW</li></ul><p><img src="/2019/10/21/deep-w2v/ng-cbow.png" alt><br><img src="/2019/10/21/deep-w2v/G.png" alt="image.png"></p><p>对应伪代码：</p><p><img src="/2019/10/21/deep-w2v/ng-code.png" alt></p><ul><li>4.基于hs的skip-gram<br><img src="/2019/10/21/deep-w2v/opt.png" alt></li></ul><p>G中表达式与基于hs的CBOW一样，只是在最外面多了一层求和，后面的过程与CBOW一样。</p><h1><span id="si-kao">思考</span><a href="#si-kao" class="header-anchor"></a></h1><ul><li>1.词向量的训练过程是一个fake task，我们的目标不是最后的语言模型，而是在这个过程中产生的feature vector，用一个real task 来训练是不是更好？</li><li>2.因为是个fake task，那我们如何评估这个task，又如何评估得到的词向量的质量？论文中使用了近似词对及线性平移的特性，有没有更好的方式？</li><li>3.词向量的“similarity”具体是什么含义？</li></ul><h1><span id="hui-da">回答</span><a href="#hui-da" class="header-anchor"></a></h1><ul><li>1.用real task来训练一般会得到更好的词向量，但一般下游任务都是在词向量之上构建，所以一般情况是训练一个词向量，然后作为embedding层的初始参数进行下游任务的训练。</li><li>2.除了相似词及线性平移性，其他情况下可以通过下游任务的效率来评估。</li><li>3.词向量的“similarity”跟通常意义的近义词或相似词有本质上的区别，词向量更多的含义是“同位词”，即上下文相近的词。换个角度，我们将模型的连接函数形式写出来：</li></ul><p><img src="/2019/10/21/deep-w2v/pwk.png" alt></p><p>上式中v分别对应左右两个词向量空间的词向量，由于模型是对称的，所以实际使用时左右两个词向量可以任选一个。</p><p>其中<img src="/2019/10/21/deep-w2v/pwi.png" alt></p><p>分母是归一化项，暂时忽略，最终最大化$P(w_k|w_i)$的同时，即让 Vwk与 Vwi的内积更大。即模型内隐式的用词向量的内积（方向）来表示词向量直接的距离远近（语义距离），所以可以利用词向量的cosine来寻找语义更接近的词。进一步的，左右两个词向量分属不同的向量空间，最小化两个词的语义距离被转化为最小化两个词在不同语义空间的距离，而不是在同一个向量空间，为什么这种方案可行？原文里提到是因为放在同一个向量空间（同一个矩阵），两个词向量正交在一起，不好优化，，分开放在两个向量空间更利于优化。两个词向量空间为什么可行？我认为主要是因为模型是对称的，虽然两个向量空间不同，但是可以认为是只是经过了旋转缩放，词在向量空间的相对位置没有发生改变（词向量之间的角度）。</p><p>优点：</p><ul><li>1.没有神经网络层，所以没有耗时的矩阵相乘，只保留了一个softmax层，计算效率高。</li><li>2.优化时使用的是随机梯度下降，罕见词不会主导优化目标<br>demo：<a href="https://github.com/xv44586/Papers/blob/master/NLP/WordVector/word2vecDemo.ipynb" target="_blank" rel="noopener">https://github.com/xv44586/Papers/blob/master/NLP/WordVector/word2vecDemo.ipynb</a></li></ul><p><strong>论文</strong><br><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1301.3781.pdf</a><br><a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">https://arxiv.org/abs/1310.4546</a></p><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于苏州</p><div class="content-footer-sponsor"><h1><span id="sponsor">Buy me a coffee</span></h1><p>如果觉得这篇文章不错，对你有帮助，欢迎打赏一杯蜜雪冰城。</p><img src="/img/sponsor.JPG" alt="logo" title="sponsor"></div><div class="post__prevs"><div class="post__prev"><a href="/2019/10/17/dropout/" title="Dropout--深度神经网络中的Bagging"><i class="iconfont icon-prev"></i>Dropout--深度神经网络中的Bagging</a></div><div class="post__prev post__prev--right"><a href="/2019/10/22/w2v-summary/" title="词向量总结">词向量总结<i class="iconfont icon-next"></i></a></div></div></div></article><script src="https://giscus.app/client.js" data-repo="xv44586/giscus" data-repo-id="R_kgDOIC6Ipg" data-category="Announcements" data-category-id="DIC_kwDOIC6Ips4CRkmo" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="zh-CN" crossorigin="anonymous" async></script></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">NLP | Machine Learning | Developer</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/Programming/">Programming</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">33</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Math/">Math</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/MachineLearning/">MachineLearning</a><span class="block-list-count">8</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/Life/">Life</a><span class="block-list-count">6</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2023/03/25/gpt4/" title="GPT-4 yes!! but"><div class="item__cover"><img src="/2023/03/25/gpt4/bg.jpeg" alt="GPT-4 yes!! but"></div><div class="item__info"><h3 class="item__title">GPT-4 yes!! but</h3><span class="item__text">2023-03-25</span></div></a></li><li class="latest-post-item"><a href="/2023/03/10/llm-inf/" title="LLM Inference串讲"><div class="item__cover"><img src="/2023/03/10/llm-inf/sd.PNG" alt="LLM Inference串讲"></div><div class="item__info"><h3 class="item__title">LLM Inference串讲</h3><span class="item__text">2023-03-10</span></div></a></li><li class="latest-post-item"><a href="/2023/02/01/fine-tuning-at-few-shot/" title="few-shot视角下的fine-tuning"><div class="item__cover"><img src="/2023/02/01/fine-tuning-at-few-shot/himalayas.JPG" alt="few-shot视角下的fine-tuning"></div><div class="item__info"><h3 class="item__title">few-shot视角下的fine-tuning</h3><span class="item__text">2023-02-01</span></div></a></li><li class="latest-post-item"><a href="/2023/01/09/zero-to-chatgpt/" title="From zero to ChatGPT"><div class="item__cover"><img src="/2023/01/09/zero-to-chatgpt/chatgpt-bg.jpeg" alt="From zero to ChatGPT"></div><div class="item__info"><h3 class="item__title">From zero to ChatGPT</h3><span class="item__text">2023-01-09</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/BERT/">BERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/BPE/">BPE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Bagging/">Bagging</a></li><li class="tag-item"><a class="tag-link" href="/tags/Boosting/">Boosting</a></li><li class="tag-item"><a class="tag-link" href="/tags/CCF/">CCF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CRF/">CRF</a></li><li class="tag-item"><a class="tag-link" href="/tags/CUDA/">CUDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/ChatGPT/">ChatGPT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Classification/">Classification</a></li><li class="tag-item"><a class="tag-link" href="/tags/Competition/">Competition</a></li><li class="tag-item"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Distillation/">Distillation</a></li><li class="tag-item"><a class="tag-link" href="/tags/EDA/">EDA</a></li><li class="tag-item"><a class="tag-link" href="/tags/FastBERT/">FastBERT</a></li><li class="tag-item"><a class="tag-link" href="/tags/Few-shot/">Few-shot</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-3/">GPT-3</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPT-4/">GPT-4</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Game/">Game</a></li><li class="tag-item"><a class="tag-link" href="/tags/Glove/">Glove</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/In-context-learning/">In-context learning</a></li><li class="tag-item"><a class="tag-link" href="/tags/Inference/">Inference</a></li><li class="tag-item"><a class="tag-link" href="/tags/LLM/">LLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/LR/">LR</a></li><li class="tag-item"><a class="tag-link" href="/tags/Language-Model/">Language Model</a></li><li class="tag-item"><a class="tag-link" href="/tags/Loss/">Loss</a></li><li class="tag-item"><a class="tag-link" href="/tags/MarkDown/">MarkDown</a></li><li class="tag-item"><a class="tag-link" href="/tags/Math/">Math</a></li><li class="tag-item"><a class="tag-link" href="/tags/Matrix/">Matrix</a></li><li class="tag-item"><a class="tag-link" href="/tags/NLG/">NLG</a></li><li class="tag-item"><a class="tag-link" href="/tags/Optimizer/">Optimizer</a></li><li class="tag-item"><a class="tag-link" href="/tags/Python/">Python</a></li><li class="tag-item"><a class="tag-link" href="/tags/QA/">QA</a></li><li class="tag-item"><a class="tag-link" href="/tags/R-Drop/">R-Drop</a></li><li class="tag-item"><a class="tag-link" href="/tags/Random-Forest/">Random Forest</a></li><li class="tag-item"><a class="tag-link" href="/tags/Segmentation/">Segmentation</a></li><li class="tag-item"><a class="tag-link" href="/tags/SimCSE/">SimCSE</a></li><li class="tag-item"><a class="tag-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-item"><a class="tag-link" href="/tags/Survey/">Survey</a></li><li class="tag-item"><a class="tag-link" href="/tags/T5/">T5</a></li><li class="tag-item"><a class="tag-link" href="/tags/UniLM/">UniLM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Unigram/">Unigram</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordPiece/">WordPiece</a></li><li class="tag-item"><a class="tag-link" href="/tags/Words-Distance/">Words Distance</a></li><li class="tag-item"><a class="tag-link" href="/tags/Xgboost/">Xgboost</a></li><li class="tag-item"><a class="tag-link" href="/tags/debug/">debug</a></li><li class="tag-item"><a class="tag-link" href="/tags/faster-decoder/">faster decoder</a></li><li class="tag-item"><a class="tag-link" href="/tags/fine-tuning/">fine-tuning</a></li><li class="tag-item"><a class="tag-link" href="/tags/horovod/">horovod</a></li><li class="tag-item"><a class="tag-link" href="/tags/multi-task/">multi-task</a></li><li class="tag-item"><a class="tag-link" href="/tags/nohup/">nohup</a></li><li class="tag-item"><a class="tag-link" href="/tags/npm/">npm</a></li><li class="tag-item"><a class="tag-link" href="/tags/simbert/">simbert</a></li><li class="tag-item"><a class="tag-link" href="/tags/skipgram/">skipgram</a></li><li class="tag-item"><a class="tag-link" href="/tags/speed-up/">speed-up</a></li><li class="tag-item"><a class="tag-link" href="/tags/swift/">swift</a></li><li class="tag-item"><a class="tag-link" href="/tags/tensorflow-gpu/">tensorflow-gpu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-item"><a class="tag-link" href="/tags/信息熵/">信息熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/新词发现/">新词发现</a></li><li class="tag-item"><a class="tag-link" href="/tags/样本不均衡/">样本不均衡</a></li><li class="tag-item"><a class="tag-link" href="/tags/装机/">装机</a></li><li class="tag-item"><a class="tag-link" href="/tags/领域词挖掘/">领域词挖掘</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Beijing, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>xv44586@gmail.com</span></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://www.matrix67.com/" title="Matrix67" target="_blank">Matrix67</a></li><li class="list-item"><a href="https://spaces.ac.cn/" title="Spaces" target="_blank">科学空间</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>. modified by <a href="https://github.com/xv44586" target="_blank">小蛋子</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/xv44586" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="xv44586@gmail.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
            });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
                for (i=0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
                }
            });</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>