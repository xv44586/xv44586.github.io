<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hello-world</title>
    <url>/2019/10/10/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>Hello World!</strong></p>
<div class="toc">

<!-- toc -->
<ul>
<li><a href="#qi-yin">起因</a></li>
<li><a href="#xuan-xing">选型</a></li>
<li><a href="#chu-zhong">初衷</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="qi-yin">起因</span><a href="#qi-yin" class="header-anchor"></a></h1><p>日常的阅读思考需要常常总结记录，偶尔回过头来再看看，尤其近两年论文阅读的越来越多，自己的理解难免浅薄，也需要有一个其他人有可能参与<br>并能一起讨论的地方。之前一直使用印象笔记，但是这个更”personal”，需要一个更”public”的方式。</p>
<h1><span id="xuan-xing">选型</span><a href="#xuan-xing" class="header-anchor"></a></h1><p><strong>私建博客</strong></p>
<p>最初是在阿里云上私建博客，而后因为种种原因，一年后宣告失败 - -！（其实主要是因为思考少，也懒的写～～）为了避免在一个地方摔两次跟头，所以pass</p>
<p><strong>知乎/csdn/简书</strong></p>
<p>由于对csdn实在没好感，pass，知乎上创建了一个专栏，结果需要审核，等了几天（当时在职时应该给自己加点”戏”的233333），最后排除上述答案后，选C。<br>其实以上选项都有一个共同问题：网站出问题了，你的内容就出问题了。</p>
<p><strong>GitPage</strong></p>
<p>简书也不给力，比知乎审查更让人受不了，整个九月都无法正常使用（让我少更多少啊，阻止我进步！！），最后还是需要一个更稳定的，维护成本低的，<br>不需要”审核”的方案，所以选了Hexo + GitPage（主要是这个主题我喜欢～)，之前简书上的内容也会陆续迁移过来，简书就当作是临时草稿箱吧～<br>PS.原主题地址：<a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank" rel="noopener">https://github.com/Mrminfive/hexo-theme-skapp</a></p>
<h1><span id="chu-zhong">初衷</span><a href="#chu-zhong" class="header-anchor"></a></h1><p>学习上，避免学习中只听不说，只看不写，只学不练的”浅层”学习，通过写的方式，督促自己多总结思考；生活上，记录日常的碎碎念，也记录下当时的一段时光。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于奥森</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title>Python中的GIL</title>
    <url>/2019/10/13/gil/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#gil-shi-shi-me">GIL是什么</a></li>
<li><a href="#gil-ji-zhi">GIL机制</a></li>
<li><a href="#dai-lai-de-wen-ti">带来的问题</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="gil-shi-shi-me">GIL是什么</span><a href="#gil-shi-shi-me" class="header-anchor"></a></h1><p>熟悉Python的人对GIL肯定都不陌生, 其全称是全局解释器锁(Global Interpreter Lock)。但是，很多人都误以为GIL是python的特性，所以，首先需要明确的一点是GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。而其他的实现版本如JPython就没有GIL。然而因为CPython是大部分环境下默认的Python执行环境。所以在很多人的概念里CPython就是Python，也就想当然的把GIL归结为Python语言的缺陷。所以这里要先明确一点：GIL并不是Python的特性，Python完全可以不依赖于GIL<br>那么CPython实现中的GIL又是什么呢？官方给出的解释：</p>
<blockquote>
<p>In CPython, the global interpreter lock, or GIL, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.)</p>
<p><a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank" rel="noopener">https://wiki.python.org/moin/GlobalInterpreterLock</a></p>
</blockquote>
<p>即原因是在CPython中的线程实际是操作系统的原生线程,而CPython的内存管理不是线程安全的，于是使用GIL这把大锁锁住其他线程,保证同一时刻只有一个线程可以解释执行字节码。</p>
<h1><span id="gil-ji-zhi">GIL机制</span><a href="#gil-ji-zhi" class="header-anchor"></a></h1><p>按照Python社区的想法，操作系统本身的线程调度已经非常成熟稳定了，没有必要自己搞一套。所以Python的线程就是C语言的一个pthread，并通过操作系统调度算法进行调度（例如linux是CFS）。为了让各个线程能够平均利用CPU时间，python会计算当前已执行的微代码数量ticks，达到一定阈值后就强制释放。而这时也会触发一次操作系统的线程调度（当然是否真正进行上下文切换由操作系统自主决定）。<br>Python释放GIL的时机之一:<br>有一个周期性计数的计数器,不断递减,保证Python线程可以在执行一段时间之后释放GIL<br>仔细分析就会发现问题,假如在解析执行字节码的过程中当前线程遇到了一个IO操作,却由于等待数据而被阻塞在该IO操作上,而从GIL的设计来看，GIL只能通过当前线程主动释放,其他线程才有可能获取。而当前<br>线程阻塞在IO操作上时,此时给其他线程运行的机会并没有什么问题,因为GIL只是用来同步线程执行字节码的,并非一般的互斥共享资源的互斥锁。在阻塞操作之前让出GIL,其他线程可以继续执行,而当前线程可以继续执行阻塞型的操作,当该阻塞型的操作完成之后,再次试图获取GIL,继续执行余下的字节码。<br>由此可以看出,Python释放GIL的第二个时机:<br>在IO操作等可能会引起阻塞的system call之前,可以暂时释放GIL,但在执行完毕后,必须重新获取GIL</p>
<h1><span id="dai-lai-de-wen-ti">带来的问题</span><a href="#dai-lai-de-wen-ti" class="header-anchor"></a></h1><p>1.CPU密集型代码(各种循环处理、计数等等)，在这种情况下，ticks计数很快就会达到阈值，然后触发GIL的释放与再竞争（多个线程来回切换当然是需要消耗资源的）<br>2.线程的释放与获取是没有间隙的，单核下没什么问题，多核情况下，当当前master线程释放GIL后，其他核心上的线程被唤醒，而此时大部分情况下当前线程又获取了GIL，而被唤醒的线程只能浪费cpu时间而无法运行，直到达到切换时间，切换为待调度状态，这样会造成线程颠簸(thrashing)，导致效率更低。<br><img src="/2019/10/13/gil/gil_war.png" alt="Multi GIL War"><br>因此，3开始做了一些优化，如将基于pcode调度改为分时调度；避免释放GIL的线程再次立即被调度等。</p>
<hr>
<p><a href="http://www.dabeaz.com/GIL/" target="_blank" rel="noopener">http://www.dabeaz.com/GIL/</a><br><a href="http://cyrusin.github.io/2016/04/27/python-gil-implementaion/" target="_blank" rel="noopener">http://cyrusin.github.io/2016/04/27/python-gil-implementaion/</a><br><a href="https://zhuanlan.zhihu.com/p/20953544" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20953544</a><br><a href="http://cenalulu.github.io/python/gil-in-python/" target="_blank" rel="noopener">http://cenalulu.github.io/python/gil-in-python/</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于世园会</p>
]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>python中的多线程</title>
    <url>/2019/10/13/multiThreads/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<!-- tocstop -->
</div>

<p>  最近在看Python的多线程，经常我们会听到老手说：“Python下多线程是鸡肋，推荐使用多进程！”，但是为什么这么说呢？<br>要知其然，更要知其所以然。所以有了下面的深入研究：<br>首先强调背景：<br><strong>1. GIL是什么？</strong><br>  GIL的全称是Global Interpreter Lock(全局解释器锁)，来源是python设计之初的考虑，为了数据安全所做的决定。</p>
<p><strong>2. 每个CPU在同一时间只能执行一个线程</strong><br>  在单核CPU下的多线程其实都只是并发，不是并行，并发和并行从宏观上来讲都是同时处理多路请求的概念。但并发和并行又有区别，并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔内发生。<br>在Python多线程下，每个线程的执行方式：<br>    <strong><strong>获取GIL</strong></strong><br>    <strong><strong>执行代码直到sleep或者是python虚拟机将其挂起。</strong></strong><br>    <strong><strong>释放GIL</strong></strong></p>
<p>  可见，某个线程想要执行，必须先拿到GIL，我们可以把GIL看作是“通行证”，并且在一个python进程中，GIL只有一个。拿不到通行证的线程，就不允许进入CPU执行。<br>在Python2.x里，GIL的释放逻辑是当前线程遇见IO操作或者ticks计数达到100（ticks可以看作是Python自身的一个计数器，专门作用于GIL，每次释放后归零，这个计数可以通过 sys.setcheckinterval 来调整），进行释放。而每次释放GIL锁，线程进行锁竞争、切换线程，会消耗资源。并且由于GIL锁存在，python里一个进程永远只能同时执行一个线程(拿到GIL的线程才能执行)，这就是为什么在多核CPU上，python的多线程效率并不高。那么是不是python的多线程就完全没用了呢？在这里我们进行分类讨论：</p>
<p>  <strong>CPU密集型</strong><br>  CPU密集型代码包括各种循环处理、计数等等，在这种情况下，由于计算工作多，ticks计数很快就会达到阈值，然后触发GIL的释放与再竞争（多个线程来回切换当然是需要消耗资源的），所以python下的多线程对CPU密集型代码并不友好。</p>
<p>  <strong>IO密集型</strong><br>  IO密集型代码包括文件处理、网络爬虫等，多线程能够有效提升效率(单线程下有IO操作会进行IO等待，造成不必要的时间浪费，而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。所以python的多线程对IO密集型代码比较友好。<br>而在python3.x中，GIL不使用ticks计数，改为使用计时器（执行时间达到阈值后，当前线程释放GIL），这样对CPU密集型程序更加友好，但依然没有解决GIL导致的同一时间只能执行一个线程的问题，所以效率依然不尽如人意。<br>请注意：多核多线程比单核多线程更差，原因是单核下的多线程，每次释放GIL，唤醒的那个线程都能获取到GIL锁，所以能够无缝执行，但多核下，CPU0释放GIL后，其他CPU上的线程都会进行竞争，但GIL可能会马上又被CPU0拿到，导致其他几个CPU上被唤醒后的线程会醒着等待到切换时间后又进入待调度状态，这样会造成线程颠簸(thrashing)，导致效率更低。<br>回到最开始的问题：经常我们会听到老手说：“python下想要充分利用多核CPU，就用多进程”，原因是什么呢？<br>原因是：每个进程有各自独立的GIL，互不干扰，这样就可以真正意义上的并行执行，所以在python中，多进程的执行效率优于多线程(仅仅针对多核CPU而言)。<br>所以在这里说结论：多核下，想做并行提升效率，比较通用的方法是使用多进程，能够有效提高执行效率</p>
<p><strong>关于头图</strong><br>摄于首开广场</p>
]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑回归与最大熵模型</title>
    <url>/2019/10/13/logistic/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>最大熵是概论模型学习的一个准则。</strong></p>
<div class="toc">

<!-- toc -->
<ul>
<li><a href="#luo-ji-hui-gui">逻辑回归</a><ul>
<li><a href="#luo-ji-hui-gui-fen-bu">逻辑回归分布：</a></li>
<li><a href="#er-xiang-luo-ji-hui-gui-mo-xing">二项逻辑回归模型：</a></li>
<li><a href="#luo-ji-hui-gui-mo-xing-te-dian">逻辑回归模型特点：</a></li>
</ul>
</li>
<li><a href="#zui-da-shang-mo-xing">最大熵模型</a><ul>
<li><a href="#shang">熵</a></li>
<li><a href="#zui-da-shang-mo-xing">最大熵模型：</a></li>
<li><a href="#mo-xing-xue-xi">模型学习：</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="luo-ji-hui-gui">逻辑回归</span><a href="#luo-ji-hui-gui" class="header-anchor"></a></h1><h2><span id="luo-ji-hui-gui-fen-bu">逻辑回归分布：</span><a href="#luo-ji-hui-gui-fen-bu" class="header-anchor"></a></h2><p>对于连续随机变量X，X服从逻辑回归分布是指X具有以下分布函数和密度函数：<br><img src="/2019/10/13/logistic/func.jpeg" alt="分布函数与密度函数"></p>
<h2><span id="er-xiang-luo-ji-hui-gui-mo-xing">二项逻辑回归模型：</span><a href="#er-xiang-luo-ji-hui-gui-mo-xing" class="header-anchor"></a></h2><p>二项逻辑回归是一种分类模型，由条件概论分布P(Y|X)表示，其中Y取值为0或1，其条件概论分布为：<br><img src="/2019/10/13/logistic/regression.jpeg" alt="二项逻辑回归条件概率"><br>对于给定的输入x，可以求出P(Y=1|x)和 P(Y=0|x)，逻辑回归通过比较两个条件概率的大小，将x分到概率值大的类中。</p>
<h2><span id="luo-ji-hui-gui-mo-xing-te-dian">逻辑回归模型特点：</span><a href="#luo-ji-hui-gui-mo-xing-te-dian" class="header-anchor"></a></h2><p>一个事件的几率（odds）是指该事件发生与不发生的概率比值，如果事件发生的概率是p，则几率为p/(1-p)，其对数几率或logit函数是logit(p) = log(p/1-p)，对逻辑回归而言:<br>             $log(P(Y=1|x)/(1-P(Y=1|x)) = wx$<br>即输出Y=1的对数几率是x的线性函数。</p>
<h1><span id="zui-da-shang-mo-xing">最大熵模型</span><a href="#zui-da-shang-mo-xing" class="header-anchor"></a></h1><h2><span id="shang">熵</span><a href="#shang" class="header-anchor"></a></h2><p>假设离散随机变量X的概率分布是P(X),其熵为：<br><img src="/2019/10/13/logistic/entropy.jpeg" alt="熵"><br>熵满足下列不等式：<br>            $0&lt;=H(P)&lt;=log|X|$<br>其中|X|指X的取值个数，当且仅当X当分布是均匀分布时，右边等号成立，即均匀分布时，熵最大。<br>最大熵原理是概率模型学习的一个准则，其认为在学习概率模型时，熵最大的模型是最好的。直观地说，即在选择概率模型时，首先要满足已有的事实，在没有更多信息的情况下，那些不确定的部分是“等可能的”，“等可能”不容易操作，而熵则是一个可以优化的数值指标，通过最大化熵来表示等可能。<br>举例说明，当没有给任何多余信息时，我们猜测抛掷硬币时，每面朝上的概率都是0.5，仍骰子时，每面朝上的概率都是1/6，即没有额外信息时，认为都是等可能是“最合理”。假如有一枚骰子，扔出1和4的概率之和是1/2，则此时我们认为1和4朝上的概率是1/4，而其余四面朝上的概率是1/8，即首先要满足已知信息，没有额外信息时，均匀分布“最合理”。</p>
<h2><span id="zui-da-shang-mo-xing">最大熵模型：</span><a href="#zui-da-shang-mo-xing" class="header-anchor"></a></h2><p><img src="/2019/10/13/logistic/p_entropy.jpeg" alt="条件熵"><br>其中条件熵最大的模型称为最大熵模型</p>
<h2><span id="mo-xing-xue-xi">模型学习：</span><a href="#mo-xing-xue-xi" class="header-anchor"></a></h2><p>最大熵模型的学习可以形式化为约束最优化问题。通过引入拉格朗日乘子将约束最优化问题转化为无约束最优化的对偶问题。</p>
<p>PS：<br>西瓜书上认为对于任意单调可微函数g(),令<br>             $(y) = wx + b$<br>这样的模型称为广义线性模型，其中函数g为联系函数。对于二分类任务，其输出为0或1，而线性模型的输出为实数域，需要一个函数将实数域的输出转化到0/1值，最理想的函数是单位阶跃函数，即大于0输出1，小于0输出0，等于0输出0.5，但是该函数不连续，不能用作g(),所以使用sigmoid函数替代。<br>逻辑回归有很多优点，如直接对分类可能性进行建模，无需事先假设数据分布，不仅预测出类别，而是得到近似概率预测。</p>
<p>统计学习方法中给出逻辑回归分布函数，通过引入最大熵模型，求解模型；西瓜书中通过广义线性模型，将sigmoid函数作为连续函数g给出罗辑回归模型，并说明罗辑回归不需要事先假设数据分布。两者都没有明确说明为什么是sigmoid函数，而答案在<a href="http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf" target="_blank" rel="noopener">http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf</a> 可以看到，感兴趣的可以读读。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于奥森</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络语言模型(NNLM)</title>
    <url>/2019/10/13/nnlm/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#tong-ji-yu-yan-mo-xing">统计语言模型</a></li>
<li><a href="#nnlm">NNLM</a><ul>
<li><a href="#why-it-works">why it works?</a></li>
<li><a href="#bing-xing">并行</a></li>
<li><a href="#out-of-vocabulary-word">out-of-vocabulary word</a></li>
<li><a href="#hou-xu-gong-zuo">后续工作</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="tong-ji-yu-yan-mo-xing">统计语言模型</span><a href="#tong-ji-yu-yan-mo-xing" class="header-anchor"></a></h1><p>首先看一个例子：<br><strong>ztc/ 上下/ 齐/ 拼搏/ ，誓为/ 春战/ 做/ 贡献</strong><br>这句话呢通顺，意思明白，那如果换一下词的位置：<br><strong>上下/ 齐/ 拼搏/ ztc/ ，春站/ 做/ 贡献/ 誓为</strong><br>意思含糊了，但是大概意思还是能猜到，那如果在变换一下：<br><strong>拼搏/ 齐/ ztc/ 上下/ ，贡献/ 誓为/ 做/ 春战</strong><br>现在这句话已经不知所云了，如何判断这个由词序组成的序列是否符合文法、含义是否正确？</p>
<p><strong>统计语言模型：一个句子是否合理，就看他的可能性的大小，即他的概率大小。</strong></p>
<p>假设一个句子S，由一连串特定顺序的词$W_1, W_2,…W_T$ 组成，T是句子中词的个数，则S出现的概率$P(S) = P(w_1, w_2,…w_T)$<br>利用条件概率公式展开：<br>$P(w_1,w_2,..w_T) = P(w_1)*P(w_2|w_1)*P(w_3|w_1,w_2)*…*P(w_T|w_1,w_2,..w_{T-1})$<br>即：<br><img src="/2019/10/13/nnlm/p.png" alt><br>当语料中词典大小为100,000，句子平均长度为5时，需要学习的参数大概$100000 * 5 -1$ 个，为了降低计算复杂度，并考虑到词序列中离的更近的词通常在语义上也更相关，所以在计算时可以通过只使用前面<code>n-1</code>个词来近似计算，即<code>n-grams</code>：<br><img src="/2019/10/13/nnlm/ngram.png" alt></p>
<p>n-grams存在的问题：1.泛化时常常有训练语料中没有出现过的词序列；2.没有考虑词之间的相似性。</p>
<h1><span id="nnlm">NNLM</span><a href="#nnlm" class="header-anchor"></a></h1><p><img src="/2019/10/13/nnlm/arc.png" alt="Neural architecture"></p>
<ul>
<li>1.对词库里的每个词指定一个分布的词向量</li>
<li>2.定义联合概率（通过序列中词对应的词向量)</li>
<li>3.学习词向量和概率函数的参数</li>
</ul>
<h2><span id="why-it-works">why it works?</span><a href="#why-it-works" class="header-anchor"></a></h2><p>如果我们已知 “走” 和 “跑” 是相似词，那很容易通过 ”猫在屋里跑“ 推出 “猫在屋里走“，因为相似的词会有相似的词向量，而且概率函数是特征的平滑函数，所以特征的微小变化，只会对概率值产生一个很小的影响。即：1.相似词在特征空间距离更接近；2.概率函数是一个相对平滑的函数，对特征值的变化不是非常敏感。<br>所以训练语料中句子的出现不光增加了自身的概率，也增加了他与周围句子的概率（句子向量空间）<br>目标：$f(w_t ,··· ,w_{t−n+1}) = Pˆ(w_t |w_1,w_2,..w_{t-1} )<br>约束：</p>
<ul>
<li><ol>
<li>$∑ |V| i=1 f(i,wt−1,··· ,wt−n+1) = 1$  </li>
</ol>
</li>
<li>2.$f&gt;0$</li>
</ul>
<p>通过得到的条件概率进行相乘，得到词序列的联合概率.<br>模型被分成二部分：<br>1.<strong>特征映射：通过映射矩阵 C∈R ∣V∣×m</strong><br>将输入的每个词映射为一个特征向量，C(i)∈Rm 表示词典中第 i 个词对应的特征向量，其中 m 表示特征向量的维度。<br>2.<strong>概率函数g</strong><br>通过context中词的词向量来映射下一个词的条件概率。g的输出是一个向量，其中第i个元素表示了字典中第i个词的概率。完整的模型表达如下：<br>       $f(i,w_{t−1},··· ,w_{t−n+1}) = g(i,C(wt−1),··· ,C(wt−n+1))$<br>函数f由两个映射（g and c)组成，其中c由所有的上下文共享。<br>训练过程中的参数就由两个映射组成，设 g 对应参数为w，c映射的参数就是自身，则 θ=（c, w)<br>训练过程就是学习θ的最大似然：<br><img src="/2019/10/13/nnlm/l.ong" alt><br>其中R(θ) 是正则项。<br>模型中参数与字典大小V成线性关系，且与n（n-grams)成线性关系，不过可以通过共享结构降低参数数量，如延时神经网络或循环神经网络。<br>实验中，神经网络层只有一个隐层，有一个可选的词向量到输出的直连层，实际上就有两个隐层，一个共享的词向量C 层，该层没有激活函数，还有一个tanh激活函数的隐层；最后的输出层是一个softmax层，来保证所有结果的和为1：<br><img src="/2019/10/13/nnlm/pnew.png" alt></p>
<p>注意：第一层是没有非线性激活函数的，因为非线性激活函数会带来其他信息（联想神经网络中非线性激活函数），而正是这种直接的线性变换，才能让第一层的参数来作为词向量<br>用yi表示每个输出词的对数概率，则<br>$y = b+Wx+U tanh(d +Hx)$<br>其中x是词向量的拼接，$x = (c(wt-1),c(wt-2),c(wt-n+1))$</p>
<h2><span id="bing-xing">并行</span><a href="#bing-xing" class="header-anchor"></a></h2><p>参数与输入的窗口大小和字典的大小成线性，但是计算量却比n-grams 要大很多，首先n-grams中不需要每次都计算所有词的概率，只需要相关词频的线性组合，另外神经网络中主要瓶颈是输出层的激活计算。</p>
<h2><span id="out-of-vocabulary-word">out-of-vocabulary word</span><a href="#out-of-vocabulary-word" class="header-anchor"></a></h2><p>首先根据窗口上下文可能出现的词，进行加权求和初始化新词的词向量，然后将新词 j 加入字典，然后利用这部分数据集重新训练，进行retune.</p>
<h2><span id="hou-xu-gong-zuo">后续工作</span><a href="#hou-xu-gong-zuo" class="header-anchor"></a></h2><ul>
<li>1，分解网络到子网络，如使用词聚类，构建许多小的子网络可能更快更简单</li>
<li>2，用树结构来表达条件概率：神经网络作用在每一个节点上，每个节点代表根据上下问得到该词类的可能性，叶子节点代表词的可能性，这种结构可以将计算复杂度从|v| 降低到 log|v|</li>
<li>3，梯度传播时可以只在部分输出词上进行，如某些条件下最相似的（如三元模型）。如果用在语音识别，可以只计算听觉上相似的词。</li>
<li>4，引入先验知识，如语义信息和语法信息。通过在神经网络结构中共享更多的结构与参数，可以捕获长期的上下文信息，</li>
<li>5，如何解释神经网络得到的词向量</li>
<li>6，上述模型对每个单词分配一个在语义空间的点，所以无法解决一词多义问题。如何扩展当前模型，在语义空间中为词分配多个点来代表词的不同语义。</li>
</ul>
<p>作者提出的后续工作中，目前是很多人的研究方向，一些已经被证明有效。</p>
<ul>
<li>第一个，优化网络结构，提到了从数据方向，构建更多的子网络，还可以直接对网络结构本身进行优化，如word2vec，将神经网络层去掉；</li>
<li>第二个，由于计算瓶颈在计算output的概率（对每个词计算概率，需要softmax归一化）,所以提出可以通过树结构，来避免直接对所有词进行计算，如 Hierarchical Softmax</li>
<li>第三个也是在计算输出时，只通过一部分词来进行梯度传播，如负采样</li>
<li>第四个是通过共享结构，来捕获更多上下文信息，如GPT，Bert</li>
<li>第五个是如何解释，也是目前很多人的研究方向</li>
<li>第六个是一次多义的解决方法，如ELMO</li>
</ul>
<hr>
<p>参考：<br><a href="http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf" target="_blank" rel="noopener">http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于望京soho</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Language Model</tag>
      </tags>
  </entry>
  <entry>
    <title>python中实现单例模式</title>
    <url>/2019/10/13/singleton/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<!-- tocstop -->
</div>

<p>python中实现单例模式的方式大致有四种：<br>1.模块<br>2.改写类的<strong>new</strong>方法，控制实例生成<br>3.装饰器<br>4.元类</p>
<p>1.模块<br>python中的模块是天然的单例模式，并且是线程安全的，所有的模块只会在运行时加载一次。<br>所以，利用模块就可以实现一个线程安全的单例。如：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my singleton.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">My_singleton</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">my_singleton = My_singleton()</span><br></pre></td></tr></table></figure></p>
<ol start="2">
<li><strong>new</strong>方法<br>python中每个类在实例化对象时，首先会调用<strong>new</strong>方法创建一个实例，然后再调用<strong>init</strong>方法动态绑定其他属性.如：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Singleton</span><span class="params">(object)</span>:</span></span><br><span class="line">    _instance = <span class="literal">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, *args, **kw)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> cls._instance:</span><br><span class="line">            cls._instance = super(Singleton, cls).__new__(cls, *args, **kw)  </span><br><span class="line">        <span class="keyword">return</span> cls._instance  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(Singleton)</span>:</span>  </span><br><span class="line">    a = <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>3.装饰器<br>装饰器可以动态的修改一个类或方法的表现，利用装饰器生成单例<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">singleton</span><span class="params">(cls)</span>:</span></span><br><span class="line">    instances = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">    @wraps(cls)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getinstance</span><span class="params">(*args, **kw)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> instances:</span><br><span class="line">            instances[cls] = cls(*args, **kw)</span><br><span class="line">        <span class="keyword">return</span> instances[cls]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> getinstance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@singleton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    a = <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>4.元类<br>元类可以控制其子类的类对象(<strong>new</strong>)及类实例对象(<strong>call</strong>)的创建。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Singleton</span><span class="params">(type)</span>:</span></span><br><span class="line">    _instances = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(cls, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> cls._instances:</span><br><span class="line">            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> cls._instances[cls]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Python2</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    __metaclass__ = Singleton</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Python3</span></span><br><span class="line">    <span class="comment"># class MyClass(metaclass=Singleton):</span></span><br><span class="line">    <span class="comment">#    pass</span></span><br></pre></td></tr></table></figure></p>
<p>最著名的就是Django中的ORM，其model中就使用了元类来控制所有model的表现</p>
<p><strong>关于头图</strong></p>
<p>摄于内蒙古乌兰布统草原</p>
]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Bagging为什么能降低过拟合</title>
    <url>/2019/10/16/bagging/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#pian-chai-yu-fang-chai">偏差与方差</a></li>
<li><a href="#jiang-di-mo-xing-guo-ni-he">降低模型过拟合</a><ul>
<li><a href="#ji-cheng-mo-xing">集成模型</a></li>
<li><a href="#bagging">Bagging</a></li>
<li><a href="#sui-ji-sen-lin">随机森林</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="pian-chai-yu-fang-chai">偏差与方差</span><a href="#pian-chai-yu-fang-chai" class="header-anchor"></a></h1><p><strong>偏差 (bias)</strong><br><img src="/2019/10/16/bagging/fx.png" alt><br>即模型的期望预测与真实值之间的差异。</p>
<p><strong>方差 (variance)</strong><br><img src="/2019/10/156/bagging/var.png" alt><br>方差通常衡量模型对<strong>不同数据集</strong>的敏感程度，也可以认为是衡量模型的不稳定性。若方差大，则表示数据的微小变动就能导致学习出的模型产生较大差异，即对应的模型结构风险更高。</p>
<p>有了偏差和方差的定义，我们就能推导出模型的期望泛化误差：</p>
<p><img src="/2019/10/16/bagging/ed.png" alt><br>如果我们能在保持bias基本不变时，降低variance，则模型的期望泛化误差降低，从而降低模型过拟合风险。</p>
<h1><span id="jiang-di-mo-xing-guo-ni-he">降低模型过拟合</span><a href="#jiang-di-mo-xing-guo-ni-he" class="header-anchor"></a></h1><h2><span id="ji-cheng-mo-xing">集成模型</span><a href="#ji-cheng-mo-xing" class="header-anchor"></a></h2><p>假设我们现在有一个集成模型，其过程为从整体样本中进行采样，得到n份独立且与整体同分布的样本集，然后选择同样的模型进行训练，最后取平均。由于单个模型对应数据同分布，模型相同，则对应的bias和variance相同，而</p>
<p><img src="/2019/10/16/bagging/exi.png" alt></p>
<p>所以最终模型的bias与单模型的bias相同；另一方面，由于各个子模型独立，则</p>
<p><img src="/2019/10/16/bagging/varn.png" alt></p>
<p>此时可以显著降低模型的variance，根据模型泛化误差期望公式，此时的集成模型的期望泛化误差将小于单模型的期望泛化误差，从而降低了模型的过拟合。</p>
<h2><span id="bagging">Bagging</span><a href="#bagging" class="header-anchor"></a></h2><p>针对上述集成模型，当各个子模型相同时，</p>
<p><img src="/2019/10/16/bagging/varxi.png" alt><br>此时不会降低variance。</p>
<p>对应公式：设有n个随机变量，两两变量之间的相关性为𝜌，则方差为</p>
<p><img src="/2019/10/16/bagging/p.png" alt><br>Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。由于子样本集有相似性，同时也使用同种模型，则各个子模型有相似的bias和variance，由上面结论可知，此时的bias与单模型近似相同，所以bagging不能显著降低bias。（因此在选择模型时，需要选择bias小的模型）子模型介于相同与独立两个极端情况之间，所以对应variance会处于var(x) 与 var(x)/n之间，即通过降低上述公式中的第二项降低整体方差。<br>而根据模型期望泛化误差公式，由于方差的降低，也能带来最终模型的期望泛化误差的降低，从而降低过拟合。</p>
<h2><span id="sui-ji-sen-lin">随机森林</span><a href="#sui-ji-sen-lin" class="header-anchor"></a></h2><p>随机森林是一种常用的Bagging模型，其通过对样本进行有放回的采样，构造n个样本集，同时对特征列进行采样后进行模型训练，即同时降低上述公式中的两项，来降低方差，从而降低过拟合。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于杭州青芝坞</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Bagging</tag>
      </tags>
  </entry>
  <entry>
    <title>Dropout--深度神经网络中的Bagging</title>
    <url>/2019/10/17/dropout/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#shen-du-shen-jing-wang-luo-guo-ni-he-wen-ti">深度神经网络过拟合问题</a></li>
<li><a href="#dropout">Dropout</a></li>
<li><a href="#bu-tong-dropout-fang-an-dui-bi-shi-yan">不同dropout方案对比实验</a></li>
<li><a href="#updating">updating…</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="shen-du-shen-jing-wang-luo-guo-ni-he-wen-ti">深度神经网络过拟合问题</span><a href="#shen-du-shen-jing-wang-luo-guo-ni-he-wen-ti" class="header-anchor"></a></h1><p>深度神经网络由于其巨大的参数量，可以很方便的拟合非常复杂的非线性关系，同时，巨大的参数量也给模型带来了过拟合的问题。为了解决这个问题，也有人提出了早停和加入正则项等手段。而在传统机器学习中，除了这些手段，还有一种手段来解决这个问题，即bagging（参考我之前的文章<a href="https://xv44586.github.io/2019/10/16/bagging/">Bagging为什么能降低过拟合</a>），最典型的就是随机森林：对样本和特征进行抽样，训练多个模型，然后进行集成（投票/求平均）。那如何将这种思路引入到深度神经网络中呢？<br>首先,由于训练单个深度神经网络就已经非常耗时，通常样本量也不够多，不适合采样，所以像随机森林一样抽样训练多个模型的方案不可取。<br>剩下的思路就是用“一个”模型来模拟多个sub-model，那如何来模拟呢？</p>
<h1><span id="dropout">Dropout</span><a href="#dropout" class="header-anchor"></a></h1><p>对于深度神经网络，其最重要的部分就是其隐藏单元（神经元），对于一个有n个神经元的层，我们可以通过设置神经元是否激活，来模拟$2^n$ 种结构，即sub-model，而在evaluate阶段，我们将这些所有的sub-mdoel的结果进行求平均。那对于现在的结构，不可能对 $2^n$中结构都去做计算然后再求平均，一种近似的做法是用当前这“一个”模型来近似模拟：设置所有神经元都处于激活状态，同时，对每个神经元的输出乘以其激活概率keep_prob.这就是Dropout的背后思想。<br><img src="/2019/10/17/dropout/dropout.png" alt="Dropout"><br>简单总结Dropout的具体做法：训练阶段，在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作；预测阶段，对每个神经元的输出乘以1-p (keep_prob)。<br>接下来思考一下，为什么这么做work:首先，训练阶段部分神经元失活，对应的结果是部分features不参与计算，本质上是对features进行采样；其次，从整个训练过程中看，每次训练（batch-data),都对应不同的失活神经元（sub-model)，对应的每个sub-model在单个epoch内，都是在对样本进行无放回的抽样，本质上是在bagging。最后，在预测阶段，为什么可以用“一个”完整的模型来模拟sub-model的求平均过程呢？从sub-model的角度看，每个sub-model被training的概率为$(1-p)$, 而神经元对所有sub-model是共享的，唯一的区别是是否激活，所以归一到每个神经元上，单个神经元被training（激活）的概率为$(1-p)$,而sub-model的总数是$2^n$，每个神经元求平均的过程即$Out_i * (1-p) * 2^n/ 2^n = Out_i * (1 -p)$.<br>具体实现时，我们的目的是在训练阶段对神经元进行随机（概率p)失活,而在test和evaluate时，对神经元的输出乘以$(1-p)$,所以在实现时，可以采用一个trick：dropout时，对样本进行mask的同时，将其除以$(1-p)$,这样就可以一次计算完成所有逻辑，同时，把所有逻辑保留在整个层中。<br><strong>tf的实现：</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, keep_prob, noise_shape=None, seed=None, name=None)</span>:</span>  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">  <span class="string">"""Computes dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  With probability `keep_prob`, outputs the input element scaled up by</span></span><br><span class="line"><span class="string">  `1 / keep_prob`, otherwise outputs `0`.  The scaling is so that the expected</span></span><br><span class="line"><span class="string">  sum is unchanged.</span></span><br><span class="line"><span class="string">    ...</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line">  <span class="keyword">with</span> ops.name_scope(name, <span class="string">"dropout"</span>, [x]) <span class="keyword">as</span> name:</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># Do nothing if we know keep_prob == 1</span></span><br><span class="line">    <span class="keyword">if</span> tensor_util.constant_value(keep_prob) == <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    noise_shape = noise_shape <span class="keyword">if</span> noise_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> array_ops.shape(x)</span><br><span class="line">    <span class="comment"># uniform [keep_prob, 1.0 + keep_prob)</span></span><br><span class="line">    random_tensor = keep_prob</span><br><span class="line">    random_tensor += random_ops.random_uniform(noise_shape,</span><br><span class="line">                                               seed=seed,</span><br><span class="line">                                               dtype=x.dtype)</span><br><span class="line">    <span class="comment"># 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)</span></span><br><span class="line">    binary_tensor = math_ops.floor(random_tensor)</span><br><span class="line">    ret = math_ops.div(x, keep_prob) * binary_tensor</span><br><span class="line">    <span class="keyword">if</span> context.in_graph_mode():</span><br><span class="line">      ret.set_shape(x.get_shape())</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure></p>
<h1><span id="bu-tong-dropout-fang-an-dui-bi-shi-yan">不同dropout方案对比实验</span><a href="#bu-tong-dropout-fang-an-dui-bi-shi-yan" class="header-anchor"></a></h1><p>如果在非dropout阶段不进行scaled会如何？<br>scaled<br><img src="/2019/10/17/dropout/scaled.png" alt="scaled.png"><br>without_scaled<br><img src="/2019/10/17/dropout/without-scaled.png" alt="without_scaled.png"><br>实验也说明非dropout阶段如果没有进行scaled（求平均），对应的loss会比train阶段高，同时acc也会降低。</p>
<p><strong>桥豆麻袋，到这里好像出现了一点问题：</strong><br>按照我们上面的思路，在test和evaluate阶段，我们从对sub-model求平均转化为对每个神经元的output进行scaled down，即 $activation(x * W ) * (1 - p)$， 而我们在实现时，只是对x进行scaled up操作，如果后面接的层的激活函数是线性的，这样处理没有什么问题，但是，后面的层不总是线性激活函数，那此时，$output = activation(x * (1-p) * W)  != activation(x*W) * (1 - p)$,即我们得到的输出与我们想要的并不一样，按照以上的理解，我们对output进行scaled-down，验证一下两者的区别。<br>scaled_input<br><img src="/2019/10/17/dropout/scaled-input.png" alt="scaled_input.png"><br>scaled_output<br><img src="/2019/10/17/dropout/scaled-output.png" alt="scaled_output.png"></p>
<p>看上去对output进行scaled-down结果稍微好一点，但是并不显著，此时的激活函数是relu，而且是在倒数第二层，换个激活函数试试。<br>scaled_input_tanh<br><img src="/2019/10/17/dropout/scaled_input_tanh.png" alt="scaled_input_tanh.png"><br>scaled_output_tanh<br><img src="/2019/10/17/dropout/scaled_output_tanh.png" alt="scaled_output_tanh.png"><br>结果看上去对output进行scaled-down效果稍微差一些，但差距不大。<br>代码地址<a href="https://github.com/xv44586/Papers/tree/master/DeepLearning/Dropout" target="_blank" rel="noopener">https://github.com/xv44586/Papers/tree/master/DeepLearning/Dropout</a>，感兴趣的可以试试其他方式。<br>既然两种方式在结果上看，效果差不多，而对output进行scaled-down需要添加一个AfterDropLayer，逻辑会在不同的层中，而对inputs直接进行scaled-up，所有逻辑都保存在一个layer中，更清晰。<br><strong>But，Why？</strong>为什么两种方式的结果效果差异不大？真让人头秃啊！</p>
<p><strong>论文地址</strong><br><a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="noopener">http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</a></p>
<p>================================</p>
<h1><span id="updating">updating…</span><a href="#updating" class="header-anchor"></a></h1><p><img src="/2019/10/17/dropout/linear.png" alt="image.png"></p>
<p>上图是sigmoid函数在[-8,8]区间的图像，其中linear是由[-8,-4,-2,2,4,8]截断的直线，看图可以看出，sigmoid在区间内都非常的接近”linear”，梯度变化较大的部分只在几个拐点周围，大部分都是近似”linear”,所以也就解释了为什么两种方式有差异，但是差异并不大。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>大四竞赛作品截图</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Glove模型</title>
    <url>/2019/10/17/glove/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#yi-zheng-ti-si-lu">一、整体思路</a></li>
<li><a href="#er-ji-ben-jia-she">二、基本假设</a></li>
<li><a href="#san-mo-xing">三、模型</a></li>
<li><a href="#si-dui-bi">四、对比</a></li>
<li><a href="#wu-si-kao">五、思考</a></li>
<li><a href="#zai-si-kao">再思考：</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="yi-zheng-ti-si-lu">一、整体思路</span><a href="#yi-zheng-ti-si-lu" class="header-anchor"></a></h1><p>获取词向量基本上有两种思路：</p>
<ul>
<li>1.利用全局统计信息，进行矩阵分解（如LSA）来获取词向量，这样获得的词向量往往在词相似性任务上表现不好，表明这是一个次优的向量空间结构；</li>
<li>2.利用局部上下文窗口单独训练，但是统计信息作为有用的先验知识，没有很好的利用到。   </li>
</ul>
<p>Glove：结合两种训练方式，获取更好的词向量</p>
<h1><span id="er-ji-ben-jia-she">二、基本假设</span><a href="#er-ji-ben-jia-she" class="header-anchor"></a></h1><p>词的共现次数与其语义的相关性往往不是严格成比例，所以直接用共线性来表征词之间相关性效果不好，因此，作者通过引入第三个词，通过词之间的差异来刻画相关性。差异选择用两个词与同一个词的共现概率的次数来更好的判断词之间的相关性。比率：$ratio_{i,j,k}=\frac{Pi,k}{Pj,k} $<br>看下面这个例子：<br><img src="/2019/10/17/glove/table1.png" alt="image.png"><br>ice 与solid相关性高，而steam与solid相关性弱，对应比例大于1；ice与gas相关性弱，steam与gas相关性高，对应比例小于1，ice与steam都与water相关，对应比例约等于1，ice与steam与fashion都不相关，对应比例也是约等于1.<br>相关性的规律：<br><img src="/2019/10/17/glove/ratio.png" alt="image.png"></p>
<h1><span id="san-mo-xing">三、模型</span><a href="#san-mo-xing" class="header-anchor"></a></h1><p>模型的数学形式为：<br><img src="/2019/10/17/glove/1.png" alt><br>其中$w_i$, $w_j$ 与$w_k$分属不同的两个词向量空间（参考skipgram），对于F函数，我们希望他能够在向量空间内预测$p(P_{ik}/P_{jk})$这个比率，由于向量空间的线性结构，最自然的方式就是用向量的差，即：<br><img src="/2019/10/17/glove/2.png" alt><br>等式的右侧是一个标量，左侧F函数可以是一个复杂函数，而我们上面提到我们希望捕捉向量的线性结构，所以避免使用复杂函数，首先将参数做内积：<br><img src="/2019/10/17/glove/3.png" alt><br>在窗口滑动的过程中，中心词与上下文词的角色会相互转化，但是当词的位置互换后，其相关性应该是保持一致的，所以，F函数需要对<code>和</code>操作与<code>商</code>操作上同态（这里同态的意思是F函数在左右两侧应该是一致的，也就是 $F((w_i - w_j)）= F(w_i) / F(w_j)$：<br><img src="/2019/10/17/glove/4.png" alt><br>其中：<br><img src="/2019/10/17/glove/5.png" alt></p>
<p>为了解决 上式4，F函数的形式就是exp(指数形式），最终求解后：<br><img src="/2019/10/17/glove/6.png" alt><br>上式中，等式左侧是对称的，即$W_i^T*W_j = W_j^T*W_i$, 而右侧是不对称的，即$log(p_{ij}) != log(P_{ji})$. 如果上式的右侧没有$log(x_i)$,则等式的左右就对称了，考虑到与$k$无关，所以把这一项并入到$i$的偏差项中，即：<br><img src="/2019/10/17/glove/7.png" alt><br>由于上式中有$log$，所以需要处理0值，同时，对于低频与高频的共线词都不能过度训练，于是，优化目标就变成了：<br><img src="/2019/10/17/glove/8.png" alt><br>其中，权重函数$f(x)$需要满足：</p>
<ul>
<li>1， f(0)=0</li>
<li>2, 非减以避免低频共现过度训练</li>
<li>3，抑制高频共现避免过度训练<br>最后采用的$f(x)$ 形式为：<br><img src="/2019/10/17/glove/9.png" alt><br>实验中他们采用的是xmax=100, a=3/4<br><img src="/2019/10/17/glove/scratch.png" alt="完整过程"></li>
</ul>
<h1><span id="si-dui-bi">四、对比</span><a href="#si-dui-bi" class="header-anchor"></a></h1><p>与局部窗口方式对比：<br><img src="/2019/10/17/glove/comp.png" alt="与local window对比"><br>优化目标使用不同的损失函数，并带有调和函数来降低高频词的影响。<br>语义相似性结果对比：<br><img src="/2019/10/17/glove/result.png" alt="不同模型对比"></p>
<h1><span id="wu-si-kao">五、思考</span><a href="#wu-si-kao" class="header-anchor"></a></h1><ul>
<li>1.相对与word2vec, Glove引入了词频统计信息，这是很重要的全局信息。</li>
<li>2.word2vec的训练次数与词频相关，Glove的训练中词频是loss的weight，高频低频词的overweight的情况更低。</li>
<li>3.将基于局部窗口的模型中，相同词进行合并，修改对应object：<img src="/2019/10/17/glove/13.png" alt><br>其中$H$为交叉熵，相对Glove的object：<br><img src="/2019/10/17/glove/16.png" alt><br>loss由交叉熵改为最小二乘，$X_i$改为$f(X_i)$函数进行调和。</li>
<li>4.Glove中的左右词向量也是两个不同的词向量空间，与word2vec一样，虽然Glove模型上看上去可以使用同一个词向量空间做，但是作者说是因为更好优化且模型更稳定，不同的时，最后的结果是左右词向量求和（虽然word2vec也可以这么做）</li>
</ul>
<p>Demo:<a href="https://github.com/xv44586/Papers/blob/master/NLP/WordVector/GloveDemo.ipynb" target="_blank" rel="noopener">https://github.com/xv44586/Papers/blob/master/NLP/WordVector/GloveDemo.ipynb</a></p>
<hr>
<h1><span id="zai-si-kao">再思考：</span><a href="#zai-si-kao" class="header-anchor"></a></h1><ul>
<li>1.通常我们都是根据模型来推导其对应的性质，而Glove是因为其应该具有的性质，来反推模型，这种方式也给人提供了一种新思路。</li>
<li>2.为什么两种模型都有两套词向量空间（中心词向量和上下文词向量）？虽然两个作者都说是因为更好优化且模型更稳定，那有没有更合理的理论上的解释呢？我的理解是：对于word2vec，模型直接对概率$p(w|context)$,如skipgram中，直接对$P(w_2|w_1)$进行建模，而$P(w_2|w_1$)与$P(w_1|w_2)$并不一定相等，所以需要针对词的位置区分，也就是需要两套不一样的词向量空间；而Glove中，如上文中公式（6）所示，模型右侧有一个与位置有关的参数，虽然通过引入两个bias可以一定程度上消除这个位置相关的参数，但是这个参数并不是均匀分布，所以仅通过bias不能完全解决这个问题，而引入两个不同的词向量空间，相当于是引入了位置信息，这样能更好的解决这个问题。其最本质的原因是在窗口滑动过程中，词位置变化的同时信息可能是不对称的，即以a为中心词的窗口中的b在以b为窗口时，a可能丢失。</li>
<li>3.对于上式8，存在一个比较严重的问题，模型为了消去位置相关参数，将其吸收进bias内，而这个bias的引入，就导致了一个严重的问题，即模型不适定。<br><img src="/2019/10/17/glove/ret.png" alt><br>即当你求得一组解后，你可以给这组解加上一个常数向量，其还是一组解。那这个问题就很严重了，你无法评估你得到的解是哪组解。如果加上的是非常大的常数向量，那这组词向量在很多度量上就失去了意义（如余弦距离）<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>摄于北京某水库</li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Glove</tag>
      </tags>
  </entry>
  <entry>
    <title>随机森林模型中是不是树越多越好</title>
    <url>/2019/10/17/randomForest/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#sui-ji-sen-lin-zhong-shu-yue-duo-yue-hao">随机森林中树越多越好？</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="sui-ji-sen-lin-zhong-shu-yue-duo-yue-hao">随机森林中树越多越好？</span><a href="#sui-ji-sen-lin-zhong-shu-yue-duo-yue-hao" class="header-anchor"></a></h1><p>面试时被问到在随机森林的树是否数量越多越好？开始只考虑构建更多的树一来浪费资源，二来数量一定后模型的性能基本保持稳定，随着树的增加提升非常小。<br>回来后又想了想，随机森林中通过引入随机抽样和随机抽列，使模型对异常点有更好的鲁棒性，模型的泛化能力更强。如果是无限颗树，那其实会抵消随机性的引入，最后的模型会是一个过拟合的模型，其泛化性能也会降低。此外，噪音较大时，模型也会学习到更多噪音相关的信息，发生过拟合，降低泛化性能。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于玉渊潭公园</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Random Forest</tag>
      </tags>
  </entry>
  <entry>
    <title>深入谈谈word2vec</title>
    <url>/2019/10/21/deep-w2v/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#nnlm-fu-za-du">NNLM复杂度</a></li>
<li><a href="#you-hua-fang-an">优化方案</a><ul>
<li><a href="#1-hierarchical-softmax">1.Hierarchical softmax</a></li>
<li><a href="#2-negative-sampling">2.negative sampling</a></li>
</ul>
</li>
<li><a href="#si-chong-xun-lian-fang-an">四种训练方案</a></li>
<li><a href="#si-kao">思考</a></li>
<li><a href="#hui-da">回答</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="nnlm-fu-za-du">NNLM复杂度</span><a href="#nnlm-fu-za-du" class="header-anchor"></a></h1><p>原始的NNLM在训练词向量时非常耗时，尤其是大规模语料上，作者在论文后也提出了可能的优化方案，所以word2vec的关注点就是如果更加有效的在大规模语料上训练词向量。<br>每个训练样本的计算复杂度：<br>$$<br>Q = N * D + N * D * H + H * V<br>$$<br>其中V是词典大小，每个词编码为1-of-V，N是当前序列中当前词的前N个词，D是词向量大小，H是神经网络层中隐层神经元个数。<br>这个$Q$中的主要部分是最后的$H * V$ 部分，但通过一些优化方法可以降低（hs/ng), 所以此时的主要的复杂度来着 $N * D * H$, 所以作者直接将神经网络层去掉，来提高计算效率。<br>作者之前的工作发现成功的训练一个神经网络语言模型可以通过两步进行：1，首先通过一个简单模型训练词向量，2，然后在这之上训练N-gram NNLM。同时，增加当前词后续的词（下文信息）可以得到更好的结果。基于此，提出了两种结构的模型：<br><img src="/2019/10/21/deep-w2v/model.png" alt="CBOW vs  Skip-gram"></p>
<p>1.CBOW：与NNLM类似，但是将网络层去掉，同时使用当前词的下文，即通过将上下文窗口内的词投影得到统一向量，然后预测当前词。此时模型内词的顺序不再影响投影结果，计算复杂度为：$Q = N * D + D * log(V)$<br>2.Skip-gram: 与CBOW类似，不过是通过当前此来预测上下文内的词，为了提高效率，在实际工程上对上下文内的词进行了采样，此时的计算复杂度为：$Q = C * (D + D * log(V))$</p>
<h1><span id="you-hua-fang-an">优化方案</span><a href="#you-hua-fang-an" class="header-anchor"></a></h1><p>之前提到原始NNLM中的主要计算复杂度是输出层，即 $H * V$，主要的优化思路是避免全量计算V的概率，作者实现了两种方案，即 Hierarchical Softmax 和negative sampling</p>
<h2><span id="1-hierarchical-softmax">1.Hierarchical softmax</span><a href="#1-hierarchical-softmax" class="header-anchor"></a></h2><p>通过词频构建霍夫曼树，然后将输出层用霍夫曼树替换，上一层结果与每个节点做二分类，判断属于词类，叶子节点为对应的词，判断属于该词的概率</p>
<p><img src="/2019/10/21/deep-w2v/hs.png" alt></p>
<p>特点：高频词的位置更靠近根节点，所需的计算进一步降低。但对于低频词，其对应位置远离根，对应路径长，所需计算量依然很大，效率不高</p>
<h2><span id="2-negative-sampling">2.negative  sampling</span><a href="#2-negative-sampling" class="header-anchor"></a></h2><p>在输出层避免对全量字典进行判断，而通过先验知识来圈出最容易混淆的一部分，然后组成负样本（相对于当前词）。作者提出通过词频来归一化后的比例来组成一定比例的候选集，随机的在候选集选取一定数量的负样本(n &lt;&lt; V)来组成负样本集，最后的softmax多分类层变成多个sigmod二分类层，来提高计算效率及词向量的质量。</p>
<h1><span id="si-chong-xun-lian-fang-an">四种训练方案</span><a href="#si-chong-xun-lian-fang-an" class="header-anchor"></a></h1><ul>
<li>1.基于hs的CBOW</li>
</ul>
<p><img src="/2019/10/21/deep-w2v/hs-cbow.png" alt><br>其中</p>
<p><img src="/2019/10/21/deep-w2v/xw.png" alt></p>
<p><img src="/2019/10/21/deep-w2v/pw.png" alt></p>
<p><img src="/2019/10/21/deep-w2v/gd.png" alt></p>
<p>对应的伪代码：</p>
<p><img src="/2019/10/21/deep-w2v/code.png" alt></p>
<ul>
<li>2.基于hs的Skip-gram<br><img src="/2019/10/21/deep-w2v/sk.png" alt></li>
</ul>
<p>对应的伪代码：<br><img src="/2019/10/21/deep-w2v/sk-code.png" alt></p>
<ul>
<li>3.基于ng的CBOW</li>
</ul>
<p><img src="/2019/10/21/deep-w2v/ng-cbow.png" alt><br><img src="/2019/10/21/deep-w2v/G.png" alt="image.png"></p>
<p>对应伪代码：</p>
<p><img src="/2019/10/21/deep-w2v/ng-code.png" alt></p>
<ul>
<li>4.基于hs的skip-gram<br><img src="/2019/10/21/deep-w2v/opt.png" alt></li>
</ul>
<p>G中表达式与基于hs的CBOW一样，只是在最外面多了一层求和，后面的过程与CBOW一样。</p>
<h1><span id="si-kao">思考</span><a href="#si-kao" class="header-anchor"></a></h1><ul>
<li>1.词向量的训练过程是一个fake task，我们的目标不是最后的语言模型，而是在这个过程中产生的feature vector，用一个real task 来训练是不是更好？</li>
<li>2.因为是个fake task，那我们如何评估这个task，又如何评估得到的词向量的质量？论文中使用了近似词对及线性平移的特性，有没有更好的方式？</li>
<li>3.词向量的“similarity”具体是什么含义？</li>
</ul>
<h1><span id="hui-da">回答</span><a href="#hui-da" class="header-anchor"></a></h1><ul>
<li>1.用real task来训练一般会得到更好的词向量，但一般下游任务都是在词向量之上构建，所以一般情况是训练一个词向量，然后作为embedding层的初始参数进行下游任务的训练。</li>
<li>2.除了相似词及线性平移性，其他情况下可以通过下游任务的效率来评估。</li>
<li>3.词向量的“similarity”跟通常意义的近义词或相似词有本质上的区别，词向量更多的含义是“同位词”，即上下文相近的词。换个角度，我们将模型的连接函数形式写出来：</li>
</ul>
<p><img src="/2019/10/21/deep-w2v/pwk.png" alt></p>
<p>上式中v分别对应左右两个词向量空间的词向量，由于模型是对称的，所以实际使用时左右两个词向量可以任选一个。</p>
<p>其中<img src="/2019/10/21/deep-w2v/pwi.png" alt></p>
<p>分母是归一化项，暂时忽略，最终最大化$P(w_k|w_i)$的同时，即让 Vwk与 Vwi的内积更大。即模型内隐式的用词向量的内积（方向）来表示词向量直接的距离远近（语义距离），所以可以利用词向量的cosine来寻找语义更接近的词。进一步的，左右两个词向量分属不同的向量空间，最小化两个词的语义距离被转化为最小化两个词在不同语义空间的距离，而不是在同一个向量空间，为什么这种方案可行？原文里提到是因为放在同一个向量空间（同一个矩阵），两个词向量正交在一起，不好优化，，分开放在两个向量空间更利于优化。两个词向量空间为什么可行？我认为主要是因为模型是对称的，虽然两个向量空间不同，但是可以认为是只是经过了旋转缩放，词在向量空间的相对位置没有发生改变（词向量之间的角度）。</p>
<p>优点：</p>
<ul>
<li>1.没有神经网络层，所以没有耗时的矩阵相乘，只保留了一个softmax层，计算效率高。</li>
<li>2.优化时使用的是随机梯度下降，罕见词不会主导优化目标<br>demo：<a href="https://github.com/xv44586/Papers/blob/master/NLP/WordVector/word2vecDemo.ipynb" target="_blank" rel="noopener">https://github.com/xv44586/Papers/blob/master/NLP/WordVector/word2vecDemo.ipynb</a></li>
</ul>
<p><strong>论文</strong><br><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1301.3781.pdf</a><br><a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">https://arxiv.org/abs/1310.4546</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于苏州</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>词向量总结</title>
    <url>/2019/10/22/w2v-summary/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#yi-tong">异同</a><ul>
<li><a href="#xiang-tong-dian">相同点</a></li>
<li><a href="#bu-tong-dian">不同点</a></li>
</ul>
</li>
<li><a href="#xing-zhi">性质</a><ul>
<li><a href="#pmi-jiao-du">PMI角度</a></li>
<li><a href="#ke-jia-xing">可加性</a></li>
<li><a href="#mo-chang">模长</a></li>
<li><a href="#xiang-guan-xing">相关性</a></li>
</ul>
</li>
<li><a href="#ying-yong">应用</a><ul>
<li><a href="#liang-ge-ju-zi-de-xiang-guan-xing">两个句子的相关性</a></li>
<li><a href="#zhong-xin-ci-guan-jian-ci-ti-qu">中心词（关键词）提取</a></li>
<li><a href="#ju-xiang-liang">句向量</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="yi-tong">异同</span><a href="#yi-tong" class="header-anchor"></a></h1><p>本文主要讨论<code>Glove</code>和<code>word2vec</code>两种模型对应词向量。</p>
<h2><span id="xiang-tong-dian">相同点</span><a href="#xiang-tong-dian" class="header-anchor"></a></h2><ul>
<li>两种模型都是在对词对的<code>PMI</code>做分解，所以他们具有相同的性质（向量可加性，点积，余弦距离，模长等）。</li>
<li>模型的基本形式都是向量的点积，且都有两套词向量空间（单词向量空间与上下文词向量空间）。</li>
</ul>
<h2><span id="bu-tong-dian">不同点</span><a href="#bu-tong-dian" class="header-anchor"></a></h2><ul>
<li>1.通常我们都是根据模型来推导其对应性质，而Glove是通过其性质来反推模型，这种方式还是给人眼前一亮的。</li>
<li>2.除Glove以外的词向量模型都是对条件概率$P(w|context)$进行建模，如<code>word2vec</code>的<code>SkipGram</code>对$P(w_2|w_1)$进行建模，但是这个信息是有缺点的，首先，他不是一个严格对称的模型，即$P(w_2|w_1)$  与 $P(w_1|w_2)$ 并不一定相等，所以，在建模时需要把上下文与中心词向量区分开，不能放到同一个向量空间；其次，这个概率是有界的、归一化的量，所以在模型里需要用softmax等对结果进行归一化，这个也会造成优化上的困难。而<code>Glove</code>可以看作是对$PMI$进行建模，而$PMI$是比概率更对称也更重要的一个量。</li>
<li>3.如Glove论文中所述，就整体目标函数而言，可以看作是两种模型采用了不同的损失函数，其基本形式是一致的。</li>
<li>4.两种模型都是词袋模型（一元模型）。</li>
<li>5.对应词向量内积含义不同：<br><img src="/2019/10/22/w2v-summary/dot.png" alt="内积含义"></li>
</ul>
<h1><span id="xing-zhi">性质</span><a href="#xing-zhi" class="header-anchor"></a></h1><h2><span id="pmi-jiao-du">PMI角度</span><a href="#pmi-jiao-du" class="header-anchor"></a></h2><p>上文说了两种模型都是对词对的PMI做分解，所以我们在解释其性质时直接从PMI的角度来解释。<br>首先来看一下PMI<br><img src="/2019/10/22/w2v-summary/pmi.png" alt="PMI"><br>对于任意两个词序列Q和A，其中$Q=(q1,q2…qk)$, $A = (a1, a2…al)$，我们模型都是采用的词袋模型，即满足朴素假设：每个特征之间相互独立。<br><img src="/2019/10/22/w2v-summary/pqa.png" alt><br>带入朴素假设<br><img src="/2019/10/22/w2v-summary/naive.png" alt><br>用贝叶斯公式变换<br><img src="/2019/10/22/w2v-summary/change.png" alt><br>再用一次朴素假设<br><img src="/2019/10/22/w2v-summary/naive-change.png" alt></p>
<p>最后得到：<br><img src="/2019/10/22/w2v-summary/pmiqa.png" alt><br>即在朴素假设下，两个序列的互信息等于两个序列中各个项的互信息的总和。</p>
<h2><span id="ke-jia-xing">可加性</span><a href="#ke-jia-xing" class="header-anchor"></a></h2><p>在Glove和word2vec中，两个词之间的相关性是通过对应词向量的内积来表达的，即对于词$W_i$, $W_j$, 其相关性等于$&lt;V_i ,V_j&gt;$, 带入上面，即：<br><img src="/2019/10/22/w2v-summary/pmiin.png" alt><br>即两个词序列的相关性可以通过将两个序列内的词向量求和后再进行点积计算。<br>如我们求两个句子的相关度时，可以先将句子内的词对应的词向量进行求和，然后再进行相似性计算。</p>
<h2><span id="mo-chang">模长</span><a href="#mo-chang" class="header-anchor"></a></h2><p>词向量w的模长正比与其内积&lt;w,w&gt;，即正比PMI(w,w)，而在一个滑动窗口内，上下文中的词与中心词相等的概率极低，所以可以认为P(w,w) ~ P(w),推出<br><img src="/2019/10/22/w2v-summary/pww.png" alt><br>即，模长正比与词频的倒数，词频越高（停用词，虚词等），其对应的模长越短，这样就表面模长能在一定程度上代表词本身的重要性。<br>从模型学习的角度来看，词向量的内积等于其模长的乘积乘以余弦值，即<br><img src="/2019/10/22/w2v-summary/cos.png" alt><br>对于高频的几乎没有什么固定搭配的词，其所含语义也相对非常少，即这些词与其他任意词的互信息都非常低，约等于0，而为了让上式等于0，与其不停的调节两个向量的方向，不如让其中一个的模长像0靠近，这样经过多次迭代后，高频的语义少的词的模长就越来越短，逐渐接近0.<br>实验结果中，也能看到按模长排序后，前面的都是高频的语义含量极低的词。<br><img src="/2019/10/22/w2v-summary/sort.png" alt="模长排序结果"><br>可以看到，排在前面的都是高频的语义极少的词（’UNK’，’以及’,’三’，符号等)</p>
<h2><span id="xiang-guan-xing">相关性</span><a href="#xiang-guan-xing" class="header-anchor"></a></h2><p>两个词的互信息正比于词对应向量的内积，即两个词互信息越大，两个词成对出现的几率越高，其对应词向量的内积也就越大，因此，可以通过内积来对词的相关性进行排序。而上面也说了，模长代表了词的重要程度，如果我们不考虑词本身的重要程度，只考虑其词义，可以用向量范数将其归一化后在进行内积计算，这样更稳定.<br><img src="/2019/10/22/w2v-summary/cosij.png" alt><br>即词的相关性可以用词向量之间的余弦距离来计算，这样比只使用内积更稳定。<br>在统计上，互信息为0，则表面这两个词无关，对应到模型，即两个词的词向量的内积为0，而根据向量的知识，两个向量的内积为0，则表明两个向量相互垂直，即两个向量无关。两个词在统计上的无关正好对应其在词向量空间上的几何无关！</p>
<h1><span id="ying-yong">应用</span><a href="#ying-yong" class="header-anchor"></a></h1><h2><span id="liang-ge-ju-zi-de-xiang-guan-xing">两个句子的相关性</span><a href="#liang-ge-ju-zi-de-xiang-guan-xing" class="header-anchor"></a></h2><p>计算两个句子或短语之间的相关性时，我们可以借鉴上面PMI在朴素假设下的性质，将两个句子中的词向量进行求和，再计算两个结果向量之间的相关性，如点积或余弦。<br>而如果一个句子内的词向量的和与某一个词的词向量相关性非常高，可以认为这个句子与这个词表达了相同的语义，或者，在词向量空间内，词对应的向量在句子的聚类中心附近。</p>
<h2><span id="zhong-xin-ci-guan-jian-ci-ti-qu">中心词（关键词）提取</span><a href="#zhong-xin-ci-guan-jian-ci-ti-qu" class="header-anchor"></a></h2><p>所谓中心词（关键词），即能概况句子的意思，通过这些词，我就能大概猜到整个句子的整体内容。即这些词（相对句子内其他词）对整个句子的相关性更高。这样就能将问题转化成词与句子相关性排序问题。<br>通过语言模型的角度来看，语言模型本身就是一个通过上（下）文来预测下一个词概率的模型，即最大化$P(w1,w2,w3..|W)$.而关键词的含义是什么呢？用数学的方式表达就是：<br>对于$S=(w1,w2, w3..wk)$,求解$P(S|wk)$值最高的$wk$,其中$wk$属于$S$。这其实与语言模型的含义是一致的。<br>最终，将问题转化成词与句子之间的相关性排序问题，而上面提到求解两个句子相关性时，可以将句子对应词向量先求和再计算相关性，最后关键词提取就变成：<br>先将句子对应词向量求和，得到sen_vec，然后计算单个词与sen_vec的相关性，然后排序即可。<br><img src="/2019/10/22/w2v-summary/keywords.png" alt><br>可以看到结果还是相当不错的。</p>
<h2><span id="ju-xiang-liang">句向量</span><a href="#ju-xiang-liang" class="header-anchor"></a></h2><p>上面在做句子相关性时，都是为了将计算从$O(n^2)$降低到$O(n)$而将句子内的词向量进行求和，然后再计算。其实也就是用词向量的求和来得到句向量，来作为句子在相同词向量空间的语义。其实这是一种简单又快捷的得到句向量的方式，在很多任务中都可以尝试使用。</p>
<p><strong>refer</strong><br><a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" target="_blank" rel="noopener">neural-word-embedding-as-implicit-matrix-factorization</a><br><a href="https://aclweb.org/anthology/P17-1007" target="_blank" rel="noopener">https://aclweb.org/anthology/P17-1007</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于秦皇岛</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Glove</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2Vec之skip-gram</title>
    <url>/2019/10/13/skipgram/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#word2vec">Word2Vec</a></li>
<li><a href="#fake-task">Fake Task</a></li>
<li><a href="#train">Train</a></li>
<li><a href="#shu-ru">输入</a></li>
<li><a href="#yin-ceng">隐层</a></li>
<li><a href="#shu-chu-ceng">输出层</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="word2vec">Word2Vec</span><a href="#word2vec" class="header-anchor"></a></h1><p><img src="/2019/10/13/skipgram/w2v.jpeg" alt="Word2Vec"><br><code>Word2vec</code> 主要有两种形式，<code>CBOW</code> 和<code>Skip-gram</code>，其中<code>CBOW</code>是通过上下文context来预测当前位置词，<code>SKip-gram</code>则是通过当前词来预测上下文</p>
<h1><span id="fake-task">Fake Task</span><a href="#fake-task" class="header-anchor"></a></h1><p>word2vec 实际上分为两部分，1，建立模型，2，通过模型获取词的嵌入向量（隐层参数）。整个过程与自编码器的思想类似，即基于训练数据训练一个神经网络，模型训练好后，并不会用这个模型处理后续的任务，真正需要的是这个模型学到的参数，如隐层的权重矩阵，基于训练数据建模的过程叫“Fake Task”，意味着建模并不是我们最终的目的。</p>
<h1><span id="train">Train</span><a href="#train" class="header-anchor"></a></h1><p>如何训练我们的神经网络模型？假如我们有一个句子“ The dog barked at the mailman”:</p>
<ul>
<li>首先，我们选择句子中一个词作为我们的input word， 如 dog</li>
<li>然后，我们需要定义一个skip_window参数，来指定上下文的大小，即input word 一侧选取词的数量，假如skip_window=2,那将从dog出发向左右两个方向取最近的两个word，即（the, dog,barked,at)，此时的$span = skip_window * 2 + 1 = 5$<br>另一个需要定义的参数是num_skips，即从上下文中选取多少个word来作为output word，这个参数应该小于等于$2 * skip_window$，即最多将所有上下文都作为output，但是不能重复。如设置num_skips = 2,此时从上下文选取2个词作为output，如（the， barked），最终我们将得到两组训练数据（dog, the) (dog, barked)</li>
</ul>
<p>神经网络将基于这些训练数据输出一个概率分布，这个概率分布代表着在输入数据下，词典中每个词是output的概率。如拿数据（dog， barked）来训练，则模型将会告诉我们每个单词是’barked’的概率大小。<br>模型的输出概率代表着词典中每个单词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。<br>我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size = 2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。</p>
<p><img src="/2019/10/13/skipgram/train.png" alt><br>模型将会从每队单词出现的次数中习得统计结果，模型可能会得到更多的（’soviet’， ‘union’）样本对，而（soviet， dog）这样的组合看到的很少。因此，当模型训练完成后，给定一个单词 soviet，输出结果中union 或者russia会比dog有更高的概率。</p>
<h1><span id="shu-ru">输入</span><a href="#shu-ru" class="header-anchor"></a></h1><p>常用做法是用训练文档构建词汇表，然后再对单词进行one-hot编码。编码后的向量，形如$dog = [0, 0, 1, 0, …0]$, 如果词汇表大小为10000， 那这个向量包含了10000的概率，即为当前词为输入的概率<br>下图是神经网络结构：</p>
<p><img src="/2019/10/13/skipgram/arc.png" alt><br>我们基于成对的单词来对神经网络进行训练， 训练样本是（input word， output word）这样的单词对，input word 和 output word都是one-hot编码的向量，最终的模型输出是一个概率分布。</p>
<h1><span id="yin-ceng">隐层</span><a href="#yin-ceng" class="header-anchor"></a></h1><p>如果我们想要用300个特征来表示一个词（即每个词是300维的向量），即隐层有300个神经元，隐层的权重为$10000 * 300$的矩阵，下图中的左右两个图代表了不同角度看隐层权重，左图中每列代表一个10000维的词向量与隐层单个神经元的连接权重，右图每行代表了一个单词的词向量。<br><img src="/2019/10/13/skipgram/weights.png" alt><br>我们最终的目标就是学习这个隐层权重矩阵。<br>输入被one-hot编码后，实际上只有一个位置不为0，所以这个向量相当稀疏，那如果我们将$1*10000$的向量与$10000*300$的矩阵相乘，相当消耗计算资源，为了高效计算，仅仅会选择矩阵中对应的向量中纬度为1的索引行<br><img src="/2019/10/13/skipgram/lookup.png" alt="lookup table"><br>即实际不会进行矩阵乘法计算，而是根据输入向量中不为0 的维度去索引。这样模型中的隐层权重矩阵便成了一个查找表（lookup table），输出就是输入单词的嵌入词向量</p>
<h1><span id="shu-chu-ceng">输出层</span><a href="#shu-chu-ceng" class="header-anchor"></a></h1><p>隐层的输出是一个1*300的向量，而输出层是一个softmax回归分类器，他的每个结点将会输出一个0-1之间的值（概率），而结点的概率之和为1.<br><img src="/2019/10/13/skipgram/softmax.png" alt><br>我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。<br>举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难（太凶残了）。<br>Word2Vec的作者在它的第二篇论文中强调了这些问题，下面是作者在第二篇论文中的三个创新：</p>
<ul>
<li><ol>
<li>将常见的单词组合（word pairs）或者词组作为单个“words”来处理。</li>
</ol>
</li>
<li><ol start="2">
<li>对高频次单词进行抽样来减少训练样本的个数。</li>
</ol>
</li>
<li><ol start="3">
<li>对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。<br>事实证明，对常用词抽样并且对优化目标采用“negative sampling”不仅降低了训练过程中的计算负担，还提高了训练的词向量的质量。</li>
</ol>
</li>
</ul>
<p><strong>word pairs and phases</strong><br>一些单词组合的含义和拆开以后具有完全不同的意义，比如 New York，单独的New 和York无法表达这个词组的含义。因此，应该把New York作为一个单独的词组来生成其词向量。<br><strong>对高频词抽样</strong><br>对于高频词，如 the ，按上面的处理方式会有两个问题：</p>
<ul>
<li><ol>
<li>当我们得到成对的单词训练样本时，（dog， the）这样的样本并不会提供更多关于dog的语义信息，因为the 在每个单词的上下文几乎都会出现</li>
</ol>
</li>
<li><ol start="2">
<li>由于the 这样的高频词出现的概率很大，因此为们将会有大量的（the ， 。。。）这样的训练样本，而这些样本的数量远远超过我们学习the这个单词所需的训练样本数。</li>
</ol>
</li>
</ul>
<p>如果直接删除掉这些高频词，会有两个问题:</p>
<ul>
<li>1.删除后，the这个单词永远也不会出现在我们的上下文窗口</li>
<li>2.训练样本会减少</li>
</ul>
<p>所以word2vec 采用抽样的方式来解决这种高频词问题。他的基本思想是：对于我们在训练原始文本中遇到的每一个单词，他们都有一定概率被我们从文本中删除掉，而这个被删除的概率与单词的频率有关。<br><img src="/2019/10/13/skipgram/del.png" alt><br>wi 是一个单词，$Z(w_i)$是这个单词在所有预料中出现的频次。$P(w_i)$是被保留的概率。</p>
<ul>
<li>当$Z(w_i) &lt;= 0.0026$时,$P(w_i) = 1.0$。当单词在语料中出现的频率小于0.0026时，它是100%被保留的，这意味着只有那些在语料中出现频率超过0.26%的单词才会被采样。</li>
<li>当$Z(w_i) = 0.00746$ 时，$P(w_i) = 0.5$，意味着这一部分的单词有50%的概率被保留。</li>
<li>当$Z(w_i) = 1.0$ 时，$P(w_i) = 0.033$，意味着这部分单词以3.3%的概率被保留</li>
</ul>
<p><strong>负采样</strong><br>训练一个神经网络意味着要输入训练样本并且不断的调整神经元的权重，不断提高对目标的准确预测。而vocabulary的大小决定了skip-gram神经网络将拥有大规模的权重矩阵，所有的这些权重需要通过我们数以亿计的样本来训练调整，非常消耗计算资源，并且实际中会非常慢。<br>负采样解决了这个问题，不同于原本每个训练样本更新所有权重，负采样每次让一个训练样本仅仅更新一部分权重，减小计算量。<br>对于训练样本（fox，quick），都是经过one-hot编码的，当vocabulary的大小为10000时，我们期望输出对应的quick单词的那个神经元的输出是1，其余9999个都是0，这9999个输出为0的神经元所对应的单词称为negative word<br>隐层-输出层拥有$300 *10000$的权重，而负采样时，我们仅仅更新quick 和我们选择的其他5个negative word的结点对应的权重，共6个神经元，$300* 6 = 1800$ 个权重，相当于只计算了0.06%的权重，计算效率大大提高。<br><img src="/2019/10/13/skipgram/sample.png" alt><br>其中f(wi)代表每个单词出现的频次，p(wi)代表被选中的概率。<br>负采样的C语言实现非常的有趣。unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，由公式$P(w_i) * table_size$，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。<br>有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于内蒙古乌兰布统</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
        <tag>skipgram</tag>
      </tags>
  </entry>
  <entry>
    <title>Xgboost原理</title>
    <url>/2019/10/14/xgb/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#xu-lun">绪论</a></li>
<li><a href="#suan-fa-yuan-li">算法原理</a><ul>
<li><a href="#xue-xi-mu-biao">学习目标</a><ul>
<li><a href="#jie-dian-hua-fen">节点划分</a></li>
<li><a href="#suo-jian-yu-lie-cai-yang">缩减与列采样</a></li>
<li><a href="#xun-zhao-zui-jia-fen-ge-dian-suan-fa">寻找最佳分割点算法</a></li>
<li><a href="#dai-quan-chong-de-fen-wei-shu-lue-tu-weighted-quantile-sketch-suan-fa">带权重的分位数略图（weighted quantile sketch）算法</a></li>
<li><a href="#xi-shu-zi-gua-ying-fen-ge-ce-lue">稀疏自适应分割策略</a></li>
<li><a href="#xgboost-de-you-que-dian">XGBoost的优缺点</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="xu-lun">绪论</span><a href="#xu-lun" class="header-anchor"></a></h1><p>在实际应用的机器学习方法里，<code>GradientTree Boosting （GBDT）</code>是一个在很多应用里都很出彩的技术。XGBoost是一套提升树可扩展的机器学习系统。2015年Kaggle发布的29个获胜方法里有17个用了XGBoost。在这些方案里，有8个仅用了XGBoost，另外的大多数用它结合了神经网络。对比来看，第二流行的方法，深度神经网络，只被用了11次。这个系统的成功性也被KDDCup2015所见证了，前十的队伍都用了XGBoost。此外，据胜出的队伍说，很少有别的集成学习方法效果能超过调好参的XGBoost。<br>主要创新点：</p>
<ul>
<li>设计和构建高度可扩展的端到端提升树系统。 </li>
<li>提出了一个理论上合理的加权分位数略图（weighted  quantile sketch ）来计算候选集。 </li>
<li>引入了一种新颖的稀疏感知算法用于并行树学习。 令缺失值有默认方向。</li>
<li>提出了一个有效的用于核外树形学习的缓存感知块结构。 用缓存加速寻找排序后被打乱的索引的列数据的过程。</li>
</ul>
<h1><span id="suan-fa-yuan-li">算法原理</span><a href="#suan-fa-yuan-li" class="header-anchor"></a></h1><h2><span id="xue-xi-mu-biao">学习目标</span><a href="#xue-xi-mu-biao" class="header-anchor"></a></h2><p>首先来看下我们是如何预测的：<br>XGBoost是一个树集成模型，他将K（树的个数）个树的结果进行求和，作为最终的预测值。即：<br><img src="/2019/10/14/xgb/target.png" alt><br>假设给定的样本集有n个样本，m个特征，则<br><img src="/2019/10/14/xgb/feature.png" alt><br>其中 xi 表示第i个样本，yi 表示第i个类别标签，回归树（CART树）的空间F为<br><img src="/2019/10/14/xgb/F.png" alt><br>其中q代表每棵树的结构，他将样本映射到对应的叶节点；T是对应树的叶节点个数；f(x)对应树的结构q和叶节点权重w。所以XGBoost的预测值是每棵树对应的叶节点的值的和。<br>我们的目标是学习这k个树，所以我们最小化下面这个带正则项的目标函数：<br><img src="/2019/10/14/xgb/target_n.png" alt><br><img src="/2019/10/14/xgb/text.png" alt><br>上式的第一项是损失误差，如MSE和logistic等，第二项是正则项，控制树的复杂度，防止过拟合。<br>式子2中目标函数的优化参数是模型（functions），不能使用传统的优化方法在欧氏空间优化，但是模型在训练时，是一种加法的方式（additive manner），所以在第t轮，我们将f（t）加入模型，最小化下面的目标函数：<br><img src="/2019/10/14/xgb/min.png" alt><br>训练时，新的一轮加入一个新的f函数，来最大化的降低目标函数，在第t轮，我们的目标函数为 ：<br><img src="/2019/10/14/xgb/lt.png" alt><br>接下来我们将目标函数进行泰勒展开，取前三项，移除高阶小无穷小项，最后我们的目标函数转化为：<br><img src="/2019/10/14/xgb/lt_merge.png" alt><br>其中：<br><img src="/2019/10/14/xgb/gi.png" alt><br>接下来我们求解这个目标函数<br><img src="/2019/10/14/xgb/scratch.png" alt></p>
<p>最终我们将关于树模型的迭代转化为关于树的叶子节点的迭代，并求出最优的叶节点分数。将叶节点的最优值带入目标函数，最终目标函数的形式为：<br><img src="/2019/10/14/xgb/ltq.png" alt><br>上式可以作为得分函数用来测量树结构q的质量，他类似与决策树的不纯度得分，只是他通过更广泛的目标函数得到<br><img src="/2019/10/14/xgb/arc.png" alt><br>通过上式我们可以看到，当树结构确定时，树的结构得分只与其一阶倒数和二阶倒数有关，得分越小，说明结构越好。</p>
<h3><span id="jie-dian-hua-fen">节点划分</span><a href="#jie-dian-hua-fen" class="header-anchor"></a></h3><p>而通常情况下，我们无法枚举所有可能的树结构然后选取最优的，所以我们选择用一种贪婪算法来代替：我们从单个叶节点开始，迭代分裂来给树添加节点。节点切分后的损失函数：<br><img src="/2019/10/14/xgb/split.png" alt><br>上式用来评估切分后的损失函数，我们的目标是寻找一个特征及对应的值，使得切分后的loss reduction最大。γ除了控制树的复杂度，另一个作用是作为阈值，只有当分裂后的增益大于γ时，才选择分裂，起到了预剪枝的作用。</p>
<h3><span id="suo-jian-yu-lie-cai-yang">缩减与列采样</span><a href="#suo-jian-yu-lie-cai-yang" class="header-anchor"></a></h3><p>除了在目标函数中引入正则项，为了防止过拟合，XGBoost还引入了缩减(shrinkage)和列抽样（column subsampling），通过在每一步的boosting中引入缩减系数，降低每个树和叶子对结果的影响；列采样是借鉴随机森林中的思想，根据反馈，列采样有时甚至比行抽样效果更好，同时，通过列采样能加速计算。</p>
<h3><span id="xun-zhao-zui-jia-fen-ge-dian-suan-fa">寻找最佳分割点算法</span><a href="#xun-zhao-zui-jia-fen-ge-dian-suan-fa" class="header-anchor"></a></h3><p>树模型学习的一个关键问题是如何寻找最优分割点。第一种方法称为基本精确贪心算法（exact greedy algorithm）：枚举所有特征的所有可能划分，寻找最优分割点。该算法要求为连续特征枚举所有可能的切分，这个对计算机要求很高，为了有效做到这一点，XGBoost首先对特征进行排序，然后顺序访问数据，累计loss reduction中的梯度统计量（式6）。<br><img src="/2019/10/14/xgb/max.png" alt><br>上述方法是一个非常精确的分割点算法，但是当数据无法完全加载进内存或分布式的情况下，该算法就不是特别有效了。为了支持这两种场景，提出了一种近似算法：根据特征分布的百分位数，提出n个候选切分点，然后将样本映射到对应的两个相邻的切分点组成的桶中，聚会统计值，通过聚会后的统计值及推荐分割点，计算最佳分割点。该算法有两种形式：全局近似和局部近似，其差别是全局近似是在生成一棵树之前，对各个特征计算其分位点并划分样本；局部近似是在每个节点进行分裂时采用近似算法。近似算法的流程：<br><img src="/2019/10/14/xgb/appr.png" alt></p>
<h3><span id="dai-quan-chong-de-fen-wei-shu-lue-tu-weighted-quantile-sketch-suan-fa">带权重的分位数略图（weighted quantile sketch）算法</span><a href="#dai-quan-chong-de-fen-wei-shu-lue-tu-weighted-quantile-sketch-suan-fa" class="header-anchor"></a></h3><p>在近似算法中重要的一步是寻找候选分割点，通常情况下，特征的百分位数使数据均匀的分布在数据上。现在我们定义一个数据集Dk = {(x1k, h1), (x2k, h2) … }代表样本的第k个特征及其对应的二阶梯度，现在我们定义一个函数rk：<br><img src="/2019/10/14/xgb/rk.png" alt><br>上式代表特征k小于特征z的样本比例，我们的目标是寻找候选分割点{sk1, sk2,…}，使它满足：<br><img src="/2019/10/14/xgb/rksk.png" alt><br>其中e是候选因子，即切分的百分位数，所以最后有大约1/e个候选分割点。那为什么可以用二阶倒数h来代替权重呢？我们将目标函数变形为<br><img src="/2019/10/14/xgb/new_target.png" alt><br>上式可以看成是label是gi/hi，权重是hi的平方损失，这对于大数据下寻找划分点非常重要。在以往的分位法中，没有考虑权值，许多存在的近似方法中，或者通过排序或者通过启发式方法（没有理论保证）划分。文章的贡献是提供了理论保证的分布式加权分位法。<br>为什么要对目标函数进行类似的变形？思考一下我们划分分割点的目标是什么？是希望均匀的划分loss，而不同样本对loss的权重不同，不考虑权重直接按样本划分时，loss的分布是不均匀的，对应的分位点就会有偏差。<br>PS：文章中的近似变形感觉不太近似，更近似的变形可能是<img src="/2019/10/14/xgb/trans.png" alt><br>即label是-gi/hi的带权重平方损失。不知道文章内为啥是另一种形式</p>
<h3><span id="xi-shu-zi-gua-ying-fen-ge-ce-lue">稀疏自适应分割策略</span><a href="#xi-shu-zi-gua-ying-fen-ge-ce-lue" class="header-anchor"></a></h3><p>实际情况下避免不了数据稀疏，产生数据稀疏的原因主要有三个：1，数据缺失，2，统计上为0，3，one-hot编码。而适应稀疏数据非常重要。XGBoost提出的是在计算分割后的分数时，遇到缺失值，分别将缺失值带入左右两个分割节点，然后取最大值的方向为其默认方向。<br><img src="/2019/10/14/xgb/gain.png" alt><br>以上就是XGBoost所涉及的算法原理。</p>
<h3><span id="xgboost-de-you-que-dian">XGBoost的优缺点</span><a href="#xgboost-de-you-que-dian" class="header-anchor"></a></h3><p><strong>与GBDT对比</strong></p>
<ul>
<li>1.GBDT的基分类器只支持CART树，而XGBoost支持线性分类器，此时相当于带有L1和L2正则项的逻辑回归（分类问题）和线性回归（回归问题）。</li>
<li>2.GBDT在优化时只使用了一阶倒数，而XGBoost对目标函数进行二阶泰勒展开，此外，XGBoost支持自定义损失函数，只要损失函数二阶可导</li>
<li>3.XGBoost借鉴随机森林算法，支持列抽样和行抽样，这样即能降低过拟合风险，又能降低计算。</li>
<li>4.XGBoost在目标函数中引入了正则项，正则项包括叶节点的个数及叶节点的输出值的L2范数。通过约束树结构，降低模型方差，防止过拟合。</li>
<li>5.XGBoost对缺失值不敏感，能自动学习其分裂方向</li>
<li>6.XGBoost在每一步中引入缩减因子，降低单颗树对结果的影响，让后续模型有更大的优化空间，进一步防止过拟合。</li>
<li>7.XGBoost在训练之前，对数据预先进行排序并保存为block，后续迭代中重复使用，减少计算，同时在计算分割点时，可以并行计算</li>
<li>8.可并行的近似直方图算法，树结点在进行分裂时，需要计算每个节点的增益，若数据量较大，对所有节点的特征进行排序，遍历的得到最优分割点，这种贪心法异常耗时，这时引进近似直方图算法，用于生成高效的分割点，即用分裂后的某种值减去分裂前的某种值，获得增益，为了限制树的增长，引入阈值，当增益大于阈值时，进行分裂；</li>
</ul>
<p><strong>与LightGBM对比</strong></p>
<ul>
<li>1.XGBoost采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，贪心法耗时，LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低，但是不能找到最精确的数据分割点。同时，不精确的分割点可以认为是降低过拟合的一种手段。</li>
<li>2.LightGBM借鉴Adaboost的思想，对样本基于梯度采样，然后计算增益，降低了计算</li>
<li>3.LightGBM对列进行合并，降低了计算</li>
<li>4.XGBoost采样level-wise策略进行决策树的生成，同时分裂同一层的节点，采用多线程优化，不容易过拟合，但有些节点分裂增益非常小，没必要进行分割，这就带来了一些不必要的计算；LightGBM采样leaf-wise策略进行树的生成，每次都选择在当前叶子节点中增益最大的节点进行分裂，如此迭代，但是这样容易产生深度很深的树，产生过拟合，所以增加了最大深度的限制，来保证高效的同时防止过拟合。</li>
</ul>
<hr>
<p>参考：<a href="https://arxiv.org/pdf/1603.02754.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.02754.pdf</a><br><a href="https://www.zhihu.com/question/41354392/answer/98658997" target="_blank" rel="noopener">https://www.zhihu.com/question/41354392/answer/98658997</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>风格迁移模型效果图</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Xgboost</tag>
        <tag>Boosting</tag>
      </tags>
  </entry>
  <entry>
    <title>文档查重之SimHash算法</title>
    <url>/2019/10/26/simhash/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#simhash">SimHash</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="simhash">SimHash</span><a href="#simhash" class="header-anchor"></a></h1><p>不同网站间相互转载内容的情况非常常见，即使同一网站，不同的URL地址也可能对应相同内容，只是以不同的形式显示出来（不同的UI），而我们在爬取大量内容时，除了靠URL去重外，还需按文档内容排重<br>指纹可以判断人的身份，比如侦探把从犯罪现场采集的指纹与指纹库中的指纹做个对比，就能确定犯罪嫌疑人的身份。类似的，我们用一个文档的语义指纹来代表文档的语义，如采用一个二进制数组来代表。从而判断文档之间的相似性转化为判断两个语义指纹之间的相似性。<br>SimHash是Google在2007年发表的论文《Detecting Near-Duplicates for Web Crawling 》中提到的一种指纹生成算法或者叫指纹提取算法，被Google广泛应用在亿级的网页去重的Job中，作为locality sensitive hash（局部敏感哈希）的一种，其主要思想是降维，即将一篇若干数量的文本内容用一个长度较短的数组来表示，而这个数组与这篇文档的主要的特征所对应。如在没有犯罪嫌疑人的身份证和指纹时，一个人的特征有无数多个，而我们可以通过调查犯罪嫌疑人的姓名，性别，出生日期，身高，体重，当天穿的衣服，外貌等一些主要特征来甄别嫌疑人的身份。simhash也是将复杂的特征，降维来简化。<br>SimHash计算过程：<br><img src="/2019/10/26/simhash/sim.png" alt="simhash计算流程"></p>
<ul>
<li>1.对文档提取特征及特征对应的权重</li>
<li>2.对特征进行hash，生成对应的hash值</li>
<li>3.hash值加权：对特征hash值的每一位做循环处理：如果该位值为1，则用weight代替，否则，用-weight代替</li>
<li>4.求和：将特征hash加权后的结果，按位求和，然后将结果按位二值化：大于0则为1，否则为0，即得到最后的SimHash值。</li>
</ul>
<p>SimHash的计算依据是要比较的对象的特征，对于结构化的记录，可以按列提取特征；对于非结构化的文档，特征可以用全文提取topk关键词、标题、最长的几句话、每段的首句、尾句等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> analyse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimHash</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, content, topK=<span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.topK = topK</span><br><span class="line"></span><br><span class="line">        self.simhash = self.getSimHash(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getSimHash</span><span class="params">(self, content)</span>:</span></span><br><span class="line"></span><br><span class="line">        seg = jieba.cut(content)</span><br><span class="line"></span><br><span class="line"><span class="comment">#        jieba.analyse.set_stop_words('stopword.txt')</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#topk words and it's tf/idf</span></span><br><span class="line"></span><br><span class="line">        keyWords = jieba.analyse.extract_tags(</span><br><span class="line"></span><br><span class="line">            <span class="string">'|'</span>.join(seg), topK=self.topK, withWeight=<span class="literal">True</span>, allowPOS=())</span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(keyWords)</span></span><br><span class="line"></span><br><span class="line">        word_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> feature, weight <span class="keyword">in</span> keyWords:</span><br><span class="line"></span><br><span class="line">            feature = self.string_hash(feature)</span><br><span class="line"></span><br><span class="line">            temp = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> feature:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> i == <span class="string">'1'</span>:</span><br><span class="line"></span><br><span class="line">                    temp.append(weight)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">                    temp.append(-weight)</span><br><span class="line"></span><br><span class="line">            word_list.append(temp)</span><br><span class="line"></span><br><span class="line">        hashSum = np.sum(np.array(word_list), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        simhash = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> code <span class="keyword">in</span> hashSum:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> code &gt; <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">                simhash += <span class="string">'1'</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">                simhash += <span class="string">'0'</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> simhash</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">string_hash</span><span class="params">(self,source)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> source == <span class="string">""</span>:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">            x = ord(source[<span class="number">0</span>]) &lt;&lt; <span class="number">7</span></span><br><span class="line"></span><br><span class="line">            m = <span class="number">1000003</span></span><br><span class="line"></span><br><span class="line">            mask = <span class="number">2</span> ** <span class="number">128</span> - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> source:</span><br><span class="line"></span><br><span class="line">                x = ((x * m) ^ ord(c)) &amp; mask</span><br><span class="line"></span><br><span class="line">            x ^= len(source)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> x == <span class="number">-1</span>:</span><br><span class="line"></span><br><span class="line">                x = <span class="number">-2</span></span><br><span class="line"></span><br><span class="line">            x = bin(x).replace(<span class="string">'0b'</span>, <span class="string">''</span>).zfill(<span class="number">64</span>)[<span class="number">-64</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#            print(source,x)</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><strong>海明距离</strong></p>
<p>得到文档的SimHash值后，我们还需要判断两个文档是否相似。对相同长度的数字序列，我们采用海明距离来衡量其相似性。海明距离是指两个码字对应比特位（数字序列对应位置）不同的比特位个数。如1011101和1001001的第三位和第五位有差别，所以对应的海明距离为2。<br>计算两个数的海明距离时，我们先把两个数按位异或（XOR），然后计算结果中1的个数，结果就是海明距离。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hamDis</span><span class="params">(l1, l2)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#异或</span></span><br><span class="line"></span><br><span class="line">    lxor = int(l1,<span class="number">2</span>) ^ int(l2,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    c = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#计算异或结果1的个数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(lxor):</span><br><span class="line"></span><br><span class="line">        lxor &amp;= lxor<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        c += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>
<p>把文档转化成SimHash后，文档的排重就变成了海明距离计算问题：给出一个f位的语义指纹集合F和一个语义指纹fg，找出F中是否存在与fg只有k位差异的语义指纹。<br>当k值很小而要找的语义指纹集合中的元素不多时，可以用逐次探查法：先把所有和当前指纹差k位的指纹扩展出来，然后用折半查找法在排好序的指纹集合中查找；<br>如果是面对的是海量的数据，且动态的增加，逐次探查法的效率将越来越慢。当k值较小，如不大于3时，我们使用一种快速方法。首先，我们将64位分成4份，当k为3时，则有一份中两者相等。<br><img src="/2019/10/26/simhash/match.png" alt><br>所以我们在存储时，将数据扩展为4份，每份以其中16位为k，剩余的部分为v，查找时精确匹配这16位。<br><img src="/2019/10/26/simhash/search.png" alt><br>除此之外，对于一个已经排序的容量为$2^d$的f位指纹集合，由于指纹集合中有很多的位组合存在，所以高d位只有少量重复存在，所以在搜索时，也可以找出高d位与当前指纹相同的集合f‘，缩小查找份范围。</p>
<p>Simhash算法对长文本500字+比较适用，短文本可能偏差较大，最后使用海明距离，求相似，在google的论文给出的数据中，64位的签名，在海明距离为3的情况下，可认为两篇文档是相似的或者是重复的，当然这个值只是参考值，针对自己的应用可以自测取值。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于河南老家冬雪</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Words Distance</tag>
      </tags>
  </entry>
  <entry>
    <title>天猫双十一销售额相关思考</title>
    <url>/2019/11/24/tm-1111/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#qi-yin">起因</a></li>
<li><a href="#shi-yan">实验</a></li>
<li><a href="#shi-yan-xiao-jie">实验小结</a></li>
<li><a href="#shi-me-shi-guo-ni-he">什么是过拟合</a></li>
<li><a href="#tong-su-de-jie-shi">通俗的解释</a></li>
<li><a href="#zhi-shi-zu-zhou">知识诅咒</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="qi-yin">起因</span><a href="#qi-yin" class="header-anchor"></a></h1><p>最近双十一，各大电商平台造势，宣传自己当天平台销售额，而有个网友爆料天猫销售额造假，因为自己在几个月前就已经成功预测了今年双十一的销售额，并给出了自己的模型参数（公式）。<br><a href="/2019/11/24/tm-1111/wb.jpeg">2019年11月12日网民认为尹立庆神推算的微博</a></p>
<h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>首先，我们按照楼主的思路做一个的实验。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tm_2009_2019 =  [<span class="number">0.50</span>, <span class="number">9.36</span>, <span class="number">52.00</span>, <span class="number">191.00</span>, <span class="number">350.00</span>, <span class="number">571.00</span>, <span class="number">912.00</span>, <span class="number">1207.00</span>, <span class="number">1682.69</span>, <span class="number">2135.00</span>]</span><br><span class="line">year = list(range(<span class="number">2009</span>, <span class="number">2019</span>))</span><br><span class="line">year = list(map(<span class="keyword">lambda</span> x: x<span class="number">-2009</span>, year))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sse</span><span class="params">(y_pre, y_true)</span>:</span></span><br><span class="line">    se = (y_pre - y_true) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> se.sum()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssr</span><span class="params">(y_pre, y_true)</span>:</span></span><br><span class="line">    sr = (y_pre - y_true.mean()) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> sr.sum()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">R</span><span class="params">(y_pre, y_true)</span>:</span></span><br><span class="line">    se = sse(y_pre, y_true) </span><br><span class="line">    sr = ssr(y_pre, y_true)</span><br><span class="line">    <span class="keyword">return</span> sr/(se+sr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(x, y, degree=<span class="number">3</span>)</span>:</span></span><br><span class="line">    x = np.array(x).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    y = np.array(y).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    pf = PolynomialFeatures(degree=degree)</span><br><span class="line">    X = pf.fit_transform(x)</span><br><span class="line">    linear_reg = LinearRegression()</span><br><span class="line">    linear_reg.fit(X, y)</span><br><span class="line">    y_pre = linear_reg.predict(X)</span><br><span class="line">    print(<span class="string">'R is: '</span>, R(y_pre, y))</span><br><span class="line">    <span class="keyword">return</span> linear_reg, pf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, pf, test_x)</span>:</span></span><br><span class="line">    test_X = pf.transform(np.array(test_x).reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> model.predict(test_X)</span><br></pre></td></tr></table></figure></p>
<p><img src="/2019/11/24/tm-1111/r0.png" alt="模型结果"><br>可以看到，此时的拟合优度R高达0.999，而预测今年的值为2689，确实也与今年的实际值2684基本吻合，所以网友认定自己成功的发现了天猫双十一的销售额”模型”。<br>接下来，我们做几个不一样的实验来看看其结果。<br><strong>实验1</strong><br>这次我们将数据修改一下：将第三年和倒数第三年的数据都减少10%，其他参数不变，看看模型效果。<br><img src="/2019/11/24/tm-1111/lower_9.png" alt="修改数据后模型结果"><br>拟合优度0.996，预测值为2642，与今年实际值也基本吻合。</p>
<p><strong>实验2</strong><br>这次我们不再使用全部数据，而只用2013-2018这六年的数据，其他不变，来看看我们的模型效果如何。<br><img src="/2019/11/24/tm-1111/r6.png" alt="六年数据模型结果"><br>拟合优度也高达0.997，而且预测值2672也与今年的实际值基本吻合。</p>
<p><strong>实验3</strong><br>这次我们不再使用三次，而换为二次线性方程来拟合，仍然使用原始的十年数据。<br><img src="/2019/11/24/tm-1111/degree_2.png" alt="修改模型参数后结果"><br>拟合优度0.999，预测值为2675，与今年的实际值也基本吻合。</p>
<h1><span id="shi-yan-xiao-jie">实验小结</span><a href="#shi-yan-xiao-jie" class="header-anchor"></a></h1><p>实验1与实验2说明，对样本做一定处理后，对模型最终的拟合影响不大，原因是在求解模型时，通常我们使用的loss是欧式距离（最小二乘估计），整体的<br>loss是对所有样本拟合loss的和，所以模型在拟合时会更”关注”值大的样本，而前几年的值与后几年相比差距一个数量级，所以不使用值很小的样本对模型<br>影响不大。而实际建模时，对于样本范围跨度很大的特征，通常我们都需要平滑而不是直接使用原始值。<br>实验3我们使用了一个参数更小的模型，$f(x) = A + Bx + Cx^2$, 而原始模型使用三次拟合，$F(x) = A + bx + Cx^2 + Dx^3$, 两个模型明显是<br>不同的，但是却都能拟合数据，这是因为这些模型都过拟合了，也就是对于一批离散点，总是能找到一个函数F，可以非常好的拟合他，这也是冯诺伊曼的那<br>个笑话：四个参数画大象，五个参数鼻子晃。</p>
<h1><span id="shi-me-shi-guo-ni-he">什么是过拟合</span><a href="#shi-me-shi-guo-ni-he" class="header-anchor"></a></h1><p>在这个问题上，一些网友说，这个模型肯定是有效的而不是过拟合，因为他”完美预测”了今年的真实值，模型如果只是单纯的在之前的数据上很好的拟合，而不能成功预测今年的真实值，这才是过拟合，现在模型成功预测了今年的值，所以现在的模型是有效的，不存在过拟合。<br>为了说明这个问题，我们来简单讨论一下，什么是过拟合，是不是成功预测了就不存在过拟合。<br>首先，我们来简单说明一下，一个模型的误差分为两部分，一部分是因为模型对训练数据拟合的不好带来的误差，这部分我们称为偏差；而由于抽样过程中，抽出的训练样本与整体样本分布不同引起的误差，我们称为方差。而当我们抽样时是有偏的，即抽样与整体样本分布不同，而我们的模型又对训练数据拟合的非常好，此时的模型我们就称之为过拟合，即他虽然对样本拟合的很好，但是由于样本与整体分布不同，导致模型在泛化时性能并不好，通常比在训练集上要差很多，因为毕竟模型拟合的是一个与整体不同分布的数据集，拟合的越好，泛化越差。<br>那当模型在新的样本上预测对时，是不是就不能说他过拟合呢？其实也不尽然，本质上过拟合只与你的整个过程有关，至于最终的模型在新样本上预测的准不准，并不能说明这个模型是否过拟合，毕竟瞎猫还能碰上死耗子，模型拟合的数据集虽然与整体分布不同，但是毕竟也是整体数据的一部分，预测准<br>一部分数据也正常，毕竟我们说过拟合的泛化性能低，但不是说他泛化低到全错。<br>所以对应天猫的这个案例，总的来说天猫的销售额是一个复杂的业务场景最终带来的结果，而仅仅用最终的销售额来进行模型，忽略各种业务策略的影响因素，显然与业务的真实分布相去甚远，不然也不用投入这么多人力物力，直接等着新的一年到来就好了，因为你不管怎么调整，最终的结果都是”模型”定好的。</p>
<h1><span id="tong-su-de-jie-shi">通俗的解释</span><a href="#tong-su-de-jie-shi" class="header-anchor"></a></h1><p>当我用上面这些论述来解释时，同事告诉我：”我不懂数学不懂机器学习，我就知道他的模型预测对了，都预测对了，你怎么能说他不准呢，还过拟合，过拟合不是预测不对才叫过拟合吗？” 这时我会换一个场景，”假如现在有个人，通过之前的十期双色球，预测对了这期对双色球，那你要不要用他的模型，重金投入买下一期的双色球呢？”</p>
<h1><span id="zhi-shi-zu-zhou">知识诅咒</span><a href="#zhi-shi-zu-zhou" class="header-anchor"></a></h1><p>最近学到了一个新的概念：知识的诅咒，其含义是当你知道了一件事后，你就无法想象自己是不知道这件事的。而造成这种现象的根本原因是信息的不对等。在我最初给人解释天猫销售额事件时，对面总是听不太懂我在说什么，我也搞不清楚为什么我说的这么简单直白，他会听不懂，当看到知识诅咒这个概念后，这个问题就有了答案了。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于故宫</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title>带约束的领域词挖掘</title>
    <url>/2019/11/28/domain-words/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#bei-jing">背景</a></li>
<li><a href="#fang-an">方案</a><ul>
<li><a href="#shi-yan">实验</a></li>
<li><a href="#shi-yan-jie-guo">实验结果</a></li>
</ul>
</li>
<li><a href="#zong-jie">总结</a><ul>
<li><a href="#you-dian">优点</a></li>
<li><a href="#que-dian">缺点</a></li>
<li><a href="#fen-xiang">分享</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="bei-jing">背景</span><a href="#bei-jing" class="header-anchor"></a></h1><p>分享一个最近做的项目方案，背景是当前的内容标签是最细粒度到一级大类，而产品希望出一些三级小类（当前最细分类）的标签，而且这些标签应该是有别于其他三级小类的。</p>
<h1><span id="fang-an">方案</span><a href="#fang-an" class="header-anchor"></a></h1><p>我将其看作是领域词挖掘任务，只是这个任务带有一些约束，及这些领域词是个性化的，与其他小类内的领域词是不同的。<br>所以，需要做的是抽词（新词发现）+领域词挖掘+个性化判断。<br>由于之前做过新词发现，主要参考<a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="noopener">互联网时代的社会语言学：基于SNS的文本数据挖掘</a>,<br>对应的java实现<a href="https://github.com/xv44586/dict_build" target="_blank" rel="noopener">dict_build</a>。<br>除了无监督抽词外，matrix67的博文后面半部分也很有意思：通过对两份语料进行抽词，然后对词进行词频统计，通过对比词在两份语料内的词频差异，来发现“热词”。<br>回到现在的任务，个性化的词，必定在当前小类内相对于其他小类更“热”，我们可以将当前小类内的词与非当前小类进行对比，就能得到当前小类的“热词”，<br>有了热词，我们在判断这些“热词”是不是领域词，即可得到想要的结果。而实际上，由于一级大类之间的词已经有很大差异，所以，我们在抽当前小类“热词”时，不必对全量非当前小类语料进行统计，只需要对其一级大类内非当前小类语料进行统计对比，即可得到“热词”。<br>而对于领域词判断，由于我们已经有了一级大类的标签，我们可以利用这部分信息来进行领域词判断。<br>所以，最终的方案是先抽词，然后对词进行领域词判断，对词进行是否是“热词”判断，对领域词与热词进行求交运算，得到个性化的领域词。</p>
<h2><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h2><p>实际处理时，由于我们挖掘的“词”其实更像是一个短语，其长度也比词稍长，而“词”的组合上限是${CharCount}^{WordLength}$，而字符大概有12000+， 词平均长度假如是7，这个量也是相当大的。<br>所以抽词对内存要求很高，而且非常耗时（10h+).在思考如何优化抽词程序时，看到了<a href="https://kexue.fm/archives/6540" target="_blank" rel="noopener">分享一次专业领域词汇的无监督挖掘</a>，作者的思路与我的思路一样，不过不同的是，<br>作者在抽词时，并没有沿用matrix67的博文中根据凝合度与信息熵来进行是否成词判断，而且用信息熵是否低于某个阈值，来判断是否切开，即为两个词，这个思路还是非常巧妙的，这样，不光计算量减小了很多，而且不在需要设置超参数中ngram的值，<br>挖掘出的词也更符号“语义完整性”；而领域词判断，作者采用的是通过语料训练词向量，再由种子词来扩展领域词，也不失为一个好思路。</p>
<h2><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h2><p>从结果上看，效果还是很不错的。<br><img src="/2019/11/28/domain-words/words.png" alt="热词结果"><br>热词结果<br><img src="/2019/11/28/domain-words/keywords.png" alt="领域词结果"><br>领域词结果<br><img src="/2019/11/28/domain-words/result.png" alt="最终结果"><br>最终挖掘的个性化领域词</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><h2><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h2><p>挖掘领域词任务中，首先是如何对”词”进行判断，然后才是”领域词”判断，所以影响最终结果好坏的主要因素也是”词”划分的质量与”领域词”判断的准确性。<br>上述方案中，通过用信息熵的最小阈值来判断相邻两个字是否属于同一个词内来进行分词，优点是速度快，能切分出高频的短语；<br>通过种子词在词向量空间来一层一层挖掘领域词，主要思路是近邻的近邻，虽然与你”直线距离”稍远，但通过你的近邻点跳转后”距离”近，这样来扩大召回，所以通过少量种子点即可召回出大量”近邻”词。</p>
<h2><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h2><p>缺点也很明显，首先分词时，容易将常用搭配错误切分，如html容易切出 *h/tml,的xxx容易切分为一个词，所以需要写一些规则过滤掉明显错误切分的词；种子词在召回时，由于词向量用的是word2vec这种静态词向量，近邻容易跳出当前domain，如用”华为”做种子词，直接聚类出”小米”/“苹果”，而苹果的近邻就可能跳出手机品牌，<br>转向水果，如”西瓜”/“桃子”，所以需要控制扩散的层次深度。</p>
<h2><span id="fen-xiang">分享</span><a href="#fen-xiang" class="header-anchor"></a></h2><p>机器学习中最重要的一个思路就是寻找差异，找到合理的差异往往是解决问题的关键。本次分享了一个通过对比差异来进行领域词挖掘的例子，希望给做类似任务的同学一些思路。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于奥森</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>新词发现</tag>
        <tag>领域词挖掘</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title>有趣的概率统计题</title>
    <url>/2019/11/30/statistics/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#fen-xiang-yi-ge-you-qu-de-gai-lu-ti">分享一个有趣的概率题</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>


<p>昨晚下雪了，开心😄！</p>
<h1><span id="fen-xiang-yi-ge-you-qu-de-gai-lu-ti">分享一个有趣的概率题</span><a href="#fen-xiang-yi-ge-you-qu-de-gai-lu-ti" class="header-anchor"></a></h1><p>一段线段上，任意取两不重合的点，将这条线段切分成三段，问，这三条线段组成三角形点概率是多少？</p>
<p>现在我们将问题转化一下，假设原始线段长度为1，即$(0，1)$表示原始线段，此时，随机在$(0，1)$范围内选两个点a，b，组成$(0，a)$，$(a，b)$，$(b，1)$三个线段，<br>此时原始问题等价于现在的三个线段组成三角形的概率。<br>现在我们来考虑一下，三条线段分别为$A$， $B$，$C$，其中$A &gt;= B &gt;= C$,则A的长度必定在$[1/3, 1)$,而要组成三角形，则需要 $B + C &gt; A$,所以 B + C 的长度为$(1/2, 1)$,<br>而对应的A的长度也就在[1/3, 1/2)内，所以，最终能构成三角形的概率即为 $A_{triangle} / A_{all} = (1/2 - 1/3) / (1 - 1/3) = 1/4$</p>
<p>看似非常复杂的问题，从最基本的数学知识就能解答，还真是有趣！ </p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>2019年的第一场雪，摄于望京地铁站</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title>分享一个有趣的游戏</title>
    <url>/2019/12/09/longton/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>今天第一次知道了一个有趣的游戏，Langton的蚂蚁，动手自己画了一个后，决定分享一下。这个游戏的有趣体现在两个方面：<br><em>1：</em> 这个游戏是个零玩家游戏，整个过程也十分的简单：有一个像围棋盘一样画满方格的画布，初始时，整个画布都是空白，<br>一只蚂蚁在画布的某一个地方，如果当前空格为白色，则将当前空格去反，然后左转90度并前进一格；如果当前空格为黑色，则将当前空格颜色去反，<br>然后右转90度并前进一格，如此往复。<br><em>2：</em> 最后的结果非常有意思，开始时，整个画布是复杂无规律的复杂图像，很难想象是这么简单的规则产生的，但当蚂蚁走了一万步以后，整个运动过程<br>进入了循环，而图像也开始变为有规律的图像。</p>
<p>开始的前一百步如上图，到一万步还有点时间，所以为直接跳过中间一万步，给出一万步后的一百步。感兴趣有耐心的可以跑下代码观察一下这个有点神奇的游戏。Have fun！</p>
<p><img src="/2019/12/09/longton/ant.gif" alt><br>    <strong>前一百步</strong></p>
<p><img src="/2019/12/09/longton/ant_r.gif" alt><br>    <strong>规律出现后的一百步</strong></p>
<p>Langlon的蚂蚁游戏代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Date    : 2019/12/9</span></span><br><span class="line"><span class="comment"># @Author  : mingming.xu</span></span><br><span class="line"><span class="comment"># @File    : test.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> animation</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> plticker</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Direction</span><span class="params">(object)</span>:</span></span><br><span class="line">    D = [<span class="string">'E'</span>, <span class="string">'N'</span>, <span class="string">'W'</span>, <span class="string">'S'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, position, direct)</span>:</span></span><br><span class="line">        self.direct = direct</span><br><span class="line">        self.position = position</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">go_one_step</span><span class="params">(self, left=True)</span>:</span></span><br><span class="line">        lr = <span class="number">1</span> <span class="keyword">if</span> left <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">        direct = self.D[(self.D.index(self.direct) + <span class="number">1</span> * lr) % len(self.D)]</span><br><span class="line">        next_position = &#123;</span><br><span class="line">            <span class="string">'E'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>], p[<span class="number">1</span>] + <span class="number">1</span> * lr],</span><br><span class="line">            <span class="string">'N'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>] - <span class="number">1</span> * lr, p[<span class="number">1</span>]],</span><br><span class="line">            <span class="string">'W'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>], p[<span class="number">1</span>] - <span class="number">1</span> * lr],</span><br><span class="line">            <span class="string">'S'</span>: <span class="keyword">lambda</span> p: [p[<span class="number">0</span>] + <span class="number">1</span> * lr, p[<span class="number">1</span>]]</span><br><span class="line">        &#125;</span><br><span class="line">        position = next_position[self.direct](self.position)</span><br><span class="line">        <span class="keyword">return</span> Direction(position=position, direct=direct)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N = <span class="number">100</span>  <span class="comment"># length of matrix</span></span><br><span class="line">lc, hc = <span class="number">1</span>, <span class="number">-1</span>  <span class="comment"># color for negative and positive</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">50</span>, <span class="number">50</span>))</span><br><span class="line">data = np.zeros((N, N)) + lc</span><br><span class="line">data[int(N/<span class="number">2</span>), int(N/<span class="number">2</span>)] *= <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># remove axis</span></span><br><span class="line">ax.get_yaxis().set_visible(<span class="literal">False</span>) <span class="comment">#不显示y轴</span></span><br><span class="line">ax.get_xaxis().set_visible(<span class="literal">False</span>) <span class="comment">#不显示x轴</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add text</span></span><br><span class="line">time_template = <span class="string">'step = %d'</span></span><br><span class="line">time_text = plt.text(<span class="number">0.5</span>, N+<span class="number">0.1</span>, <span class="string">''</span>, fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>: <span class="string">'red'</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#add grid</span></span><br><span class="line"><span class="comment"># myInterval = 1</span></span><br><span class="line"><span class="comment"># loc = plticker.MultipleLocator(base=myInterval)</span></span><br><span class="line"><span class="comment"># ax.xaxis.set_major_locator(loc)</span></span><br><span class="line"><span class="comment"># ax.yaxis.set_major_locator(loc)</span></span><br><span class="line"><span class="comment"># ax.grid(which='major', axis='both', linestyle='-')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># imshow</span></span><br><span class="line">ln = plt.imshow(data, animated=<span class="literal">True</span>, cmap=<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dct = Direction(position=[int(N / <span class="number">2</span>), int(N / <span class="number">2</span>)], direct=<span class="string">'N'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">()</span>:</span></span><br><span class="line">    ax.set_xlim(<span class="number">0</span>, N)</span><br><span class="line">    ax.set_ylim(<span class="number">0</span>, N+<span class="number">5</span>)</span><br><span class="line">    time_text.set_text(<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> ln, time_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move</span><span class="params">(dct, data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    当前为白色方格，则对当前方格取反，左转前进一格；若当前为黑色方格，取反后右转前进一格</span></span><br><span class="line"><span class="string">    :param dct:</span></span><br><span class="line"><span class="string">    :param mat:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    pos = dct.position</span><br><span class="line">    <span class="keyword">if</span> data[pos[<span class="number">0</span>], pos[<span class="number">1</span>]] &gt; <span class="number">0</span>:</span><br><span class="line">        data[pos[<span class="number">0</span>], pos[<span class="number">1</span>]] *= <span class="number">-1</span></span><br><span class="line">        dct_ = dct.go_one_step(left=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data[pos[<span class="number">0</span>], pos[<span class="number">1</span>]] *= <span class="number">-1</span></span><br><span class="line">        dct_ = dct.go_one_step(left=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dct_, data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(f)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> dct</span><br><span class="line">    <span class="keyword">global</span> data</span><br><span class="line">    dct, data = move(dct, data)</span><br><span class="line">    ln.set_data(data)</span><br><span class="line">    time_text.set_text(time_template % f)</span><br><span class="line">    <span class="keyword">return</span> ln, time_text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">()</span>:</span></span><br><span class="line">    frame = <span class="number">0</span></span><br><span class="line">    max_step = <span class="number">11000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> frame &lt; max_step:</span><br><span class="line">        frame += <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> frame</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">anim = animation.FuncAnimation(fig, update, frames=data_gen, interval=<span class="number">10</span>,</span><br><span class="line">                               init_func=init, blit=<span class="literal">True</span>, repeat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">anim.save(<span class="string">'ant.gif'</span>, writer=<span class="string">'imagemagick'</span>, fps=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>游戏的前一百步</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Game</tag>
      </tags>
  </entry>
  <entry>
    <title>Backup</title>
    <url>/2019/12/27/backup/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#pei-zhi">配置</a><ul>
<li><a href="#cha-jian">插件</a></li>
</ul>
</li>
<li><a href="#zhu-ti">主题</a></li>
<li><a href="#markdown">MarkDown</a><ul>
<li><a href="#bu-ju">布局</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id> </span><a href="#" class="header-anchor"></a></h1><p><strong>Just for me！<br>现在使用的博客虽然使用的是开源的，但是自己做了部分修改，加上一些常用语法一段时间不用后又需要重新查，所以在此记录一下当前博客常用的。 </strong> </p>
<h1><span id="pei-zhi">配置</span><a href="#pei-zhi" class="header-anchor"></a></h1><p>Hexo 部署文档： <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="noopener">http://hexo.io/docs/deployment.html</a><br>Hexo _config.xml 的配置 <a href="https://gist.github.com/btfak/18938572f5df000ebe06fbd1872e4e39" target="_blank" rel="noopener">https://gist.github.com/btfak/18938572f5df000ebe06fbd1872e4e39</a></p>
<h2><span id="cha-jian">插件</span><a href="#cha-jian" class="header-anchor"></a></h2><ul>
<li>hexo-toc  <a href="https://github.com/bubkoo/hexo-toc" target="_blank" rel="noopener">Insert a markdown TOC before posts be rendered</a><br>用来生产目录</li>
<li>hexo-renderer-marked + MathJax<br>整体顺序是先由renderer渲染，然后交给MathJax渲染Math相关，前者在遇见$ $后将escape _ 导致下标失效（_在renderer中认为是黑体，<br>所以产生这种冲突），所以修改了部分escape</li>
</ul>
<h1><span id="zhu-ti">主题</span><a href="#zhu-ti" class="header-anchor"></a></h1><ul>
<li>hexo-theme-skapp <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank" rel="noopener">https://github.com/Mrminfive/hexo-theme-skapp</a><ul>
<li>主要修改：</li>
</ul>
<ol>
<li>部分页面布局，包括footer和header</li>
<li>字体样式，包括部分元素样式，如<strong>code</strong></li>
</ol>
</li>
</ul>
<h1><span id="markdown">MarkDown</span><a href="#markdown" class="header-anchor"></a></h1><h2><span id="bu-ju">布局</span><a href="#bu-ju" class="header-anchor"></a></h2><ul>
<li><p>添加大纲<br>在正文最开始添加 </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- toc --&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>标题<br><code> # </code> ~ <code>######</code>，<code>#</code>号的个数表示几级标题，即表示一级标题到六级标题</p>
</li>
<li><p>有序列表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. **我是一级序列** </span><br><span class="line">2. **我是一级序列** </span><br><span class="line">3. **我是一级序列** </span><br><span class="line"> 1. *我是二级序列* </span><br><span class="line"> 1. *我是二级序列* </span><br><span class="line">  1. *我是二级序列*</span><br></pre></td></tr></table></figure>
</li>
</ul>
  <blockquote><ol>
<li><strong>我是一级序列</strong> </li>
<li><strong>我是一级序列</strong> </li>
<li><strong>我是一级序列</strong> <ol>
<li><em>我是二级序列</em> </li>
<li><em>我是二级序列</em> </li>
<li><em>我是二级序列</em> </li>
</ol>
</li>
</ol>
</blockquote>
<ul>
<li>无序列表</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* *列表展示  </span><br><span class="line"> * *列表展示</span><br><span class="line">  * *列表展示</span><br><span class="line">+ +列表展示</span><br><span class="line"> + +列表展示</span><br><span class="line">  + +列表展示</span><br><span class="line">- -列表展示</span><br><span class="line"> - -列表展示</span><br><span class="line">  - -列表展示</span><br></pre></td></tr></table></figure>
  <blockquote><ul>
<li>*列表展示<ul>
<li>*列表展示<ul>
<li>*列表展示</li>
</ul>
<ul>
<li>+列表展示</li>
</ul>
</li>
</ul>
<ul>
<li>+列表展示<ul>
<li>+列表展示</li>
</ul>
<ul>
<li>-列表展示</li>
</ul>
</li>
</ul>
<ul>
<li>-列表展示<ul>
<li>-列表展示</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>表格</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&amp;nbsp; | l1     | l2     </span><br><span class="line">  ----- | --- | ---- </span><br><span class="line">    w0 | $e_&#123;<span class="number">01</span>&#125;$ | $e_&#123;<span class="number">02</span>&#125;$ </span><br><span class="line">    w1 | $e_&#123;<span class="number">11</span>&#125;$ | $e_&#123;<span class="number">12</span>&#125;$ </span><br><span class="line">    w2 | $e_&#123;<span class="number">21</span>&#125;$ | $e_&#123;<span class="number">22</span>&#125;$</span><br></pre></td></tr></table></figure>
<p>表头与正文用–来分割，列之间用|来分割。注：列名不能使用空格，如需要列名为空，需要使用 &amp;nbsp；替换</p>
  <blockquote><table>
<thead>
<tr>
<th>&nbsp;</th>
<th>l1</th>
<th>l2     </th>
</tr>
</thead>
<tbody>
<tr>
<td>  w0</td>
<td>$e_{01}$</td>
<td>$e_{02}$ </td>
</tr>
<tr>
<td>  w1</td>
<td>$e_{11}$</td>
<td>$e_{12}$ </td>
</tr>
<tr>
<td>  w2</td>
<td>$e_{21}$</td>
<td>$e_{22}$ </td>
</tr>
</tbody>
</table>
</blockquote>
<ul>
<li><p>强调</p>
 **加粗** __加粗__ _斜体_  ***加粗并斜体*** ~~删除线～~ 
<blockquote><p><strong>加粗</strong> <strong>加粗</strong> _斜体_  <strong><em>加粗并斜体</em></strong> ~~删除线～~</p>
</blockquote>
</li>
<li><p>高亮</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">用&lt;code&gt; content &lt;/code&gt; 来包裹想要高亮的元素</span><br></pre></td></tr></table></figure>
  <blockquote><p>用<code> content </code> 来包裹想要高亮的元素</p>
</blockquote>
<ul>
<li><p>关闭MarkDown</p>
<blockquote><p>{% raw %} content {% endraw %}</p>
<p>这种方式将会忽略空格回车等，有些场景也会失效，此时可以用 <strong>代码块</strong> 代替</p>
</blockquote>
</li>
<li><p>引用</p>
<blockquote><p>{% blockquote 江泽民%}科技是第一生产力 {% endblockquote %} </p>
</blockquote>
<blockquote><p>科技是第一生产力</p>
<footer><strong>江泽民</strong></footer></blockquote>
</li>
<li><p>Math</p>
<blockquote><p>行内公式用 $包裹，多行时用$$包裹<br>使用的是MathJax，语法可参考博客<a href="https://blog.csdn.net/ethmery/article/details/50670297" target="_blank" rel="noopener">MathJax基本语法</a> 和官方文档(<a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference</a>)<br><code>调试参考<a href="https://www.quicklatex.com/" target="_blank" rel="noopener">quicklatex</a></code></p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$</span><br><span class="line"> states = $$\begin&#123;pmatrix&#125;</span><br><span class="line"> e_&#123;<span class="number">01</span>&#125;&amp;e_&#123;<span class="number">01</span>&#125;\\\\</span><br><span class="line"> e_&#123;<span class="number">02</span>&#125;&amp;e_&#123;<span class="number">02</span>&#125;\\\\</span><br><span class="line"> \end&#123;pmatrix&#125;$$</span><br><span class="line">$</span><br></pre></td></tr></table></figure>
  <blockquote><p>$<br>states = $$\begin{pmatrix}<br>e_{01}&amp;e_{01}\\<br>e_{02}&amp;e_{02}\\<br>\end{pmatrix}$$<br>$</p>
<p><strong>注：公式内\ 会被转义，需要用双\，尤其在矩阵中。</strong></p>
</blockquote>
<ul>
<li>代码<blockquote><p>使用三个<code>`</code>包裹，如果需要显示三个 <code>`</code>， 可以用四个。也可以用  {%codeblock%}</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">do</span><span class="params">(self, job_func, *args, **kwargs)</span>:</span></span><br><span class="line">        print(<span class="string">'hello world'</span>)  </span><br><span class="line">  ```</span><br></pre></td></tr></table></figure>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do</span><span class="params">(self, job_func, *args, **kwargs)</span>:</span></span><br><span class="line">   print(<span class="string">'hello world'</span>)  </span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<p>$$log(Z_{(0-&gt;1-&gt;2)}) = log(exp(states[0]) + exp(states[1])) \\<br>=log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{21} + t_{11}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{21}+t_{21})) \\+<br>log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{22} + t_{12}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{22}+t_{22})) \\<br>= log(exp(e_{01}+e_{11}+t_{11}+e_{21}+t_{11}) + exp(e_{02}+e_{11}+t_{21}+e_{21}+t_{11}) \\ +<br>exp(e_{01}+e_{12}+t_{12} +e_{21}+t_{21}) + exp(e_{02}+e_{12}+t_{22}+e_{21}+t_{21}) \\+<br>exp(e_{01}+e_{11}+t_{11} +e_{22}+t_{12}) + exp(e_{01}+e_{11}+t_{11}+e_{22}+t_{12}) \\+<br>exp(e_{01}+e_{12}+t_{12} +e_{22}+t_{22}) + exp(e_{01}+e_{12}+t_{12}+e_{22}+t_{21}))$$</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p> 望京SOHO夜景</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次npm环境问题</title>
    <url>/2020/01/11/npm-environment/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#qi-yin">起因</a></li>
<li><a href="#ddbug">Ddbug</a></li>
<li><a href="#jie-lun">结论</a></li>
<li><a href="#guan-yu-tou">关于头</a></li>
</ul>
<!-- tocstop -->
</div>


<h1><span id="qi-yin">起因</span><a href="#qi-yin" class="header-anchor"></a></h1><p>上周写完博客，本地预览时，突然报错，看了一下是 sharp.js 的问题，以为是个小场面，然后就开始了一周的痛苦修环境经历，哭了😭</p>
<h1><span id="ddbug">Ddbug</span><a href="#ddbug" class="header-anchor"></a></h1><ol>
<li>首先根据提示，重新安装  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rm -rf node_modules/sharp</span><br><span class="line">npm i sharp</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>此时报错：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c++: error: unrecognized command line option <span class="string">'-stdlib=libc++'</span></span><br><span class="line">make: *** [Release/.node] Error <span class="number">1</span></span><br><span class="line">gyp ERR! build error</span><br><span class="line">gyp ERR! stack Error: `make` failed <span class="keyword">with</span> exit code: <span class="number">2</span></span><br><span class="line">gyp ERR! stack     at ChildProcess.onExit (/usr/local/Cellar/node@<span class="number">8</span>/<span class="number">8.16</span><span class="number">.2</span>/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:<span class="number">262</span>:<span class="number">23</span>)</span><br><span class="line">gyp ERR! stack     at emitTwo (events.js:<span class="number">126</span>:<span class="number">13</span>)</span><br><span class="line">gyp ERR! stack     at ChildProcess.emit (events.js:<span class="number">214</span>:<span class="number">7</span>)</span><br><span class="line">gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:<span class="number">198</span>:<span class="number">12</span>)</span><br></pre></td></tr></table></figure></p>
<p>这个问题的本质是当前的包需要通过源码编译，而当前用的是gcc（macOS），而gcc不支持当前命令，之前装环境没有遇到过这个问题，可能是我最近跟新了gcc？<br>查了一下解决这个问题最简单的方式是指定c++:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CXX=clang++ npm i xxx</span><br></pre></td></tr></table></figure></p>
<p>这次确实装成功了，走起！<br>此时又报新错：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">can <span class="keyword">not</span> find sharp xxx</span><br><span class="line">rm node_modules/sharp <span class="keyword">and</span> rebuild</span><br></pre></td></tr></table></figure></p>
<p>阿嘞？装上了又找不到？why？<br>又尝试了全局安装，依然找不到，此时，有点上头，我干脆把node_modules全部删掉，重新装吧。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rm -rf node_modules</span><br><span class="line">CXX=clang++ npm i</span><br></pre></td></tr></table></figure></p>
<p>阿嘞？这次装也失败了，错误大致原因是node-gyp rebuild nodejieba失败。<br>开始以为是node-gyp的问题，后来查了一下，node-gyp是用来帮助丛源码编译的工具，所以本质上不是他的问题，还是别的问题。<br>又查了一下nodejieba, 有人说是在lunr.js中用的nodejieba在node高版本中会存在编译失败，建议用 node8.x ,python版本最好是2.7，那我上次没失败？先将node降级到node8, python切到2.7。<br>再装一次，依然失败。但是错误信息不够定位，查一下怎么看更全的日志。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CXX=clang++ npm i --verbose</span><br><span class="line">```  </span><br><span class="line">发现两条重要信息，大概是无法找到lib下某个库，可能是本级的环境出了问题，整理一下吧。</span><br><span class="line"> ```python</span><br><span class="line">brew update</span><br><span class="line">brew cleanup</span><br><span class="line">brew doctor</span><br><span class="line">```  </span><br><span class="line">环境的一堆issue解决掉（主要是link无效），然后把之前无法找到的两个lib重新装了一次（很慢）。</span><br><span class="line">此时还是同样问题，这次又查看了一下当前环境问题，两个包没link，一个是python(其实是python3，之前link的python2)， 一个是swig，后来一想，可能是swig这个工具在源码编译是缺失导致的，</span><br><span class="line">```python</span><br><span class="line">brew link swig</span><br></pre></td></tr></table></figure></p>
<p>此时在装，搞定！走起，又报新错：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">FATAL Something<span class="string">'s wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.html</span></span><br><span class="line"><span class="string">Error: spawn /Users/xuming/Project/blog/node_modules/optipng-bin/vendor/optipng ENOENT</span></span><br><span class="line"><span class="string">    at Process.ChildProcess._handle.onexit (internal/child_process.js:190:19)</span></span><br><span class="line"><span class="string">    at onErrorNT (internal/child_process.js:362:16)</span></span><br><span class="line"><span class="string">    at _combinedTickCallback (internal/process/next_tick.js:139:11)</span></span><br><span class="line"><span class="string">    at process._tickCallback (internal/process/next_tick.js:181:9)</span></span><br></pre></td></tr></table></figure></p>
<p>这是缺module，但是我是npm i，为什么还缺？此时可能只是当前node_modules的问题，为了验证本机其他环境已经ok了，新建了一个博客，验证后发现本机确实OK了。<br>额，那就缺什么装什么吧。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">npm i optipng-bin</span><br></pre></td></tr></table></figure></p>
<p>走起！一切正常！此时我的心情就好比火箭发射成功一样。折磨了一周的环境问题，终于搞定了，中间还有许多其他方向的试探，但都记不得了。- -！</p>
<h1><span id="jie-lun">结论</span><a href="#jie-lun" class="header-anchor"></a></h1><ol>
<li>npm 失败后，可以加 –verbose 参数查看详细日志，定位问题。</li>
<li>编译源码可能你还需要装xcode-select --install</li>
<li>对于 Error: spawn .../node_modules/xxx/vendor/.. ENOENT,单独安装一下对应缺失module即可。</li>
<li>node-gyp 和 libvips可能也会影响，建议重装一次</li>
<li>MacOS中，可能会gcc与clang并存，加上系统升级，可能导致相应版本不兼容问题。指定clang ：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CXX=clang++ npm i</span><br></pre></td></tr></table></figure>
6. 本机环境问题，可以通过<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">brew doctor</span><br></pre></td></tr></table></figure></li>
</ol>
<h1><span id="guan-yu-tou">关于头</span><a href="#guan-yu-tou" class="header-anchor"></a></h1><p>雪中奥森</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title>optimizer of bert</title>
    <url>/2020/08/01/optimizer-in-bert/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#zheng-ti-you-hua-fang-an">整体优化方案</a></li>
<li><a href="#adam-in-bert">Adam in bert</a><ul>
<li><a href="#weight-decay">weight decay</a><ul>
<li><a href="#weight-decay-1">weight decay</a></li>
<li><a href="#l2-regularization">L2 regularization</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#learning-rate">Learning rate</a><ul>
<li><a href="#learning-rate-decay">Learning rate decay</a></li>
<li><a href="#warmup">warmup</a></li>
</ul>
</li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>最近尝试实现了 bert ,在最后 pretraining 时发现 bert 中的优化方法比较有趣，所以记录一下自己的理解。</p>
<h1><span id="zheng-ti-you-hua-fang-an">整体优化方案</span><a href="#zheng-ti-you-hua-fang-an" class="header-anchor"></a></h1><p>bert中的优化方案可以总结为：线性分段学习率 + weight decay Adam</p>
<h1><span id="adam-in-bert">Adam in bert</span><a href="#adam-in-bert" class="header-anchor"></a></h1><p>首先简单回忆一下 Adam Optimizer：<br>整体框架：<br>$$<br>g_{t}=\bigtriangledown f(w_{t})<br>$$<br>$$<br>m_{t}=\Phi (g_{1},g_{2},…,g_{t})<br>$$<br>$$<br>v_{t}=\Psi (g_{1},g_{2},…,g_{t})<br>$$<br>$$<br>\eta =\alpha \cdot m_{t}/\sqrt{V_{t}}<br>$$<br>$$<br>\omega_{t+1}=\omega_{t}-\eta_{t}<br>$$</p>
<p>其中一阶动量 m 与二阶动量 v 的计算方式：<br>$$<br>m_{t}=\beta_{1}m_{t-1} + (1-\beta_{1})\cdot g_{t}<br>$$<br>$$<br>v_{t}=\beta_{2}v_{t-1} + (1-\beta_{2})\cdot g_{t}^{2}<br>$$<br>参数一般取值：ß1=0.9，ß2=0.999<br>而也是这个原因，初期对一阶动量与二阶动量v的估算都偏小，会导致优化方向朝着 0 走，所以，一般会进行一个修正（bias correct），方式是：<br>$$<br>\hat{m_{t}}=m_{t}/1-{\beta_{1}}^{t}<br>$$<br>$$<br>\hat{v_{t}}=v_{t}/1-{\beta_{2}}^{t}<br>$$<br>而 bert 中实现的 Adam 却没有进行这个修正，至于原因，放在下面一起说。</p>
<h2><span id="weight-decay">weight decay</span><a href="#weight-decay" class="header-anchor"></a></h2><p>在 bert 中对 Adam 进行了weight decay，具体代码上是这一段：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Just adding the square of the weights to the loss function is *not*</span></span><br><span class="line"><span class="comment"># the correct way of using L2 regularization/weight decay with Adam,</span></span><br><span class="line"><span class="comment"># since that will interact with the m and v parameters in strange ways.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Instead we want ot decay the weights in a manner that doesn't interact</span></span><br><span class="line"><span class="comment"># with the m/v parameters. This is equivalent to adding the square</span></span><br><span class="line"><span class="comment"># of the weights to the loss with plain (non-momentum) SGD.</span></span><br><span class="line"><span class="keyword">if</span> self._do_use_weight_decay(param_name):</span><br><span class="line">      update += self.weight_decay_rate * param</span><br></pre></td></tr></table></figure></p>
<p>这里讲到<code>直接将权重的平方加入到loss 上进行L2 regularization 在 Adam 上是一种错误到方式</code></p>
<h3><span id="weight-decay">weight decay</span><a href="#weight-decay" class="header-anchor"></a></h3><p>Weight decay是在每次更新的梯度基础上减去一个梯度</p>
<p>$$\theta_{t+1}=(1-\lambda )\theta_{t} -\alpha \bigtriangledown f_{t}(\theta_{t})$$</p>
<h3><span id="l2-regularization">L2 regularization</span><a href="#l2-regularization" class="header-anchor"></a></h3><p>L2 regularrization是在参数上加上L2惩罚</p>
<p>$$ f_{t}^{reg}(\theta)=f_{t}(\theta)+\frac{ {\lambda }’}{2}\left \| \theta\right \| _{2}^{2}$$</p>
<p>可以看出，在标准SGD下，两者是等价的<br>但是，在Adam下，两者却不是。我们将Adam下的梯度更新完整公式写出来：</p>
<p>$$ \theta_{t}\leftarrow \theta_{t-1} -\alpha \frac{\beta_{1}m_{t-1}+(1-\beta_{1})(\bigtriangledown f_{t}+\lambda \theta_{t-1})}{\sqrt{\hat{v_{t}}} + \varepsilon  }$$</p>
<p>而与参数有关的是右上角的部分：<code>$\frac{\lambda \theta_{t-1}}{\sqrt{v_{t}}}$</code> 而这一项表明，在梯度变化越大的方向上，v的值也越大，但对应的权重约束却越小，这显然是不合理的，此外，L2 与 weight decay 都是各个方向同性的，<br>所以针对这一问题，一种调整方式是将梯度更新与weight decay 解偶，<br><img src="/2020/08/01/optimizer-in-bert/de.png" alt><br>具体参考<a href="https://arxiv.org/pdf/1711.05101.pdf" target="_blank" rel="noopener">DECOUPLED WEIGHT DECAY REGULARIZATION</a><br>而 bert 中也是使用了这种weight decay 方式，来达到与L2正则等效</p>
<h1><span id="learning-rate">Learning rate</span><a href="#learning-rate" class="header-anchor"></a></h1><h3><span id="learning-rate-decay">Learning rate decay</span><a href="#learning-rate-decay" class="header-anchor"></a></h3><p>通常，为了让模型在后期避免震荡，更加稳定，都会随着训练的进行，将learning rate 进行调整，即越是后期learning rate 越小。</p>
<h3><span id="warmup">warmup</span><a href="#warmup" class="header-anchor"></a></h3><p>而bert中的learning rate的调整是两段线性调整学习率：前 10% steps 将learning rate 从 0 增长到 init_learning_rate，然后，再一致递减 到0<br>而warmup为何有效？</p>
<ol>
<li>可以避免较早的对mini-batch过拟合，即较早的进入不好的局部最优而无法跳出；</li>
<li>保持模型深层的稳定性<br>具体可以参考<a href="https://www.zhihu.com/question/338066667/answer/771252708" target="_blank" rel="noopener">warmup 为什么有效</a></li>
</ol>
<p>此外，由于warmup要求前期保持较小的更新，所以Adam中由于前期会导致更新变小而需要进行的bias correct也可以去掉了。这也就是最初留下到那个问题到答案</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>bert在 pretraining 为了让模型收敛到一个较好的点，不但在优化器 Adam 上使用了与 L2 regularization等效的weight decay，为了避免模型前期过早拟合进入local minimal，使用了warmup 策略。<br>bert作者也建议在进行fine-tuning时，使用与bert源码中相同的优化器，我也做了一些实验，提升有大概不到0.5个点（没有细调），所以在下游任务上可以尝试使用。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于圆明园荷花池</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Optimizer</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Distillation (1) &amp;#58; 模块替换之bert-of-theseus-下篇</title>
    <url>/2020/08/19/bert-of-theseus-2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#fu-xian-shi-de-wen-ti">复现时的问题</a><ul>
<li><a href="#si-kao-1-wei-shi-me-shi-xiao">思考1：为什么失效</a></li>
<li><a href="#si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me">思考二 ：bert-of–theseus有效的本质是什么</a></li>
<li><a href="#shi-yan-1">实验1</a></li>
<li><a href="#si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing">思考三：直接在predecessor 上抽successor行不行？</a></li>
<li><a href="#shi-yan-2">实验2</a></li>
</ul>
</li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>上一篇<a href="https://xv44586.github.io/2020/08/09/bert-of-theseus/">模块替换之bert-of-theseus-上篇</a>中介绍了bert-of-theseus论文的主要思路，并贴了两组实验的结论，这篇是对上篇的后续一些思考与实验。</p>
<h1><span id="fu-xian-shi-de-wen-ti">复现时的问题</span><a href="#fu-xian-shi-de-wen-ti" class="header-anchor"></a></h1><p>在复现时，遇到最大的问题就是结果不稳定。首先每次训练predecessor时，其最优结果就会有上下1个点左右的波动，而因为theseus 中引入了随机数<br>来概率替换对应block，所以结果上一言难尽，有时能比12层bert低0.6个点, 有时只能达到直接3层fine tuning 的效果，于是我做了些观察与思考。</p>
<h2><span id="si-kao-1-wei-shi-me-shi-xiao">思考1：为什么失效</span><a href="#si-kao-1-wei-shi-me-shi-xiao" class="header-anchor"></a></h2><p>在训练theseus model时，其中抽出的successor在每个epoch结束后在验证集上的结果有时会很高，基本到达只比三层fine-tuning低6个点，有时又很<br>低，基本不到0.1%, 第一种明显是successor在theseus中训练太多，以至于接近直接fine tuning，而另一种情况下可能是successor训练不充足，<br>也可能是替换次数太少导致没有被训练，而且大多数情况下successor的验证集上都是不到0.1%。<br>为了验证第二种情况下是否是未替换导致successor在做fine tuning，我将successor进行单独fine tuning后,将得到的classifier 拼回predecessor，<br>发现此时在验证集上d结果只下降了2个点，所以此时大概率是替换次数过少，基本没有训练到successor，所以导致结果不好，而这里开始我以为是我<br>实现问题，后来来来回回检查了一周，也没发现问题，于是我就想换一种更稳定的方式。</p>
<h2><span id="si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me">思考二 ：bert-of–theseus有效的本质是什么</span><a href="#si-kao-er-bert-of-theseus-you-xiao-de-ben-zhi-shi-shi-me" class="header-anchor"></a></h2><p>熟悉bert的同学肯定对warm up不陌生，而warm up之所以有效，我认为比较重要的一点是如果在最初的steps中，模型提前拟合了样本，进入了一个局部最优区域，后期无论你怎么迭代他都跳不出来，而由已经<code>fine tuned predecessor</code>带着一起再进行训练，也和warm up有些相似，即用小的<br>步子带着你朝着更优的方向走几步，跳出来，让你有进入更好的局部最优点的可能，此外，概率替换的思路也与<code>Dropout</code>有几分相似，让successor<br>有一定的几率参与训练，从而让successor在缺少predecessor的情况下也有一定的robust。<br><a href="https://spaces.ac.cn/archives/7575" target="_blank" rel="noopener">苏剑林的博客</a>里也提到了替换的数学形式：<br>$$<br>\begin{equation}\begin{aligned}<br>&amp;\varepsilon^{(l)}\sim U(\{0, 1\})\\<br>&amp;x^{(l)} = x_p^{(l)} \times \varepsilon^{(l)} + x_s^{(l)} \times \left(1 - \varepsilon^{(l)}\right)\\<br>&amp;x_p^{(l+1)} = F_p^{(l+1)}\left(x^{(l)}\right)\\<br>&amp;x_s^{(l+1)} = F_s^{(l+1)}\left(x^{(l)}\right)<br>\end{aligned}\end{equation}<br>$$<br>同时，他也提到$\epsilon$能否不取非0即1，那既然我们是想让successor在task方向上warm up一下，那直接相加，即此时 $\epsilon = k$,<br>k是常数也是可以的。此时只要调节k 就能避免successor训练不充分或太充分的情况了，模型也就稳定了，可以满足我们的要求了。</p>
<h2><span id="shi-yan-1">实验1</span><a href="#shi-yan-1" class="header-anchor"></a></h2><p>实验代码其实比较容易修改，只需将BinaryRandomChoice 层替换为相加即可。具体代码在<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py" target="_blank" rel="noopener">classification_ifytek_bert_of_theseus</a><br>中可以看到。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProportionalAdd</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="string">"""将两层的结果乘比例后相加，output = (input_1 * proportion + input_2 * (1 - proportion)) / 2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, proportion=<span class="number">0.5</span>, **kwargs)</span>:</span></span><br><span class="line">        super(ProportionalAdd, self).__init__(**kwargs)</span><br><span class="line">        self.supports_masking = <span class="literal">True</span></span><br><span class="line">        self.proportion = proportion</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_mask</span><span class="params">(self, inputs, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> mask[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        source, target = inputs</span><br><span class="line">        source = source * self.proportion</span><br><span class="line">        target = target * (<span class="number">1</span> - self.proportion)</span><br><span class="line">        output = (source + target)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> K.in_train_phase(output, target)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> input_shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>文本分类：CLUE的iflytek数据集</p>
<p>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 60.11\% &amp; 58.99\%  &amp; 57.96\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 59.7\%  &amp; 59.5\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p>
<p>结果上看确实更稳定了，也更好一点点了，基本比predecessor低<code>0.5%~1%</code> .</p>
<h2><span id="si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing">思考三：直接在predecessor 上抽successor行不行？</span><a href="#si-kao-san-zhi-jie-zai-predecessor-shang-chou-successor-xing-bu-xing" class="header-anchor"></a></h2><p>既然我们说bert-of-theseus有效的原因是在task 的方向进行了warm up，那predecessor已经在task上fine tuned了，能不能<code>直接抽取某几<br>层作为successor来直接fine tuning?</code>此外，之前我们也说了，predecessor与successor的classifer差距很小，那我们能不能改变successor<br>的classifer的学习率，让他进一步学习，来弥补一部分前三层无法拟合的分布呢？</p>
<h2><span id="shi-yan-2">实验2</span><a href="#shi-yan-2" class="header-anchor"></a></h2><p>具体实验代码<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/two_stage_fine_tuning.py" target="_blank" rel="noopener">two-stage-fine-tuning</a><br>实验时尝试了<code>随机初始化classifier/predecessor classifier初始化classifier/ 放大classifier lr</code>组合策略，最后的结果就不贴了，基本都没有<br>超过3层bert fine tuning的效果。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>尝试分析了bert-of-theseus复现中的问题，并尝试了一些修复方案，同时，实验测试了theseus model的必要性，最后结论是binary random choice<br>策略不如 proportion add 策略稳定，同时，theseus是必须的。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://github.com/JetRunner/BERT-of-Theseus" target="_blank" rel="noopener">论文原作者配图</a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Distillation</tag>
        <tag>speed-up</tag>
      </tags>
  </entry>
  <entry>
    <title>模型增强（1）&amp;#58; 利用NLG 增强QA 任务性能</title>
    <url>/2020/08/22/qa-augmentation/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#bei-jing">背景</a></li>
<li><a href="#unilm">UniLM</a></li>
<li><a href="#shu-ju-zeng-qiang">数据增强</a></li>
<li><a href="#shi-yan">实验</a><ul>
<li><a href="#wen-ti-sheng-cheng">问题生成</a></li>
<li><a href="#wen-ti-da-an-dui-sheng-cheng">问题答案对生成</a></li>
</ul>
</li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="bei-jing">背景</span><a href="#bei-jing" class="header-anchor"></a></h1><p>上周打算把UniLM在<a href="https://github.com/xv44586/toolkit4nlp" target="_blank" rel="noopener">toolkit4nlp</a>的基础上实现一下，又刷了一遍<a href="https://arxiv.org/pdf/1905.03197.pdf" target="_blank" rel="noopener">论文</a>,发现作者提到用UniLM做问题生成，来增强QA任务的性能，觉得很有意思，所以想尝试一下。</p>
<h1><span id="unilm">UniLM</span><a href="#unilm" class="header-anchor"></a></h1><p>因为这篇 UniLM 是主角，所以简单介绍一下该模型。该模型是通过灵活使用 attention mask ，将 NLG 与 NLU 任务统一在来一起，所以叫 unified LM，<br>他的做法是将 left-to-right/right-to-left/masked lm/seq2seq lm/放在一个框架里训练，从而让模型兼具 NLU 与 NLG 的能力。<br><img src="/2020/08/22/qa-augmentation/lm.png" alt><br>而为了达到这个训练，只需要在 bert 的基础上根据不同的 lm 调整 attention mask 即可。所以利用 bert 做 NLG 时，只需要调整 attention mask<br>为 seq2seq lm 对应mask即可。</p>
<h1><span id="shu-ju-zeng-qiang">数据增强</span><a href="#shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>通常增强都是同义词/近义词替换，subsequence的随机删除/重复/互换等，我之前在做百度比赛时尝试过随机删除和随机两个片段互换位置，提升不是<br>非常大而论文里大问题生成带来大提升还是相当大的：<br><img src="/2020/08/22/qa-augmentation/table9.png" alt><br>仔细想一下，由于attention机制，互换只是改变了position embedding部分内容，而这部分的互换对模型的影响是很弱的；随机删除可能会破坏语义，<br>所以增加模型robust的同时可能会降低模型性能。而问题生成，则可以看作是同义词/近义词替换的句子级别替换，所以理论上能带来不错的提升。<br>从对抗的角度来看，生成的问题在语义上与原问题基本一致，这也正好符合<code>输入的微小改变</code>，从而让模型在这种带有微小扰动的前提下仍然能很好的预测。</p>
<h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>既然UniLM具有很强的NLG能力能力，那就有很多不同的玩法。首先，可以训练一个模型，来针对 context 和 answer 生成对应的问题，来对问题进行<br><code>“换个问法”</code>，其次，既然可以对问题<cdoe>“换个问法”,自然也可以<code>“换个问题”</code>,也就是根据 context 生成新的问题<br>和答案。另外，由于是扩增训练数据，所以有一个技巧是做生成是将 train data 与 dev data 互换，不过由于我用的是百度比赛数据，dev data 太少，<br>所以我是 train + dev。</cdoe></p>
<h2><span id="wen-ti-sheng-cheng">问题生成</span><a href="#wen-ti-sheng-cheng" class="header-anchor"></a></h2><p>问题生成时，就是将 context 与 answer 拼接，然后生成对应的question。具体样本形如：<code> [CLS] answer + context [SEP] question [SEP]</code> .<br>模型直接用bert base权重按UniLM的seq2seq方式来构建，可以看到效果还是很不错的，比如：</p>
<blockquote><br>context：报雅思或者托付培训班,一般情况下要900元左右。 雅思和托福考试可以自学: 一、基础知识准备:单词、基本语法、长难句分析; 二、板块训练:听说读写,四个板块; 三、合理备考计划,可以参见中国上别人经验结合自己的自身条件; 四、效果强化跟踪,使用合理的备考软件或者是自测题目随时跟踪自己的学习状态<br>question：雅思班价格<br>answer: [‘900元’, ‘900元左右’]<br>generate question: 雅思班报名多少钱<br></blockquote>

<blockquote>context：USB电压有5伏 USB一般4根线, 中间两根是数据线, 左右各为 +- 线 只要不短路是不会烧主板该插口的 ,我想你应该这样做,手机的线的一端直接插入手提电脑,另一头剪掉头子,从线中分离出四根线, 用万用表测出(红色+和其它色如黑-)剩下两根用胶布包扎(不用)然后 在这两根线上(正电极中最好串一50到100欧电阻)后接入一支高亮度发光二极管就成功了.<br>question：usb线电压<br>answer: [‘5伏’]<br>generate question: usb线电压 </blockquote>

<p>解码时，有两种选择：随机抽样与 beam search 。随机抽样可以增加问题的多样性，并且可以生成多个问题；beam search近似最优，得到一个最优的<br>问题。由于我们是使用 train data 训练模型，在对 train data 生成新的问题时，beam search 将可能产生很多一摸一样的问题，这样将降低新增<br>数据的量；而随机抽样能产生很多新的问题，但可能新生成的问题与答案并不配套，还需要一些后处理之后才能真正拿来用。这里两种方式都拿来做实验，<br>并对生成的问题做一个简单的过滤：新生成的问题与原问题中有70%以上的字是重合的。<br>$$<br>\begin{array}{c|c|c}<br>\hline \\<br>\text{base line} &amp; \text{beam search} &amp; \text{random sample}\\<br>\hline \\<br>80.39\% &amp; 81.0\% &amp; 79.8\%<br>\end{array}<br>$$</p>
<p>random sample的样本经过了很多次过滤之后才能基本达到baseline的效果，所以生成的问题如果”问非所答”，对最终的效果反而是不好的，这也符合预期。</p>
<h2><span id="wen-ti-da-an-dui-sheng-cheng">问题答案对生成</span><a href="#wen-ti-da-an-dui-sheng-cheng" class="header-anchor"></a></h2><p>问题答案对生成时，由于答案是在 context 内的，相对问题生成简单一些，所以我们先生成答案，再根据 context 和生成的 answer 来生成对应的<br>question。不过为了让问题答案对更丰富多样，解码答案时我们采用随机抽样，而生成问题时，为了让问题尽量准确，我们采用 beam search。<br>样本形如 <code>[CLS]context[SEP]answer[SEP][question][SEP]</code>，生成的效果如下：</p>
<p><blockquote><br>context：您好，孕妇整个孕期体重增加12.5公斤左右，在增加的这12.5公斤中，胎儿的体重占3公斤左右，胎盘和羊水约是2公斤左右。在孕早期（怀孕3个月以内）增加2公斤，中期（怀孕3－6个月）以及末期（怀孕7－9个月）各增加5公斤左右。所以怀孕6个月体重增加7公斤左右比较正常。希望我的回答对你有所帮助。<br>question：孕妇6个月体重增加多少<br>answer: 7公斤左右<br>generate question: 孕妇6个月体重增加多少<br>generate answer: 12.5公斤左右<br></blockquote><br>不过也由于train data 参与训练，所以很多生成的问题答案对与原始问题答案对一致，如果有更多的外部数据，可以利用外部数据来训练。<br>$$<br>\begin{array}{c|c|c|c}<br>\hline \\<br>\text{base line} &amp; \text{beam search} &amp; \text{random sample} &amp; \text{question answer generation}\\<br>\hline \\<br>80.39\% &amp; 81.0\% &amp; 79.8\% &amp; 81.76\% \\<br>\hline<br>\end{array}<br>$$</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>通过生成新的问题与新的问题答案对能在一定程度上提高qa 任务的性能，在生成问题时，用beam search 得到的新问题虽然量少但由于更准确，所以<br>能带来一定的提升；用随机采样生成的问题会有部分与答案无关的或者语义有点不通顺的问题，所以可能反而会导致性能降低；问题答案对的生成时，<br>先生成相对简单的回答再生成对应问题，能对性能带来不错的提升，在做qa相关任务时，可以尝试使用一下。<br>实验代码：<br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/qa_baseline.py" target="_blank" rel="noopener">qa_baseline</a><br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/qa_question_generation_seq2seq.py" target="_blank" rel="noopener">qa_question_generation_seq2seq</a><br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/qa_question_answer_generation_seq2seq.py" target="_blank" rel="noopener">qa_question_answer_generation_seq2seq</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>看瓜的怒气小猫</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>QA</tag>
        <tag>NLG</tag>
        <tag>UniLM</tag>
      </tags>
  </entry>
  <entry>
    <title>模型增强（2）&amp;#58; 从label下手</title>
    <url>/2020/09/13/classification-label-augment/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#gou-zao-xin-label">构造新label</a><ul>
<li><a href="#zi-jian-du">自监督</a></li>
<li><a href="#xiang-si-xing">相似性</a></li>
</ul>
</li>
<li><a href="#shi-yan-jie-guo">实验结果</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>上篇<a href="https://xv44586.github.io/2020/08/31/bert-01/">Knowledge Distillation (2): distilling knowledge of bert</a>中我们提到，模型压缩时一个很重要的知识是soft labels，<br>并且提出了一种简单的方式：自蒸馏（self-distillation），而从label 的角度来看，可以看作是一种label augmentation，即构造了一个新的label，为模型新增了一个任务，通过新任务的学习，来提高模型对原来任务的性能。本文就label augmentation 继续脑洞。</p>
<h1><span id="gou-zao-xin-label">构造新label</span><a href="#gou-zao-xin-label" class="header-anchor"></a></h1><p>构造新label，其实本质上是构造一个与当前任务相关的新的任务，而对应的label则是通过当前样本通过某种方式获得，获得的label至少要比随机好，否则只会帮倒忙。</p>
<h2><span id="zi-jian-du">自监督</span><a href="#zi-jian-du" class="header-anchor"></a></h2><p>构造新label，我们可以借鉴自监督的方式，如Mask Language Model，AutoRegressive，而BERT中已使用来MLM，UniLM中也告诉我们增加Seq2Seq的AR 任务对NLU任务提高不显著，不过今年的论文<a href="http://arxiv.org/abs/2004.10964" target="_blank" rel="noopener">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a> 实验证明了进一步预训练是能进一步提升下游任务的性能。而当前任务是文本分类，MLM也许不是很合适，所以Seq2Seq 的方式可以尝试。<br>具体的，我们让模型学习目标类别的同时，希望模型能同时生成样本的描述字段（或者人为给定的某种相关性短语），即利用类别对应描述字段构造一个seq2seq任务。</p>
<h2><span id="xiang-si-xing">相似性</span><a href="#xiang-si-xing" class="header-anchor"></a></h2><p>对于同一个类别的样本，他们必然有某种相似性，至少比与其他类别的样本更相似。而何如构造样本呢？<br>一种简单的方式是对每个样本都从类当中抽取一个样本与他组成一对，然后让每个<code>i</code>样本与<code>i+1</code>样本相似。这种方式由于每次样本都是shuffle 的，只要让batch size 小于label number，一个batch 内同时出现多个同一类别的样本概率就会很小。<br>既然在构造seq2seq任务时，我们使用来label对应的描述，此时我们也可以继续尝试使用：每个样本构造一个新的样本，新样本由label对应描述与label id组成。</p>
<h1><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h1><p>两组实验结果如下：</p>
<p>$$<br>\begin{array}{c|c|c}<br>\hline \\<br>\text{seq2seq} &amp; \text{similarity} \\<br>\hline \\<br>59.91\% &amp; 56.9\%<br>\end{array}<br>$$</p>
<p>可以看到，对于构造seq2seq 任务，其结果与直接fine-tuning 结果基本一致，这也符合预期。而构造相似性任务，其结果直接fine-tuning 结果相比反而更差了。原因可能是样本不均衡，所以同一batch 内有较高概率出现同一类别的样本，同时通过让样本与同一样本相似来间接相似，这种方式可能有些曲折了，不过最根本的原因应该还是batch 内同一类别样本的出现干扰了学习。<br>具体实验代码可以查阅<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_auxiliary_seq2seq_task.py" target="_blank" rel="noopener">classification_ifytek_auxiliary_seq2seq_task</a> 和<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_with_similarity.py" target="_blank" rel="noopener">classification_ifytek_with_similarity</a></p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文只是由于之前实验想到的尝试对label 做增强来实现模型增强的尝试，最后两组实验都没取得什么好的结果。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于翠湖湿地</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Classification</tag>
      </tags>
  </entry>
  <entry>
    <title>年轻人的第一个swift：ios 模拟定位打卡</title>
    <url>/2020/09/30/location/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#qi-yin">起因</a></li>
<li><a href="#mo-ni-ding-wei">模拟定位</a></li>
<li><a href="#fang-fa">方法</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="qi-yin">起因</span><a href="#qi-yin" class="header-anchor"></a></h1><p>由于最近马上过节了，而我也被这节日的气氛所支配，所以今天完成了未打卡三连成就。公司现在规定每个月只有两次未正常打卡的机会，超过了会有相应的惩罚机制。我感觉自己工作时长接近十个小时，但是因为忘记打卡被处罚的话心里多少有点不爽，所以就想是不是可以补救一下。- -！</p>
<h1><span id="mo-ni-ding-wei">模拟定位</span><a href="#mo-ni-ding-wei" class="header-anchor"></a></h1><p>很早之前就听说Xcode 提供了模拟定位，方便开发者调试，所以我想这个应该是个切入点，问了一个懂这个大佬，也得到了肯定的答案。<br>所以主体思路是通过Xcode 模拟定位，然后借助这个模拟定位功能，定位到公司附近，然后钉钉打卡。不过网上找到的方法大多数都不是swift 的，我自己摸索了一个小时找到了具体方案，其实非常的简单。</p>
<h1><span id="fang-fa">方法</span><a href="#fang-fa" class="header-anchor"></a></h1><ol>
<li><p>首先，通过<a href="https://lbs.amap.com/console/show/picker" target="_blank" rel="noopener">高德</a>/百度地图/腾讯地图获取公司附近经纬度，不过需要注意的是iOS原生坐标为<code>世界标准地理坐标(WGS-84)</code>, 百度地图的坐标为<code>BD-09</code>,高德为<code>中国国测局地理坐标（GCJ-02）</code>,需要将位置转换为iOS 坐标下的。<br>坐标转换有比较简单的方法，GitHub上找到一个<a href="https://github.com/wandergis/coordTransform_py" target="_blank" rel="noopener">coordTransform_py</a>(忽略我是一个GIS专业学生 - -！),对应转换：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> coordTransform_utils <span class="keyword">import</span>  *</span><br><span class="line">lon, lat = <span class="number">120.177239</span>,<span class="number">30.216698</span></span><br><span class="line">w_lon, w_lat = gcj02_to_wgs84(lon, lat)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Xcode 新建一个项目，项目内新建一个gpx 文件，文件内容里添加对应的经纬度：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">gpx</span> <span class="attr">version</span>=<span class="string">"1.1"</span> <span class="attr">creator</span>=<span class="string">"Xcode"</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">     Provide one or more waypoints containing a latitude/longitude pair. If you provide one</span></span><br><span class="line"><span class="comment">     waypoint, Xcode will simulate that specific location. If you provide multiple waypoints,</span></span><br><span class="line"><span class="comment">     Xcode will simulate a route visiting each waypoint.</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">wpt</span> <span class="attr">lat</span>=<span class="string">"39.99200300843388"</span> <span class="attr">lon</span>=<span class="string">"116.46688673941635"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>Cupertino<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">         Optionally provide a time element for each waypoint. Xcode will interpolate movement</span></span><br><span class="line"><span class="comment">         at a rate of speed based on the time elapsed between each waypoint. If you do not provide</span></span><br><span class="line"><span class="comment">         a time element, then Xcode will use a fixed rate of speed.</span></span><br><span class="line"><span class="comment">         </span></span><br><span class="line"><span class="comment">         Waypoints must be sorted by time in ascending order.</span></span><br><span class="line"><span class="comment">         --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">time</span>&gt;</span>2014-09-24T14:55:37Z<span class="tag">&lt;/<span class="name">time</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">wpt</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">gpx</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>手机与电脑连接，然后让程序在手机上运行起来，同时保持手机定位打开状态，此时就可以通过模拟位置来修改手机的当前定位了。最终打卡成功。</p>
</li>
</ol>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文是由于未正常打卡紧缺，我尝试补救的一次探索，由于是第一次接触Xcode ，感觉有点兴奋。不过希望看到这里的你不要用这个方法来做坏事。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>打卡成功</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>swift</tag>
      </tags>
  </entry>
  <entry>
    <title>一切三段成三角形</title>
    <url>/2020/10/19/triangle/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#ti-mu">题目</a></li>
<li><a href="#jie-da">解答</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>听到一个题目，还挺有意思，所以记录一下。</p>
<h1><span id="ti-mu">题目</span><a href="#ti-mu" class="header-anchor"></a></h1><p>问：一个绳子长<code>a</code>，任意剪两刀变成三段后，可以组成一个三角形的概率是多少？</p>
<h1><span id="jie-da">解答</span><a href="#jie-da" class="header-anchor"></a></h1><p>分析一下题目，一段长度固定的绳子，切割为三段，则：<br>1.<em> 任意一段绳子的长度大于0小于<code>a</code>；</em><br>2.<em> 任意两段绳子的长度大于0小于<code>a</code>。</em></p>
<p>而三段绳子可以组成一个三角形，而三角形中两边之和大于第三边，所以意味着：<br>1.<em>任意一段的长度小于<code>a/2</code>；</em><br>2.<em>任意两段的长度和大于<code>a/2</code>.</em></p>
<p>现在设其中两段的长度分别为 <code>x</code>, <code>y</code>，画出一个直角坐标系，如下图所示。<br><img src="/2020/10/19/triangle/triangle.png" alt></p>
<p>同时连接<code>(0, a), (a, 0)</code>两点，则切割绳子后其中两段的长度的所有可取的值对应由<code>(0, 0),(a, 0), (0, a)</code>组成的三角形区域。<br>简单证明如下：对于任意一点<code>B</code>,我们做其对<code>y</code>轴的垂线，相交与点<code>P</code>,而点<code>B</code>位于<code>(a, 0) (0, a)</code>的连线时，对应$\angle PAB  = 45^o$,此时<br>$PB=PA \Rightarrow  PB + PO = PA + PO = a$,而当<code>P</code>位于连线外侧，则$\angle PAB  &gt; 45^o$,对应$PO + PB &gt; PO + PA = a$,不满足约束。而$\Delta A O C$ 的面积为 $a^2 / 2$.</p>
<p>而对于能组成三角形时，首先任意一条长度小于$a/2$，对应<code>(0, a/2),(a/2,a/2)</code>连线与<code>(a/2,0)(a/2,a/2)</code>连线围城的矩形区域，而任意两边之和大于$a/2$,利用之前的方法可以证明是(0,a/2)(a/2, 0)连线右侧区域，两者的交即图中绿色三角形区域，其面积为$(a/2)^2/2 = a^2 / 8$</p>
<p>所以最后结论是组成三角形的概率为两个面积之比：<br>$$<br>pro = \frac{a^2/8}{a^2/2} = 1/4<br>$$</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title>AdaBelief-更稳定的优化器</title>
    <url>/2020/10/25/adabelief/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#warmup">warmup</a></li>
<li><a href="#xiu-gai-adam">修改Adam</a></li>
<li><a href="#you-dian">优点</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>对<code>Adam</code> 进行改进的算法有很多，今天介绍一篇改动很小改动小效果不错的-<code>AdaBelief</code>。</p>
<h1><span id="warmup">warmup</span><a href="#warmup" class="header-anchor"></a></h1><p>在bert中第一次见到warmup的使用，而warmup的作用是让训练更稳定，最后收敛的更好。而warmup有效的一个原因是减缓训练初期模型对mini-batch的提前过拟合，同时，在训练初期，由于loss较大，模型还没得到多少训练，不同step 之间的梯度方差较大，而此时如果使用较大的步长更新，则会朝错误的方向走一大步，而随后的模型不断得到训练，对应的梯度不断减小，同时一般我们会采用不断衰减的学习率，这些都导致随着模型的训练，更新的步长不断变小，而前期朝错误方向的一大步更新可能需要后期很多步的更新才能弥补，有时候可能甚至无法弥补，这就导致模型最后收敛在一个不怎么好的局部最优点，而如果在前期时抑制可能出现的大步更新，保持模型保持“小步走”，则可以避免模型在错误方向上的大步更新，而由模型的不断训练调整会正确的轨道。<br>所以一个重要的点是梯度更新方差大时（不同time step），我们需要谨慎行事，防止出现大错步，而方差小时，我们可以大胆一些，因为此时方向上基本一致，所以可以大踏步的往前走。</p>
<h1><span id="xiu-gai-adam">修改Adam</span><a href="#xiu-gai-adam" class="header-anchor"></a></h1><p>现在让我们来回顾一下Adam更新公式：</p>
<p>$$<br>\theta_t = \theta_{t-1} - \alpha \frac{m_t}{\sqrt{v_t}}<br>$$</p>
<p>其中$m_t$是对$g_t$的预测，$v_t$是对$g_t^2$的预测，对应的更新方向为$\frac{m_t}{\sqrt{v_t}}$.<br>$m_t$除了是对$g_t$的预测外，还可以看做是最近一段时间内（大概为$\frac{1}{1-\alpha}$）梯度的均值,而为了表征当前梯度$g_t$所处区域的方差，我们可以使用$belief = \left | g_t - m_t\right |$,即当前梯度距最近一段区域梯度均值的距离。在结合Adam的更新公式，我们可以用$s_t = (g_t - m_t) ^ 2$ 来代替$v_t$,即在方差大的区域更新时减小步长，而在方差小的区域，快步大走，最后的更新公式为：</p>
<p>$$<br>\theta_t = \theta_{t-1} - \alpha \frac{m_t}{\sqrt{s_t}}<br>$$</p>
<p>此时的更新方向为$\frac{m_t}{\sqrt{s_t}}$.<br>这就是<a href="https://arxiv.org/pdf/2010.07468.pdf" target="_blank" rel="noopener">AdaBelief Optimizer</a>的核心思想。具体的更新流程与Adam只需要修改一小部分即可：<br><img src="/2020/10/25/adabelief/opt.jpg" alt></p>
<h1><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h1><p>作者在论文中提到AdaBelief能媲美Adam的收敛速度，同时达到SGD的准确率。我做了几个实验，由于是在小数据集上fine-tuning，所以可能不如在大数据集上从头训练效果明显。不过依然可以得到：<br>1.<em>loss上相对Adam更平稳</em><br>2.<em>收敛上比Adam稍快</em><br>3.<em>性能上比Adam更好</em></p>
<p>loss 对比图<br><img src="/2020/10/25/adabelief/loss.png" alt></p>
<p>accuracy对比图<br><img src="/2020/10/25/adabelief/acc.png" alt></p>
<p>实验代码：<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_adabelief.py" target="_blank" rel="noopener">classification_adabelief</a></p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文介绍一个最新的优化器AdaBelief，并从与论文不同角度解释其主要作用，在实际工作中可以尝试使用AdaBelief，也许能得到比Adam收敛更快性能更好的结果。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>算法改进对比图</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title>PET-文本分类的又一种妙解</title>
    <url>/2020/10/25/pet/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#classification-to-nlg">Classification to NLG</a></li>
<li><a href="#mlm">MLM</a></li>
<li><a href="#ren-wu-zhuan-huan">任务转换</a></li>
<li><a href="#pattern-exploiting-training">Pattern-Exploiting Training</a></li>
<li><a href="#yu-nlg-chai-yi">与NLG差异</a></li>
<li><a href="#shi-yan">实验</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>之前的一篇<a href="https://xv44586.github.io/2020/09/13/classification-label-augment/">《模型增强-从label下手》</a>中，我们提到了通过转换label，将分类转换为NLG的方法，而由于性能没有得到增加，所以就没有继续往下做。今天看到两篇文章，思路略微相似，也让我眼前一亮，发现原来我与顶会思路这么近（误），所以总结对比一下。</p>
<h1><span id="classification-to-nlg">Classification to NLG</span><a href="#classification-to-nlg" class="header-anchor"></a></h1><p>对于分类任务，我们可以将其转换为一个生成任务。比如此时我们有一个样本：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"context：'「天天特价房」华庭仁和国际 3室2厅2卫仅售65万', label: '房产', label_id: 0"</span></span><br></pre></td></tr></table></figure></p>
<p>通常我们直接预测对应的label id，而由于其也有label，所以我们可以将其转换为一个NLG任务，即：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"context：['「天天特价房」华庭仁和国际 3室2厅2卫仅售65万', '房产']"</span></span><br></pre></td></tr></table></figure></p>
<p>即通过样本生成label对应的token。借助UniLM同时具有NLU与NLG的能力，只需要很小的改动就可以利用BERT做该任务了，对应的示意图如下：<br><img src="/2020/10/25/pet/unilm.png" alt></p>
<p>不过当时考虑到UniLM中提到seq2seq的训练<code>不能</code>提高NLU的能力，所以当时并没有选择使用MLM来尝试，最后得到的结论是：</p>
<p>1.<em> 将分类转为生成后，性能基本一致；</em><br>2.<em> 将分类与生成联合起来训练，性能与单个任务性能基本一致。</em></p>
<h1><span id="mlm">MLM</span><a href="#mlm" class="header-anchor"></a></h1><p>MLM,即Masked Language Model,中文翻译又叫“掩码语言模型”，即以自监督的方式，mask 掉一部分，然后通过剩余的部分来还原被mask 掉的部分，示意图如下：<br><img src="/2020/10/25/pet/mlm.png" alt></p>
<p>而mask的方式也有多种，如随机选择token进行mask；将token所在的整个词都mask（whole word mask）；或者将某个span内的token都mask掉（span mask）。<br>虽然mlm在预训练任务上已经被证明十分有效，但是通常认为mlm部分的参数是与mlm任务相关的，而通常在下游任务中我们是别的任务，所以会舍弃掉这部分参数，而只使用encoder部分。<br>但是论文<a href="http://arxiv.org/abs/2009.07118" target="_blank" rel="noopener">It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</a>与<a href="http://arxiv.org/abs/2001.07676" target="_blank" rel="noopener">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</a>却告诉我们，mlm不仅有用，在few-shot场景下，通过一下简单的融合手段，性能能超过当前的明星GPT-3.</p>
<h1><span id="ren-wu-zhuan-huan">任务转换</span><a href="#ren-wu-zhuan-huan" class="header-anchor"></a></h1><p>与之前的思路类似，我们针对分类任务，不再直接对label进行预测，而是预测其label description，即将其转换为完形填空形式的任务，来预测不同label description的概率。<br>而如何转换成完形填空呢？也很简单，我们添加一个简单的语义通顺的描述，然后将其中与分类有关的内容mask掉即可。举个例子：<br>假如我们现在的任务是短文本分类，一个样本为“context：’「天天特价房」华庭仁和国际 3室2厅2卫仅售65万’, label: ‘房产’”，我们添加一个统一的描述句，将其变为：<br>“下面是一则__相关新闻标题: 「天天特价房」华庭仁和国际 3室2厅2卫仅售65万”,其中的空格可选的内容是所有的label description，对应的真实值是”房产”两个字，这样，我们就将分类任务转换为一个完形填空的形式。<br>而添加的方式也可以分为前缀、后缀两种，完整的方式：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"以下是一则__相关新闻标题: 「天天特价房」华庭仁和国际 3室2厅2卫仅售65万"</span></span><br><span class="line"><span class="string">"「天天特价房」华庭仁和国际 3室2厅2卫仅售65万，以上是一则__相关新闻标题"</span></span><br></pre></td></tr></table></figure></p>
<h1><span id="pattern-exploiting-training">Pattern-Exploiting Training</span><a href="#pattern-exploiting-training" class="header-anchor"></a></h1><p>上面我们添加的前缀/后缀句子称为<code>Pattern</code>, 而label description可以有多种方式，比如，对于“房产”这个label，我们也可以用“地产”来表达，对于“娱乐”label，也可以用“八卦”来表达，所以需要一个token到label的映射，这个映射可以是多对一的，这个被称为<code>Verbalizer</code>,所以在预测时可以将多个token的概率结合起来判断其对应的label。<br>由于是few-shot，为了提高性能，作者采用了与Knowledge Distillation类似的思路，具体方案如下：</p>
<p>1.<em> 对每个Pattern利用多个pre-train model 进行fine-tuning，得到多个模型.其中$loss=L_{ce} + L_{mlm}$;</em><br>2.<em> 将多个模型的结果进行融合，得到一个融合模型Teacher Model；</em><br>3.<em> 利用Teacher Model在大量unlabed数据上进行预测，得到对应的soft labels；</em><br>4.<em> 利用soft labels数据，训练一个常规的分类模型（非MLM模型）。</em></p>
<p>以上就是论文<a href="http://arxiv.org/abs/2001.07676" target="_blank" rel="noopener">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</a>中提到的PET。<br>此外，该论文中还提到了一个改进：iPET。其中的区别是：在ipet中，得到mlm的多个model后，增加一个迭代：每次会从训练mlm的model中抽取一个$m_i$，然后从剩余的model中选取一部分对unlabeled data进行预测，将其中预测结果确定（不是准确，此时意味着结果的熵很小）的部分打上一个fake label，让$m_i$进行训练。重复多次后，融合模型对unlabeled data进行预测，得到一个soft labels data，在此基础上训练一个常规分类器。</p>
<p>可以看到，PET的方式主要适用label description为有限空间，即选择题，此外，每个样本的label description需要长度相同，而且由于mask之间相互独立，所以长度也不能太长。</p>
<h1><span id="yu-nlg-chai-yi">与NLG差异</span><a href="#yu-nlg-chai-yi" class="header-anchor"></a></h1><p>在之前的脑洞中，我们将分类任务转变为NLG任务，即利用样本来生成对应的label description，而他与PET中的主要差别主要有几点：</p>
<p>1.<em> NLG中我们并没有没有限制label description的长度，且不同label对应description也可能是不同长度；</em><br>2.<em> NLG中我们每个token的生成是有依赖关系的，即后面的token会依赖之前的token，所以token长度可以比PET中稍微长一些；</em><br>3.<em> PET中对应的解码空间大大减小，只需要得到label对应token的概率即可;</em><br>4.<em> PET中的pattern可以放在前缀也可以放在后缀，NLG可以看作是后缀PET.</em><br>5.<em> PET 中由于pre-train是mlm任务，所以zero-show性能更好。</em></p>
<h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>针对这些差异尝试做了几组实验，验证一下想法。</p>
<ol>
<li>NLG中label长度同一且解码时利用PET的方式解码，在few-shot下准确率从$52.4%$上升到$52.9%$，所以生成的label越短，解码空间越小越准确；</li>
<li>PET前缀pattern下准确率为%53.7%$,所以前缀pattern比后缀性能更好，这也与苏剑林<a href="https://spaces.ac.cn/archives/7764" target="_blank" rel="noopener">《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》</a>的结论一致。</li>
<li>zero-shot情况下，PET的准确率为$47.2%$, 而NLG只有$17.9%$，考虑到数据集全量下目前最好成绩才$60.7%$,说明PET的方式在zero-shot下效果相当惊人。</li>
</ol>
<p>主要实验代码在<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_pet_seq2seq.py" target="_blank" rel="noopener">classification_pet_seq2seq</a> 与 <a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_tnews_pet.py" target="_blank" rel="noopener">classification_tnews_pet</a></p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文介绍了一种新的转变分类任务获得更好性能的方法：即将分类任务转化为mlm模型进行完形填空，同时与之前脑洞的将分类转变为生成任务进行对比，通过实验验证了两者的差异与有效性。同时也提醒自己，多想几步，也许就能有新的发现。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Few-shot</tag>
      </tags>
  </entry>
  <entry>
    <title>ccf问答匹配比赛</title>
    <url>/2020/11/08/ccf-qa/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#bi-sai-shuo-ming">比赛说明</a></li>
<li><a href="#baseline">baseline</a><ul>
<li><a href="#qa-pair">qa pair</a></li>
<li><a href="#point">point</a></li>
<li><a href="#pet">pet</a></li>
<li><a href="#update-concat">update: concat</a></li>
</ul>
</li>
<li><a href="#dui-bi">对比</a></li>
<li><a href="#chang-shi">尝试</a><ul>
<li><a href="#post-training">Post-training</a></li>
<li><a href="#focal-loss">focal loss</a></li>
<li><a href="#dui-kang-xun-lian-yu-ti-du-cheng-fa">对抗训练与梯度惩罚</a></li>
<li><a href="#tricks">tricks</a></li>
</ul>
</li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>这两周玩了一下ccf 2020 中的<a href="https://www.datafountain.cn/competitions/474" target="_blank" rel="noopener">房产聊天问答匹配</a>比赛，虽然还没完赛，但是先总结一下目前的收获。</p>
<h1><span id="bi-sai-shuo-ming">比赛说明</span><a href="#bi-sai-shuo-ming" class="header-anchor"></a></h1><p>首先，这个比赛的任务是在一系列回答中找到针对客户问题的回答。而客户提问前的对话及回答前的对话都是不可见的，即整个IM信息是不连续的，任务就是在不连续的回答中找到那些针对客户问题的答案。样本示例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query: 采荷一小是分校吧。</span><br><span class="line">reply:</span><br><span class="line">  是的  <span class="number">0</span></span><br><span class="line">  杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。  <span class="number">1</span></span><br><span class="line">  这是五楼  <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，样本中所谓的针对问题的回答，不仅仅是直接回答问题的答案，而是更有针对性和说明的回答。</p>
<h1><span id="baseline">baseline</span><a href="#baseline" class="header-anchor"></a></h1><p>模型选择上，baseline全部使用bert，鉴于相对位置编码优于绝对位置编码，所以选择<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow" target="_blank" rel="noopener">NEZHA</a>作为预训练权重。备选方案Roberta。</p>
<h2><span id="qa-pair">qa pair</span><a href="#qa-pair" class="header-anchor"></a></h2><p>由于回答是不连续的，所以可以将问题和答案一一对应，组成qa pair，然后分别判断是否是针对问题的回答。</p>
<p><img src="/2020/11/08/ccf-qa/pair.png" alt="pair"></p>
<h2><span id="point">point</span><a href="#point" class="header-anchor"></a></h2><p>虽然对话是不连续的，但是是同一个对话，所以不同的回答能相互支撑，提供部分信息，所以，第二种思路就是将同一个问题的所有回答都拼接在当前回答后面，然后同时对每一个回答进行判断。</p>
<p><img src="/2020/11/08/ccf-qa/point.png" alt="point"></p>
<h2><span id="pet">pet</span><a href="#pet" class="header-anchor"></a></h2><p>由于预训练模型使用的语料与当前任务所处领域有一定的gap，所以一个比较简单的想法是先在任务语料上进行Post-training，然后再进行fine-tuning。不过，上次我们介绍过<code>Pattern-Exploiting Training</code>,不了解的可以参考<a href="https://xv44586.github.io/2020/10/25/pet/">PET-文本分类的又一种妙解</a>。借鉴PET的方式，我们将posting-training与fine-tuning结合，即在label data上进行pattern exploiting training，在unlable data上进行mlm任务进行post-traing.</p>
<p><img src="/2020/11/08/ccf-qa/pet.png" alt="pet"></p>
<p>以上三种baseline的代码放在<a href="https://github.com/xv44586/ccf_2020_qa_match" target="_blank" rel="noopener">ccf_2020_qa_match</a>,感兴趣的可以查阅。</p>
<h2><span id="update-concat">update: concat</span><a href="#update-concat" class="header-anchor"></a></h2><p>由于bert 不同的transformer 层提取到的语义粒度不同，而不同粒度的信息对分类来说起到的作用也不同，所以可以concat所以粒度的语义信息，拼接后作为特征进行分类。<br><img src="/2020/11/08/ccf-qa/concat.png" alt="concat"></p>
<h1><span id="dui-bi">对比</span><a href="#dui-bi" class="header-anchor"></a></h1><p>第一种方案（pair-wise），由于缺少一定的上下文信息，加上很多回答都非常短，同时又可能会离提问”较远”，所以效果是最差的，不过线上提交单模型也有0.75左右了，所以bert确实强大！<br>第二种方案（point）中，将所有已知的上下文信息都整合到一起，所以相对上一种有所提升，不过由于这种上下文的可见性，所以也会带来一定的迷惑：即对某一个reply来说，假如其他的reply中有一个是针对性的回答，就有可能会干扰对当前reply的判断。<br>第三种方案（pet）中，通过mlm进行post-training，可以将领域间的gap缩小，同时，由于在训练时”看到”了测试数据，也在一定程度上减小了线上线下的差距，所以性能是最好的，单模型最好能达到0.765左右。</p>
<h1><span id="chang-shi">尝试</span><a href="#chang-shi" class="header-anchor"></a></h1><h2><span id="post-training">Post-training</span><a href="#post-training" class="header-anchor"></a></h2><p>第一个想法是尝试进行post-training，来提升所有方案的性能。由于问答之间是不连续的，所以在组织语料上进行了不同方式：</p>
<ol>
<li>query-reply pair</li>
<li>query-reply-list pair</li>
<li>cut-sentence to make pair</li>
</ol>
<p>以上文提到的样本为例；</p>
<ol>
<li>query-reply pair:</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">采荷一小是分校吧。</span><br><span class="line">是的</span><br><span class="line"></span><br><span class="line">采荷一小是分校吧。</span><br><span class="line">杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><p>query-reply-list pair</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">采荷一小是分校吧。</span><br><span class="line">是的 杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。  这是五楼</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
<li><p>cut-sentence to make pair</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">采荷一小</span><br><span class="line">是分校吧。</span><br><span class="line"></span><br><span class="line">是</span><br><span class="line">的</span><br><span class="line"></span><br><span class="line">杭州市采荷第一小学钱江苑校区，</span><br><span class="line">杭州市钱江新城实验学校。</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>第一种，将同一对话作为同一篇文档顺序排列；第二种，将问题作为单独文档，同一问题的所有回答作为单独文档，第三种，将问题和回答都作为单独文档，同时将其拆分为左右两个部分来做nsp任务。<br>在mask选择上，选择动态mask，即每个epoch都重新选择mask的token。</p>
<p>最终结果是如果直接使用<code>[CLS]</code>做最终特征，以上三种都不能带来pair-wise方案的提升，反而会带来不小的降低，猜测原因可能与以上三种方式的nsp任务与当前任务的模式不同，所以反而会引起降低。而在bert 后面接其他层（AttentionPooling1D，DGCNN）后能带来大约一个点左右提升。</p>
<h2><span id="focal-loss">focal loss</span><a href="#focal-loss" class="header-anchor"></a></h2><p>由于针对性回答与非针对性回答在数量上有不小差距，大约3:1，所以也想到尝试在loss上进行调节。<br>最终结果是没有多少提升，最后将普通loss训练后的模型在train data上进行了predict，并借鉴之前<a href="https://xv44586.github.io/2020/10/14/focal-loss/">focal loss</a>中的方式分析了一下，画出对应的难易样本分布。<br><img src="/2020/11/08/ccf-qa/focalloss.png" alt></p>
<p>上图中不难发现其难样本并不多也不聚集，所以focal loss并不能带来提升。</p>
<h2><span id="dui-kang-xun-lian-yu-ti-du-cheng-fa">对抗训练与梯度惩罚</span><a href="#dui-kang-xun-lian-yu-ti-du-cheng-fa" class="header-anchor"></a></h2><p>对抗训练与梯度惩罚也是两种比较有效的提升模型泛化性能的方法。其中对抗采用的FGM。<br>最终实验后发现两者都能带来线上线下的提升，尤其是对抗，最高能提升三个点，不过相同参数下结果也会差二个点左右，所以每个模型都要少不了调参的过程，所以适合后期提高时使用。</p>
<h2><span id="tricks">tricks</span><a href="#tricks" class="header-anchor"></a></h2><p>由于也是第一次做比赛，所以走了不少弯路，也学到了一些trick：</p>
<ol>
<li>对样本进行kfold然后训练，得到k个模型再进行ensemble。其中k从5增加到10，也会有提升。这种方式的好处是可以让更多的数据参与到训练，同时多个模型进行投票，也会带来或多或少的提升。</li>
<li>对数据进行post-training，虽然我的尝试暂时没有起到提升，但是交流时有其他组的同学通过这个方法就达到单模型0.77以上。而我三种方案对比，pet的方式也是最好的，所以也在一定程度上说明这种方式的有效性。</li>
<li>bert后接新的层，如cnn,dgcnn等。虽然bert的特征提取能力强大，但是在bert后面接一些新的层，总能带来一定的提升，尤其是DGCNN。这种方式可以看作是两种模型的stacking，即利用bert做特征提取，后面的模型在其上做下游任务。</li>
<li>不同模型进行ensemble，如将上述三种方案进行ensemble，由于不同模型关注点不同，融合后会带来一定提升。</li>
<li>更大的模型，如bert-xxlarge等。虽然我的显卡没法实验这种方案，但是交流后发现很多同学都是使用的大模型，baseline就可以达到0.77以上了，所以有时候还是需要一些”钞能力”.</li>
<li>数据清洗与增强。交流中有人提到用外部数据做增强，所以如果有能力，先做清洗与增强，结果也会提升很多。</li>
</ol>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>以上就是对当前比赛的一些思考与总结，现在单模型最好的成绩为线上<code>0.7779</code>, 虽然只排到61名，不过鉴于我使用的是base模型，同时也是单模型，没有任何其他后续处理，所以结果感觉还行。后续完赛后如果有新的收获再更新一篇吧。最后，附上暂时排名截图。</p>
<p><img src="/2020/11/08/ccf-qa/leadboard.png" alt></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>QA</tag>
        <tag>CCF</tag>
        <tag>Competition</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Distillation (3) &amp;#58; 看样本下菜的FastBERT</title>
    <url>/2020/09/25/fastbert/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#knowledge-distillation-mu-de">Knowledge Distillation 目的</a></li>
<li><a href="#zen-me-zuo">怎么做</a><ul>
<li><a href="#qu-fen-yang-ben">区分样本</a></li>
<li><a href="#mo-xing-can-shu-gong-xiang">模型参数共享</a></li>
<li><a href="#zheng-ti-jia-gou">整体架构</a></li>
</ul>
</li>
<li><a href="#fu-xian">复现</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>之前Knowledge Distillation 相关的两篇分别介绍了两种知识蒸馏的方式：<a href="https://xv44586.github.io/2020/08/09/bert-of-theseus/">模型替换之bert-of-theseus</a> 和<a href="https://xv44586.github.io/2020/08/31/bert-01/">知识迁移</a>,本篇介绍一种从样本入手的知识蒸馏方法。</p>
<h1><span id="knowledge-distillation-mu-de">Knowledge Distillation 目的</span><a href="#knowledge-distillation-mu-de" class="header-anchor"></a></h1><p>再来看看我们做knowledge distillation 的目的是什么：我们是想要模型即性能好又推理快，那要推理快，我们直接使用一个更小的模型，比如3层的bert就比12层的bert快，那为什么不这么做呢？这是因为直接用3层bert来fine-tuning的结果往往不那么“性能好”，所以他只能满足推理快这一半。<br>所以我们要通过一个teacher 来引导这个小模型，来把“性能好”这个特性补上。</p>
<h1><span id="zen-me-zuo">怎么做</span><a href="#zen-me-zuo" class="header-anchor"></a></h1><p>而一般做KD ，我们往往关注怎么去让student 更好的学习teacher，但是好像没人关注过student 直接fine-tuning 的时候到底有多差？拿文本分类来说，我们用bert-3 在IFLYTEK数据上进行fine-tuning，最终的accuracy 大概在57.9%，而bert-12 大概在60.7%((结果)[<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py])，3层是不如12层，但是差距只有不到3个点，换句不严谨的话说，只有不到3%的数据需要12层的bert才能达到当前最优性能，而大部分样本在前3层就已经能确定了。" target="_blank" rel="noopener">https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py])，3层是不如12层，但是差距只有不到3个点，换句不严谨的话说，只有不到3%的数据需要12层的bert才能达到当前最优性能，而大部分样本在前3层就已经能确定了。</a><br>换成一句我们都能理解的事实描述就是：样本有难易之分，有的样本容易区分，有的样本不容易区分。这时候，如果全部样本都当不容易区分看待，对这部分容易区分的样本来说就是“杀鸡用牛刀”了，那一个简单直观的办法就是，我们“杀鸡时用杀鸡刀，杀牛时用杀牛刀”，即我们按样本难易程度，分别为他们指定不同的模型来分类，简单的样本只需要用小模型，因为他就能得到与大模型一致的结果，而难的样本再用大模型，这样就能“性能好”的同时推理又快了，因为大部分模型只需要小模型推理即可。</p>
<h2><span id="qu-fen-yang-ben">区分样本</span><a href="#qu-fen-yang-ben" class="header-anchor"></a></h2><p>接下来的问题就是我们怎么区分样本是简单样本还是难样本了。这里我们将其换个思路：假如小模型对自己的结果非常有信心（确定），那我们就相信小模型的结果，反之，我们就将样本送进大模型，让大模型来进一步判断。注意，这里如果小模型非常“确定”的将样本给了错误结果，那这个结果也将认为是最终结果，即使这个结果送进大模型有被改正确的可能。那如何判断一个结果的不确定性呢？通常我们用熵来判断一个分布的不确定性，这里也一样。</p>
<h2><span id="mo-xing-can-shu-gong-xiang">模型参数共享</span><a href="#mo-xing-can-shu-gong-xiang" class="header-anchor"></a></h2><p>到了这一步，我们取得了“性能好”又“推理快”的目标了吗？其实还没有，因为此时我们会有多个模型，每个模型对应不同难易程度的样本，这样无疑是将推理从一次变成了多次，那怎么解决呢？我们可以利用上一个小模型的结果而不用再从头算，这样最终的模型就由一系列模型变为一个带有多个分支的大模型，只是每个分支的部分会进行一次判断，如果其结果的不确定性非常低，则直接返回结果而不再往后继续计算。而由于利用了上一层的结果，所以整体的时间上只增加了多个分类器与判断结果置信度的时间，而这个时间相对于其他计算要小的多。</p>
<h2><span id="zheng-ti-jia-gou">整体架构</span><a href="#zheng-ti-jia-gou" class="header-anchor"></a></h2><p>模型整体架构示意图：<br><img src="/2020/09/25/fastbert/fastbert.png" alt></p>
<p>以上就是fastbert 模型的整体思路了。对于fastbert 来说，越靠前的层的性能越好，其推理速度提升的就越大，所以有必要尽量提高前面层的性能。这里就是Knowledge Distillation 的任务了：由于fastbert 本身就是一个12层bert，所以将最后一个分类器作为Teacher Model，然后生成对应的soft labels，然后迁移到fastbert 的每一个分支model上。之前的<a href="https://xv44586.github.io/2020/08/31/bert-01/">实验</a>我们也提到过这种self-distillation 能提高性能，作者这里也是一样的思路。</p>
<h1><span id="fu-xian">复现</span><a href="#fu-xian" class="header-anchor"></a></h1><p>实验代码在<a href="https://github.com/xv44586/Knowledge-Distillation-NLP/blob/master/knowledge_distillation_fastbert.py" target="_blank" rel="noopener">fastbert</a>感兴趣的同学可以看看.不过由于我只会keras(tensorflow)，而tf 这种静态图不好实现这种分支结构，所以我的实验代码其实并没有真的提前终止计算返回结果，暂时没找到更好的实现方式，如果有知道的同学也欢迎告知。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>fastbert从思路上来说，通过对样本进行难易程度进行划分，对样本进行adaptive predict ，但是缺点也比较明显：1. 用确定性来代替难易，中间有不对等会导致较难样本在初期被错分后没有修正对机会；2.其基本假设是易分样本远多于难分样本，否则会使推理速度不降反增。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>芝麻街</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Distillation</tag>
        <tag>FastBERT</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP中的数据增强</title>
    <url>/2020/11/10/eda/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#shu-ju-zeng-qiang">数据增强</a></li>
<li><a href="#bao-chi-yu-yi-shu-ju-zeng-qiang">保持语义数据增强</a><ul>
<li><a href="#hui-yi">回译</a></li>
<li><a href="#sheng-cheng">生成</a></li>
</ul>
</li>
<li><a href="#ju-bu-rao-dong">局部扰动</a><ul>
<li><a href="#tong-yi-ci-ti-huan">同义词替换</a></li>
<li><a href="#cha-ru">插入</a></li>
<li><a href="#shan-chu">删除</a></li>
<li><a href="#hu-huan">互换</a></li>
</ul>
</li>
<li><a href="#zong-jie">总结</a></li>
</ul>
<!-- tocstop -->
<p></div><br>最近有同学问nlp中如何做data augmentation，这篇总结下目前知道的方法。</p>
<h1><span id="shu-ju-zeng-qiang">数据增强</span><a href="#shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>数据增强技术已经是图像领域的标配了，如旋转、镜像、翻转等。由于图像本身的特性，通过这些操作后生成的图像虽然与原始图像不同，但其图像的内容确实基本一致的。所以可以增强模型的鲁棒性和泛化能力。<br>而在NLP领域情况确是不同的，因为NLP中改变一个词有可能变为语义完全想反的句子，比如：<code>“这好吃吧”</code> -&gt; <code>“这好吃吗”</code>.<br>所以，NLP中数据增强主要有两种方式：一种是保持语义的数据增强，一种是可能破坏语义的局部扰动增强。</p>
<h1><span id="bao-chi-yu-yi-shu-ju-zeng-qiang">保持语义数据增强</span><a href="#bao-chi-yu-yi-shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>保持语义的数据增强主要是构造与原句子语义一样的新句子，如回译、生成等。  </p>
<h2><span id="hui-yi">回译</span><a href="#hui-yi" class="header-anchor"></a></h2><p>回译即将句子从当前语种翻译至新的语种，然后再翻译回来，得到语义相同表达不同的句子。如将句子从中文翻译为英文然后再翻译回中文。可以借助各大互联网平台的免费API来完成。除此之外，还可以多翻译几组中间语种，增加其丰富性。  </p>
<h2><span id="sheng-cheng">生成</span><a href="#sheng-cheng" class="header-anchor"></a></h2><p>生成的方式即通过样本构建一个生成模型，生成与样本语义相同的句子。如<a href="https://arxiv.org/abs/1906.06045" target="_blank" rel="noopener">Learning to Ask Unanswerable Questions for Machine Reading Comprehension
</a> 就是通过生成新的问题来做SQuAD2.0. 此外，之前的文章<a href="https://xv44586.github.io/2020/08/22/qa-augmentation/">利用NLG 增强QA 任务性能</a>里也总结了通过生成问题及问题答案对来增强qa模型性能，不熟悉的可以翻看一下。</p>
<p>由于两种方式构造的新句子都是与原句子语义相同的句子，所以，这种方式进行数据增强表达模型偏好是：模型应对于不同表达形式的同一语义的文本具有不变性。</p>
<h1><span id="ju-bu-rao-dong">局部扰动</span><a href="#ju-bu-rao-dong" class="header-anchor"></a></h1><p>局部扰动主要包括同义词替换、插入、删除、互换四种操作，出自论文<a href="http://arxiv.org/abs/1901.11196" target="_blank" rel="noopener">EDA: Easy Data Augmentation Techniques for Boosting Performance on<br>Text Classification Tasks</a>,因为操作简单，所以也叫<code>EDA</code>（Easy Data Augmentation)。下面分别介绍一下这四种策略。</p>
<h2><span id="tong-yi-ci-ti-huan">同义词替换</span><a href="#tong-yi-ci-ti-huan" class="header-anchor"></a></h2><p>从句子中随机找出1个非停用词，并求出其同义词，然后用同义词替换该词，重复n次操作</p>
<h2><span id="cha-ru">插入</span><a href="#cha-ru" class="header-anchor"></a></h2><p>从句子中随机找出一个非停用词，并求出其同义词，然后将同义词插入句子中的一个随机位置，重复n次操作。</p>
<h2><span id="shan-chu">删除</span><a href="#shan-chu" class="header-anchor"></a></h2><p>以概率p，随机删除句子中的每一个单词</p>
<h2><span id="hu-huan">互换</span><a href="#hu-huan" class="header-anchor"></a></h2><p>随机选择句子中的两个词，然后互换其位置，重复n次。<br>此外，论文中给出了替换删除等操作的比例$\alpha$ 与新增句子数量$n$ 的建议值：<br><img src="/2020/11/10/eda/eda.png" alt>  </p>
<p>而现在我们通常都使用bert等transformer模型做下游任务，所以删除操作可以使用padding，即删除token但保留其占位，即保留其位置编码；互换操作可以选择更大的span进行；插入和同义词替换操作也可以尝试从当前句子选择一个词代替同义词等。<br>文章开头我们也提到了，对句子中的词进行改变时，很可能得到语义完全相反的句子，那上面这四种方式为何有效呢？首先，句子中引入的新词都是同义词，所以语义不会发生很大的变换，其次，论文作者通过分析发现，虽然构造的新句子变得可能都不是一个通顺的句子了，但其特征空间分布下的label并没有发散，即经过EDA变换后，原始数据一方面引入了很多噪声，扩大了数据集，同时又保持了原有的标签，因而有效的扩大了样本集的信息容量。<br>此外，上面的方式相当于对模型增加了一个正则约束，其所表达的模型偏好是：模型应该对文本的局部噪声不敏感</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>以上就是当前NLP中常用的几种数据增强方案，尤其在样本不均衡及小样本任务下，数据增强往往能带来非常不错的提升。所以值得尝试。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>EDA</tag>
      </tags>
  </entry>
  <entry>
    <title>跨界之阿尔滋海默病的分类竞赛</title>
    <url>/2020/11/21/ad-dti/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#bi-sai-jie-shao">比赛介绍</a></li>
<li><a href="#shu-ju-te-dian">数据特点</a></li>
<li><a href="#si-lu">思路</a></li>
<li><a href="#jie-lun">结论</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>几周前受小王萌的邀请，参加了天津大学医学部组织的一场关于<a href="https://mp.weixin.qq.com/s/KlJOOvwqXkD1ANoVG2AOuw" target="_blank" rel="noopener">利用DTI影像进行阿尔滋海默病的分类竞赛</a>，结果虽然只得了第四名，但是这也是第一次跨界参加比赛，所以总结一下。</p>
<h1><span id="bi-sai-jie-shao">比赛介绍</span><a href="#bi-sai-jie-shao" class="header-anchor"></a></h1><p>弥散磁共振影像（DTI）在<code>阿尔茨海默病（Alzheimer’s disease, AD）</code>中应用广泛，从DTI影像中提取扩散参数可以用来描述白质结构的完整性，进而显示AD中脑白质的退化模式。利用机器学习的方法可以比较有效的对AD进行诊断和分类。所以比赛的内容是希望选手通过给定的18条主要的脑白质纤维束的扩散指标作为特征，建立并评估出对AD和健康人群的最优分类模型，如果有可能，进一步探索对轻度认知损害患者的预测性能。<br>说成通俗一点就是给定由18个扩散指标组成的一系列特征，然后希望选手通过这些特征来对正常人与病人进行建模，而轻度患者由于比较难区分，所以，如果可能就继续做关于病人/轻度患者/正常人的分类模型。</p>
<h1><span id="shu-ju-te-dian">数据特点</span><a href="#shu-ju-te-dian" class="header-anchor"></a></h1><p>本次的数据虽然是18个扩散指标，实际上拿到的是沿18条脑内主要纤维束上100个点的8种不同的指标，即每个纤维束上有8种不同指标，每个指标是按顺序固定间隔采样的100个点，合起来每个样本有$18 <em> 8 </em> 100 $个 基础特征。而样本包含三个类别才700个左右。所以此次的数据是一个样本少而基础特征特别多。<br>按每一百个点为一个集合，通过简单的统计，大多数均值方差都在0～1之间，也有几个为负值，还有几个为几百的大数。同时有部分数据为nan。</p>
<h1><span id="si-lu">思路</span><a href="#si-lu" class="header-anchor"></a></h1><p>基于上述特点，首先要解决的是样本数量太少的问题，其次需要解决特征太多的问题。<br>针对特征太多的问题，可以看作是高维数据，这里我们采样SVM 与 XGBoost两种方案来做，SVM在高维稀疏数据上效果相对较好，而XGBoost我们将其看作是一个特征选择器（encoder），然后在其后面接一个LR进行分类。<br>此外，特征过多我们也尝试了两种平滑方案，将每一百个点的集合进行降纬。1. 对每一百个点的集合上，每十个点进行求均值方差来代替原始特征.  2. 对全量样本按label 求取其中心点，然后求取其对三个中心点的 <cdoe>Wasserstein Distance.<br>针对样本少的问题，也尝试了两种方案：1. 利用GAN 来生成，这个方案是借鉴之前看到的一篇利用GAN 来做CONV-19 的文章，其中由于样本过少，通过GAN 进行生成新样本后，提高了模型的性能；另一个方案是互换相同label 的样本之间的特征，这个方案是由GAN 联想到的，即GAN 实际上在生成分布类似的特征，然后组合成新样本，与其通过一个模型生成相同分布然后组合，不然直接互换，这样他们一定是同分布的。<br>此外，我们还尝试了利用CNN 来提取样本特征然后进行分类。</cdoe></p>
<h1><span id="jie-lun">结论</span><a href="#jie-lun" class="header-anchor"></a></h1><ol>
<li>SVM上效果实际比XGB 略差一些，所以最后没有采样SVM。</li>
<li>XGB的效果最后，尤其是将其看作特征提取器，在后面接了一个分类器的模型。</li>
<li>平均方差平滑与WD 效果类似，最后保留了WD </li>
<li>GAN 的效果较差，一来是特征过多，二来是对GAN 不怎么熟，不太会调</li>
<li>随机互换特征效果时好时坏，感觉这也正常，毕竟特征之间是有关系的</li>
<li>深度学习在这种数据上基本不收敛</li>
</ol>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本篇主要是总结一下跨界参加的一个脑科学比赛，出发点是觉得如果能有什么好的效果，说不定能做点贡献，实际参加后发现很多领域的问题由于各种各样的限制，实际上应用机器学习时还是有很大难度的。最后，贴一下排名吧。</p>
<p><img src="/2020/11/21/ad-dti/result.png" alt></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>西伯利亚森林猫</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Competition</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning</title>
    <url>/2020/11/23/scl/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#dui-bi-xue-xi">对比学习</a></li>
<li><a href="#dui-bi-xue-xi-de-zi-jian-du-xue-xi">对比学习的自监督学习</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>最近看了两篇关于对比学习（Contrastive Learning）的论文，觉得思路挺有意思，所以总结一下。</p>
<h1><span id="dui-bi-xue-xi">对比学习</span><a href="#dui-bi-xue-xi" class="header-anchor"></a></h1><p>不知道大家有没有小时候抽读课文的回忆，当读到不认识读字时，我们就会猜他的发音。猜的时候呢，通常就是先看看他的组成部分里有没有认识的，如果有，则读认识的部分的发音，即读<code>半边</code>, 如果拆开的部分不是独立的字，那我们就会联想一下有哪些字跟他有共同的部分，然后那些字都是如何发音的。即便到了现在，遇到不认识的字，我依然会使用这种方法去<code>读</code>. 而这背后的原理其实就是我们认为，该字（样本x）与其相似的字（距离更近的样本）具有相同的发音（label）。这样一个没有经过多少学习的<code>模型</code>却在很多时候都是奏效的。<br>而对比学习与这有着相似的含义：我们在学习表示的时候，不需要完完全全的学习样本的细节，只需要让样本相对其正样本的得分远远大于其相对负样本的得分即可：<br>$$<br>s(f(x), f(x^+)) &gt;&gt; s(f(x),f(x^-))<br>$$<br>其中 $f$是映射函数，也是我们的主要学习任务，$s$是一个打分函数，用来衡量样本之间的距离，最常用的如点积。<br>以上就是一个对比学习的框架，而要使用对比学习，核心问题只有两个：</p>
<ol>
<li>如何定义目标函数，上面提到的点积就是一种比较简单通用的方式，此外，还可以用triple-loss等。</li>
<li>如何构造正负样本，正负样本的构造，合理的正负样本，才能让学到的表示更好的作用在下游任务上。</li>
</ol>
<p>此外，如果熟悉Word2Vec的同学也会发现，其实负采样也在对比学习的范畴中，即在使用负采样训练Word2Vec 时，我们的优化目标是让正样本对之间的概率尽可能的大，而其与负样本之间的概率尽可能小，也即拉近正样本对之间的距离。其中定义的目标函数使用的是点积，而正样本的构造是通过窗口大小来选取，负样本的构造即负采样。</p>
<h1><span id="dui-bi-xue-xi-de-zi-jian-du-xue-xi">对比学习的自监督学习</span><a href="#dui-bi-xue-xi-de-zi-jian-du-xue-xi" class="header-anchor"></a></h1><p>目前学习表示的主流是自监督学习，那为了引入对比学习，我们就需要构造样本的正负样本，其中可以将所有非自己的样本都认为是负样本，需要拉远，那如何构造正样本呢？答案是data augmentation（数据增强）。在CV 中通常用旋转、裁剪等</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>辞旧迎新</title>
    <url>/2021/02/19/happy-new-year/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#ci-jiu">辞旧</a></li>
<li><a href="#ying-xin">迎新</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>在新年这个重要的时间节点上，总应该做些总结与计划，来辞旧迎新，然而我总是懒惰加拖延，到了今天初八才正式开写，所以新的一年，第一个flag就是：希望能找到良药医治一下我的拖延症 - -！</p>
<h1><span id="ci-jiu">辞旧</span><a href="#ci-jiu" class="header-anchor"></a></h1><p>2020年是神奇的一年，这一年里发生了很多意想不到的事，用网友们的话讲就是这一年总忙着见证历史了。而需要见证的历史太多，大多数我都没关注，所以只记录一下我自己”见证”的历史。<br>首先是过年回家，刚到家我就去县里的药店屯了口罩和感冒药，之后就是持续至今的新冠，而由于新冠初期，大家对其认识很少，所以限制出行，而我也就因此度过了高中以来在家最久的一次”寒假”–整整三个月，直到四月初，才买到了机票回来上班。<br>在家的三个月，对我最大的帮助可能就是心灵治愈了，<code>人间烟火气，最抚凡人心</code><br>虽然在家陪父母，但是后期还是需要正常打卡的，所以我也顺便研究了一下藏文语言模型和文本纠错，也就是这个时期，让我更坚定的想要一台自己的服务器，来实验想法。于是，回到北京后，我就开始调研硬件相关，然后自己动手搭了一台<a href="https://xv44586.github.io/2020/05/05/make-a-computer/">服务器</a>，这里还要感谢一下银翔老哥，帮我报销了2080Ti的钱，哈哈。<br>有了服务器，我就开始<code>放弃幻想，拥抱transformer </code>, 开始了看论文与写demo 的路程，期间参考<a href="https://github.com/bojone/bert4keras.git" target="_blank" rel="noopener">bert4keras</a> 与 <a href="https://github.com/CyberZHG/keras-bert" target="_blank" rel="noopener">keras-bert</a>, 实现了一下Bert，不过中间有次实验结果有些奇怪，为了查问题，API基本上改成bert4keras一致了。后面一些相transformer相关的实验代码基本也都在<a href="https://github.com/xv44586/toolkit4nlp" target="_blank" rel="noopener">toolkit4nlp</a><br>这里还有一个小插曲，王萌老哥带我打了一个脑科学的比赛，我们俩花了一天最后拿了个铜牌，这也让我了解到在很多领域，对机器学习和深度学习还是运用的相当少的，一方面是因为任务形态的原因，一方面也是这些领域接受新事物比较缓慢可能，所以我认为与机器学习做交叉，还是有很多事可以做的。<br>接着就到了十一月，由于看到群里有人讨论小学生解数学题的比赛，就去看了<a href="https://www.datafountain.cn/" target="_blank" rel="noopener">datafountain</a>，然后就写了<a href="https://www.datafountain.cn/competitions/474/datasets" target="_blank" rel="noopener">房产行业问答匹配比赛</a>的baseline：<a href="https://github.com/xv44586/ccf_2020_qa_match" target="_blank" rel="noopener">ccf_2020_qa_match</a>, 其实比赛到这就结束了，我就去看论文了，然而有时候事情就是这么奇妙，很多群里的同学看到我的baseline 邀请我去组队，后来与南大的”钞人小哥”刘猛组队，最终拿了第一还。不过答辩时发现，在看论文与做实验的纬度上，我们应该也是第一了- - ！无论如何，也要感谢一下刘猛小哥，不是他邀请我，我也不会拿到这个金牌。毕竟baseline 我写过不少，但是真的认真打到最后还是第一次～～</p>
<p>总结一下，去年发生了新冠，直到现在大家还是备受困扰，我个人度过了最爽的一个年假，在NLP 上也有了实足的进步，不过也有很多年初的计划没有完成，比如每周一篇blog，看完花书等。</p>
<h1><span id="ying-xin">迎新</span><a href="#ying-xin" class="header-anchor"></a></h1><p><code>凡事预则立，不预则废。</code>所以，flag还是要立的，新的一年，希望自己更少一点浮躁，做事能更多一点踏实。</p>
<ul>
<li>经典论文还有很多没读，最新的论文又源源不断，所以希望新的一年能找到适合自己的读论文节奏。</li>
<li>一周一篇blog 可能太困难了，希望能一年写三十篇吧就</li>
<li>感情上也要更成熟一些，对未来要有更清晰的规划</li>
<li>有空就多运动</li>
</ul>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>成都海洋馆的可爱白鲸</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title>多任务学习-以天池比赛为例的三种思路</title>
    <url>/2021/03/28/multi-task/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#ti-mu-jian-jie">题目简介</a></li>
<li><a href="#baseline">baseline</a></li>
<li><a href="#yang-ben-qu-fen">样本区分</a></li>
<li><a href="#ruan-qu-fen">软区分</a></li>
<li><a href="#shi-yan-jie-guo">实验结果</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>几个月前曾关注过天池的多任务学习比赛:<a href="https://tianchi.aliyun.com/competition/entrance/531841/information" target="_blank" rel="noopener">NLP中文预训练模型泛化能力挑战赛</a>,本来应该代码写完就应该写这篇总结，不过拖延症患者总是无法按时完成计划！</p>
<h1><span id="ti-mu-jian-jie">题目简介</span><a href="#ti-mu-jian-jie" class="header-anchor"></a></h1><p>题目是希望选手能够通过算法实现泛化能力更强的中文预训练语言模型，来提高下游业务的准确性，为了评测模型的泛化能力，题目给出了三个不同的任务及对应的数据。  </p>
<ul>
<li>OCNLI：是第一个非翻译的、使用原生汉语的大型中文自然语言推理数据集；</li>
<li>OCEMOTION：是包含7个分类的细粒度情感性分析数据集；</li>
<li>TNEWS：来源于今日头条的新闻版块，共包含15个类别的新闻；简单来说就是需要用一个模型，来实现多个</li>
</ul>
<p>此外，最重要的一点，题目要求”单模型”，即一个任务只能有一个预测函数，所有任务只能使用同一个bert，在计算图中只能有一个bert来完成任务，不能集成多个模型进行预测；</p>
<p>在验证泛化能力时强加这个要求是否合理这里不做讨论，不过可以看出，题目是一个多任务学习的题目，不过又与平时常见的多任务学习不太一样：通常的多任务学习是在同一个样本上，同时进行多个目标的优化，如Bert中的MLM和NSP，而此次题目却是在多个样本上进行多个任务的学习，不同的任务间并不共享数据。</p>
<h1><span id="baseline">baseline</span><a href="#baseline" class="header-anchor"></a></h1><p>实现多任务学习最简单的方式就是共享backbone，然后对不同的任务拼接不同的分类器。参考BERT的训练过程，即在transformer last layer后面接task 相关的分类层即可。不过由于不同样本对应不同的任务，此时我们需要针对每个样本指定对应的分类层。由于笔者只会写keras，而由于tf这种静态图，对每个样本进行switch 不同的层不是很方便，所以实现的时候对每个样本都拼接了三个分类层，然后对loss 进行mask，来屏蔽掉非任务部分。<br>具体的实现细节还是看代码，代码：<a href="https://github.com/xv44586/tianchi_multitask/blob/master/baseline.ipynb" target="_blank" rel="noopener">multi-task-baseline</a></p>
<h1><span id="yang-ben-qu-fen">样本区分</span><a href="#yang-ben-qu-fen" class="header-anchor"></a></h1><p>上面的baseline 是通过拼接不同的分类层来完成多任务学习，本质上是希望模型能对不同对任务进行区分，鉴于预训练语言模型的强大，我们还可以在样本上进行区分，让模型”感知”到任务的不同。而样本上进行区分，最简单的方式就是增加一个”任务说明”了。对GPT这种单向语言模型，可以在样本最前面加一个任务相关的字段，如”文本多分类”/“情感分析”，然后将所有任务都转成生成任务。而对bert这种掩码语言模型，也是类似的，即借鉴PET的思路，将所有任务都转成分类任务，并通过不同的pattern 来区分不同的任务，pattern的结果来区分对应的任务的结果。而pet的思路已经写过很多次了，如之前<a href="https://xv44586.github.io/2020/10/25/pet/">PET-文本分类的又一个秒解</a>。<br>不管是GPT还是BERT下，本质都是一样的：借鉴语言模型的生成能力，转化任务类型，对样本增加固定pattern来区分任务类型，最终通过训练语言模型来完成任务。</p>
<h1><span id="ruan-qu-fen">软区分</span><a href="#ruan-qu-fen" class="header-anchor"></a></h1><p>上面样本区分提到将任务转化为语言模型，那更进一步的想，在语言模型（seq2seq）下，多任务学习是如何做的呢？笔者认为，在语言模型下可以将多任务学习类比为有条件的文本生成任务，而不同的任务类型对应着不同的条件，由此就可以借鉴条件本文生成的做法来做多任务了。<br>条件文本生成的做法，笔者了解不多，所以这里直接借鉴苏神<a href="https://kexue.fm/archives/7124" target="_blank" rel="noopener">基于Conditional Layer Normalization的条件文本生成</a>里提到的做法，即把条件融合到Layer Normalization的$β$和$γ$中去。对不同的条件，来调节所有的LayerNormalization层的平移缩放参数，来进行”软区分”。<br>而条件，则可以通过任务类型进行embedding后来表达。对应的实验代码<a href="https://github.com/xv44586/tianchi_multitask/blob/master/tnews_baseline-adapt.ipynb" target="_blank" rel="noopener">multi-task-adapt</a></p>
<h1><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h1><p>上面的三个思路中，笔者没有样本区分的思路进行实验，原因是实在想不到三个长度相同的pattern，所以就没写实验～而另外两个思路，从结果上看效果差别不大，甚至baseline 比软区分还有更优一点点，我想原因可能在于三个不同任务的数据量不同，或三个任务的数据量都偏少，导致conditional学习的不充分，理论上至少应该学到与未加conditional效果持平，即此时conditional无效。具体调优就留给感兴趣的读者了。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>以上就是笔者三个月前针对天池比赛对多任务学习进行的一些思考与总结，不过时间间隔有些久了，难免有些东西忘了或者记错了，如有发现还望提出。<br>PS：拖延症该管管了！！</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>multi-task</tag>
      </tags>
  </entry>
  <entry>
    <title>speed-up</title>
    <url>/2021/08/14/speed-up/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#xun-lian-jia-su">训练加速</a></li>
<li><a href="#hun-he-jing-du">混合精度</a><ul>
<li><a href="#dui-ying-de-ruan-ying-jian-yao-qiu">对应的软硬件要求</a></li>
<li><a href="#ru-he-kai-qi">如何开启</a></li>
<li><a href="#yi-xie-ce-shi-jie-guo">一些测试结果</a></li>
</ul>
</li>
<li><a href="#tui-li-jia-su">推理加速</a><ul>
<li><a href="#ce-shi">测试</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>之前文章中介绍了通过模型压缩来加速其推理速度的主要思路，并就知识蒸馏总结了三篇内容，分别是：<a href="https://xv44586.github.io/2020/08/09/bert-of-theseus/">模型替换之bert-of-theseus</a> 、<a href="https://xv44586.github.io/2020/08/31/bert-01/">知识迁移</a> 和 <a href="https://xv44586.github.io/2020/09/25/fastbert/">看样本下菜的FastBERT</a>。本文总结两种与模型无关的加速方案。</p>
<h1><span id="xun-lian-jia-su">训练加速</span><a href="#xun-lian-jia-su" class="header-anchor"></a></h1><p>训练加速的主要方法包括pipeline 和混合精度，其中pipeline是指通过构造一个input pipeline将数据IO与GPU计算分开，从而避免GPU因IO而空闲，这个问题不是本篇重点，想进一步了解的可以参考<a href="https://zhuanlan.zhihu.com/p/27238630?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=43831500210176" target="_blank" rel="noopener">tensorflow数据读取机制-何之源</a>和<a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/data/Dataset" target="_blank" rel="noopener">Doc-tf-data-dataset</a></p>
<h1><span id="hun-he-jing-du">混合精度</span><a href="#hun-he-jing-du" class="header-anchor"></a></h1><p>混合精度是指训练时在模型中同时使用 16 位和32位浮点类型，从而加快运行速度，减少内存使用的一种训练方法。通过让模型的某些部分保持使用 32 位类型以保持数值稳定性，可以缩短模型的单步用时，而在评估指标（如准确率）方面仍可以获得同等的训练效果。（<a href="https://tensorflow.google.cn/guide/mixed_precision?hl=zh-cn" target="_blank" rel="noopener">tf doc</a>)简单说就是开启混合精度，既能更省显存又能加速训练，保证性能的前提下，偶尔还能提高性能，真是又省又快又好，了解更多混合精度相关知识，可以参考<a href="https://zhuanlan.zhihu.com/p/103685761" target="_blank" rel="noopener">浅谈混合精度训练</a>.<br>然而很早之前笔者就知道开启混合精度的好处了，在pytorch 下有<a href="https://github.com/NVIDIA/apex" target="_blank" rel="noopener">apex</a> 可以很方便的开启，但是在keras（tensorflow 1.x） 下尝试了多次，也没能找到正确的方法，最近笔者又一次尝试，终于找到了正确的姿势，这里也分享一下。</p>
<h2><span id="dui-ying-de-ruan-ying-jian-yao-qiu">对应的软硬件要求</span><a href="#dui-ying-de-ruan-ying-jian-yao-qiu" class="header-anchor"></a></h2><p>tensorflow要求版本在1.14+ ，对应的显卡需要算力（compute capability）在7及以上, 可以在<a href="https://developer.nvidia.com/cuda-gpus#compute" target="_blank" rel="noopener">cuda-gpus#compute</a>查看对应型号卡的算力</p>
<h2><span id="ru-he-kai-qi">如何开启</span><a href="#ru-he-kai-qi" class="header-anchor"></a></h2><p>对应代码中，只需要增加一行代码，修改一下optimizer即可。不过仍有两点非常需要注意：</p>
<ol>
<li>修改optimizer最好在build model前完成，否则某些情况下可能会报错。 </li>
<li>optimizer需要是tf.train.Optimizer or tf.keras.optimizers.Optimizer继承来的，不支持keras 原生的optimizer。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">...</span><br><span class="line">opt = ...</span><br><span class="line">opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) <span class="comment"># rewrite opt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build_model</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>当在打印信息中看到 <code>tensorflow/core/grappler/optimizers/auto_mixed_precision.cc </code>相关信息，则说明已成功开启混合精度。<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">2021-08-04 07:36:24.231900: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1816] Running auto_mixed_precision graph optimizer</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<h2><span id="yi-xie-ce-shi-jie-guo">一些测试结果</span><a href="#yi-xie-ce-shi-jie-guo" class="header-anchor"></a></h2><p>笔者在V100 下用bert-base 做了部分测试，测试结果如下：</p>
<p><strong>batch_size=32, maxlen=128</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>epoch 1</th>
<th>epoch 2</th>
<th>epoch 3</th>
<th>epoch 4</th>
<th>epoch 5</th>
</tr>
</thead>
<tbody>
<tr>
<td>开启前：</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>277s</td>
<td>246s</td>
<td>244s</td>
<td>246s</td>
<td>244s</td>
</tr>
<tr>
<td></td>
<td>164ms/step</td>
<td>147ms/step</td>
<td>146ms/step</td>
<td>147ms/step</td>
<td>146ms/step</td>
</tr>
<tr>
<td></td>
<td>bset acc:</td>
<td><code>0.57</code> </td>
</tr>
<tr>
<td>开启后：</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>235s</td>
<td>200s</td>
<td>201s</td>
<td>200s</td>
<td>201s</td>
</tr>
<tr>
<td></td>
<td>140ms/step</td>
<td>120ms/step</td>
<td>121ms/step</td>
<td>120ms/step</td>
<td>121ms/step</td>
</tr>
<tr>
<td></td>
<td>best acc:</td>
<td><code>0.576</code></td>
</tr>
</tbody>
</table>
<p><strong>batch_size=64, maxlen=128</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">epoch 1</th>
<th style="text-align:center">epoch 2</th>
<th style="text-align:center">epoch 3</th>
<th style="text-align:center">epoch 4</th>
<th style="text-align:center">epoch 5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">开启前：</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">234s</td>
<td style="text-align:center">202s</td>
<td style="text-align:center">201s</td>
<td style="text-align:center">203s</td>
<td style="text-align:center">203s</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">281ms/step</td>
<td style="text-align:center">242ms/step</td>
<td style="text-align:center">241ms/step</td>
<td style="text-align:center">244ms/step</td>
<td style="text-align:center">244ms/step</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">best acc:</td>
<td style="text-align:center"><code>0.567</code></td>
</tr>
<tr>
<td style="text-align:center">开启后:</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">180s</td>
<td style="text-align:center">141s</td>
<td style="text-align:center">140s</td>
<td style="text-align:center">140s</td>
<td style="text-align:center">141s</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">216ms/step</td>
<td style="text-align:center">169ms/step</td>
<td style="text-align:center">168ms/step</td>
<td style="text-align:center">168ms/step</td>
<td style="text-align:center">169ms/step</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">best acc:</td>
<td style="text-align:center"><code>0.571</code></td>
</tr>
</tbody>
</table>
<p>可以看到，batch size越大，加速比越可观，约能节省1/3的训练时间，同时，性能不会出现明显下降甚至可能也会高一点点。<br>另外，测试使用的bert 是keras 代码，其中有一条日志是<code> converted 1265/17548 nodes to float16 precision</code>，所以约有不到10%的节点使用了半精度？所以猜测使用半精度的节点越多加速比越可观。</p>
<h1><span id="tui-li-jia-su">推理加速</span><a href="#tui-li-jia-su" class="header-anchor"></a></h1><p>推理时通常需要我们提供一个SDK或一个API 服务，这里我们只讨论API 服务的情况。<br>而API 服务通常有两种做法：</p>
<ol>
<li>在server 端load 模型，然后直接预测给出结果；</li>
<li>backend 调用tf-serving ，模型的预测由tf-serving 来提供，其余的（数据的预处理，结果的后处理等）则在backend 端进行。tf-serving 具有热更新，支持多模型多版本，异步调用，高可用等特性，所以也推荐使用tf-serving。使用了tf-serving后，完整的路线变为：<br><strong>client –&gt; backend –&gt; rpc/rest –&gt; tf-serving</strong><br>其中tf-serving 提供了两种形式的api：restful api 和 grpc<br>对应的demo 代码可以查看<a href="https://github.com/xv44586/toolkit4nlp/tree/master/serving" target="_blank" rel="noopener">serving</a></li>
</ol>
<h2><span id="ce-shi">测试</span><a href="#ce-shi" class="header-anchor"></a></h2><h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>可爱修狗</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>speed-up</tag>
      </tags>
  </entry>
  <entry>
    <title>faster-decoder之 decoder解码加速</title>
    <url>/2022/05/23/faster-decoder/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#1-bei-jing">1 背景</a></li>
<li><a href="#2-attention-cache">2 attention cache</a><ul>
<li><a href="#2-1-yuan-li">2.1 原理</a></li>
<li><a href="#encoder-decoder-cross-attention">encoder-decoder cross-attention</a></li>
<li><a href="#self-attention">self-attention</a></li>
<li><a href="#2-2-shi-xian">2.2 实现</a><ul>
<li><a href="#1-attention-ceng-xiu-gai">1. attention 层修改</a></li>
<li><a href="#2-attention-mask-de-jiu-zheng">2. attention mask 的“纠正”</a></li>
<li><a href="#3-position-bias-de-jiu-zheng">3. position bias 的“纠正”</a></li>
<li><a href="#4-jie-ma-shi-xian">4. 解码实现</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3-onnx">3 onnx</a></li>
<li><a href="#4-demo">4 demo</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="1-bei-jing">1 背景</span><a href="#1-bei-jing" class="header-anchor"></a></h1><p>Transformer 模型舍弃了 step by step 的时序循环，依靠 self-attention 的并行结构获得了远超同量级 LSTM 网络的训练速度。即使做auto-regresisve 任务时，通过attention-mask 机制依然可以像encoder 一样并行计算。然而在解码时，却任然需要step by step 的进行，即需要知道上一个time step 的结果后才能进行下一个time step 的解码。此外，通常我们的解码策略是在获得模型结果后在内存中计算的，需要不停的将结果从GPU load 进 CPU 然后计算，这就进一步的拖慢了解码速度。而通常我们在部署时，首选的tf-serving 需要将结果通过网络传输，这将进一步的拖慢解码速度。而针对解码慢的问题，主要的加速方案有：</p>
<ol>
<li>将解码策略放在GPU 上计算，这样将避免结果在GPU/CPU 之间转换与等待；</li>
<li>attention cache，根据attention 层的特点，对attention 中的 $K$ / $V$ 进行cache，避免对之前的time step 进行重复计算，将attention 层的计算由 $O(n^2)$  降低到 $O(n)$。</li>
<li>transformer 计算最耗时的是attention 层中的softmax，尝试使用一些线性函数进行近似替换</li>
</ol>
<h1><span id="2-attention-cache">2 attention cache</span><a href="#2-attention-cache" class="header-anchor"></a></h1><p>三种方案中，GPU 上进行解码需要一些底层技术进行开发，暂时没能力，而替换softmax 方案则会或多或少的损失一些精度，本文都不做进一步讨论。本文聚焦在attention cache 方案上，加速的同时又“不会”损失精度。</p>
<h2><span id="2-1-yuan-li">2.1 原理</span><a href="#2-1-yuan-li" class="header-anchor"></a></h2><p>attention 的计算公式：</p>
<p>$$<br>A = Softmax(QK^{T})* V<br>$$</p>
<p>在解码时，我们是step by step 进行的，所以，我们将时刻 t 的attention 写出来：</p>
<p>$$<br>A = Softmax(Q_{t}K^{T})* V<br>$$</p>
<p>即：对于时刻t 来说，attention 只需要当前的 $Q_{t}$ 时刻信息，$K$ / $V$ 的所有时刻信息进行计算。而 $Q_{t}$ 的计算只需要 $Token_{t-t}$ 即可，如何加速计算的关键就剩下如何更加高效的计算 $K$ / $V$.</p>
<h2><span id="encoder-decoder-cross-attention">encoder-decoder cross-attention</span><a href="#encoder-decoder-cross-attention" class="header-anchor"></a></h2><p>对于encoder-decoder cross-attention 来说，对应的 $K$ / $V$ 都来自encoder 的outputs，所以直接将其整个进行cache 即可，而无需每步都重新计算。</p>
<h2><span id="self-attention">self-attention</span><a href="#self-attention" class="header-anchor"></a></h2><p>而当attention 是self-attention 时，对于时刻 $t$ 来说，此时的 $K$ / $V$ 为 $K$ / $V$ 的前 $t$ 时刻信息，即 $K_{\leq t}$ / $V_{\leq t}$ .此时的 attention 计算为：</p>
<p>$$<br>A = Softmax(Q_{t}K_{\leq t}^{T})* V_{\leq t}<br>$$</p>
<p>而 $K_{t}$/$V_{t}$ 的计算只与 $Token_t$ 有关，与其他时刻的 $Token$ 无关，且不论是时刻 $t$ 还是时刻 $t+1$,对应的 $K_{t-1}$ / $V_{t-1}$ 的计算结果都是一样的。因此，每个时刻都对 $K_{\leq t}$ / $V_{\leq t}$ 全部计算是低效且浪费的。</p>
<p>由于 $K_t$ / $V_t$ 有只需 $Token_t$ 计算且不同时刻结果“一致”的特点，我们将每个时刻的 $K_t$ / $V_t$ 进行cache，在进行attention 计算时使用cache 中的 $K_{\leq t}$ / $V_{\leq t}$<br>即可。<br>此外，由于使用了attention cache 后，每次解码输入只需要 $Token_t$ 而非 $Token_{\leq t}$ ，这样将其他层的计算量也会随之降低。<br>PS：由于decoder 中为了实现auto-regressive 而采用了下三角的attention mask，因此，不同时刻的attention mask 是不同的，这会导致不同时刻的 $K_t$ / $V_t$ 的结果略有不同（约e-10)，但是这并不影响最终端到端的结果。 </p>
<h2><span id="2-2-shi-xian">2.2 实现</span><a href="#2-2-shi-xian" class="header-anchor"></a></h2><p>attention 层在实现时，除了进行attention 计算的同时，还会包含attention mask 和 position bias 两种信息，其中，attention mask 来实现auto regressive，即当前位置的attention 只能包含当前位置及之前的信息；position bias 则包括各种position 信息的实现，所以在使用attention cache 后，还需要对这两种信息进行“纠正”。</p>
<h3><span id="1-attention-ceng-xiu-gai">1. attention 层修改</span><a href="#1-attention-ceng-xiu-gai" class="header-anchor"></a></h3><p>具体实现时，对于encoder-decoder cross-attention, 我们之间将encoder outputs 计算一次后进行cache，每次进行解码时作为inputs 送人decoder；</p>
<p>对于self-attention ，我们在得到 $Q_{t}$/$K_{t}$/$V_{t}$ 后，将 $K_t$/$V_t$ 与之前的 $K_{\leq t-1}$/ $V_{\leq t-1}$ cache 进行拼接，构造出完整的$K_{\leq t}$ / $V_{\leq t}$, 然后将$Q_t$ / $K_{\leq t}$ / $V_{\leq t}$ 进入self-attention 层进行计算。</p>
<h3><span id="2-attention-mask-de-jiu-zheng">2. attention mask 的“纠正”</span><a href="#2-attention-mask-de-jiu-zheng" class="header-anchor"></a></h3><p>由于attention mask 的作用是防止当前位置看到其后位置的信息，而在使用cache 后，当前位置即最后时刻的位置，所以此时的attention mask 已没有存在的必要，直接取消即可；PS: 由于这里直接取消了attention mask，而attention mask 的实现通常是通过加上一个 负无穷(-e12) 来实现的，所以加了cache 后的outputs 与没加之前会有一定的差异，大概在e-10 量级。</p>
<h3><span id="3-position-bias-de-jiu-zheng">3. position bias 的“纠正”</span><a href="#3-position-bias-de-jiu-zheng" class="header-anchor"></a></h3><p>由于position bias 通常是通过inputs 的长度进行计算的，而加了attention cache 后，每次的inputs 的长度变为1 了（当前时刻的$Token_t$），所以此时的position bias 恒等于长度为1 的序列。为了还原他原始的position bias，我们使用拼接了cache 后的$K_{\leq t}$ 进行计算完整序列的position bias， 然后取出当前query 在完整序列中位置对应的position bias 即可。</p>
<h3><span id="4-jie-ma-shi-xian">4. 解码实现</span><a href="#4-jie-ma-shi-xian" class="header-anchor"></a></h3><p>此外，在解码函数上，也需要进行相应的修改，以获得当前时刻的$K_t$/$V_t$ , 然后与之前时刻的所有 $K_{\leq t-1}$ / $V_{\leq t-1}$ cache 进行拼接，为下一个时刻计算做准备。</p>
<h1><span id="3-onnx">3 onnx</span><a href="#3-onnx" class="header-anchor"></a></h1><p>由于tensorflow 会对当前显卡的显存全部占用，所以一个显卡只能启动一个tensorflow 进程，这样就导致当一个模型的显存不需要占用所有显存即可解码时，使用tensorflow 会浪费一部分显存，这里我们将其转为onnx ，这样只需要占用模型需要的显存即可，避免显存浪费。即一个显卡可以起多个解码进程。</p>
<h1><span id="4-demo">4 demo</span><a href="#4-demo" class="header-anchor"></a></h1><p>在bert4keras 的基础上，对 T5/Roformer 进行了实现，具体代码参考：<a href="https://github.com/xv44586/faster-decoder" target="_blank" rel="noopener">faster-decoder</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>网上流传的某个可达鸭形象😄</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>faster decoder</tag>
        <tag>T5</tag>
        <tag>simbert</tag>
      </tags>
  </entry>
  <entry>
    <title>nohup踩坑记</title>
    <url>/2022/07/05/nohup-debug/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#bei-jing">背景</a></li>
<li><a href="#nohup">nohup</a></li>
<li><a href="#ti-dai">替代</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="bei-jing">背景</span><a href="#bei-jing" class="header-anchor"></a></h1><p>最近在测试<a href="https://xv44586.github.io/2022/05/25/horovod/">horovod</a>下的单机多卡/多机多卡的性能，结果遇到一个神奇的现象：训练总是会在一段时间后意外停止，停止后从系统到程序，没有任何异常，没有任何错误日志。而训练停止这个现象总是能复现，但是复现时又不完全一样，即同样的训练脚本，其每次训练意外停止时完成的更新次数还不一样。<br>为了解决这个神奇的bug，我们尝试从物理机、系统资源、驱动、软件环境、训练脚本、训练数据等多个环节进行检查，均未发现任何问题，最终无意间发现使用xshell （之前使用的WSL2)后训练停止的现象不在发生。最终发现竟然是因为对nohup 的坑，特此记录一下。</p>
<h1><span id="nohup">nohup</span><a href="#nohup" class="header-anchor"></a></h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup is a POSIX command which means &quot;no hang up&quot;. Its purpose is to execute a command such that it ignores the HUP (hangup) signal and therefore does not stop when the user logs out.</span><br></pre></td></tr></table></figure>
<p>上面这段是引自<a href="https://en.wikipedia.org/wiki/Nohup#Overcoming_hanging" target="_blank" rel="noopener">wikipedia</a>，即nohup 的作用是忽略HUP 信号，从而让用户log out 时，程序依然保持执行而不中断。</p>
<p>然而有些场景下，nohup 依然会失效，即使用了nohup 后程序依然可能被中断，如<a href="https://unix.stackexchange.com/questions/420594/why-process-killed-with-nohup" target="_blank" rel="noopener">hy-process-killed-with-nohup</a>提到的两个场景，此外，通常我们使用nohup 的场景时ssh 下，而ssh 为了避免丢失数据，会拒绝log out。 参考wiki中提到的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Note that nohupping backgrounded jobs is typically used to avoid terminating them when logging off from a remote SSH session. A different issue that often arises in this situation is that ssh is refusing to log off (&quot;hangs&quot;), since it refuses to lose any data from/to the background job(s).[6][7] This problem can also be overcome by redirecting all three I/O streams:</span><br><span class="line"></span><br><span class="line">$ nohup ./myprogram &gt; foo.out 2&gt; foo.err &lt; /dev/null &amp;</span><br></pre></td></tr></table></figure></p>
<p>我的训练脚本使用时，对stdout/stderr 进行了重定向，未对stdin 进行重定向，即：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nohup ./myprogram &gt; foo.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<p>最终导致训练在进行一段时间后意外中断而没有任何异常信息。而这个像幽灵一样无声无息的“小问题”让我们花了接近三周时间debug，最终在猜测中定位到nohup 。</p>
<h1><span id="ti-dai">替代</span><a href="#ti-dai" class="header-anchor"></a></h1><p>由上面可以看出，nohup 并不是一个”好” 的后台执行解决方案，而对应的替代品，推荐使用<a href="https://github.com/tmux/tmux/wiki" target="_blank" rel="noopener">tmux</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tmux is a terminal multiplexer. It lets you switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them to a different terminal.</span><br></pre></td></tr></table></figure></p>
<p>tmux稳定可靠，利用tmux，将session 与 terminal 进行detach，从而让程序在后台执行。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>深度debug</p>
]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>horovod</tag>
        <tag>nohup</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title>训练加速篇（3）horovod之多机多卡</title>
    <url>/2022/07/06/horovod-multi-nodes/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#horovod-duo-ji-duo-qia">horovod 多机多卡</a><ul>
<li><a href="#huan-jing-da-jian">环境搭建</a></li>
<li><a href="#rdma">RDMA</a></li>
<li><a href="#qi-dong-xun-lian">启动训练</a></li>
</ul>
</li>
<li><a href="#xing-neng-dui-bi">性能对比</a></li>
<li><a href="#ref">ref</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="horovod-duo-ji-duo-qia">horovod 多机多卡</span><a href="#horovod-duo-ji-duo-qia" class="header-anchor"></a></h1><p><a href="https://xv44586.github.io/2022/05/25/horovod/">上一篇</a> 中介绍了如何在单机多卡下使用horovod 进行训练，本篇介绍如何在多机多卡下使用horovod 进行训练。<br>这篇中的测试GPU 为V100， 上篇A100 中遇到的环境问题在V100 中全都没有了，所以整个环境的搭建就异常简单了。</p>
<h2><span id="huan-jing-da-jian">环境搭建</span><a href="#huan-jing-da-jian" class="header-anchor"></a></h2><p>拉取最新的ngc 中的image，加载镜像并在container 中配置互相免密登陆，注意docker 启动时需要加入<code>privileged</code>权限，以便docker能够访问RDMA网口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -itd --rm --gpus all --shm-size=32g --<span class="built_in">ulimit</span> memlock=-1 --<span class="built_in">ulimit</span> stack=67108864 --net=host --privileged -v /data:/data --name horovod tensorflow:22.06-tf1-py3</span><br></pre></td></tr></table></figure></p>
<p>容器内互相免密<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 允许root 使用ssh</span></span><br><span class="line">sed -i <span class="string">'s/#PermitRootLogin prohibit-password/PermitRootLogin yes/'</span> /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改容器内 ssh 默认端口为2222，防止与 host 所使用的22端口冲突</span></span><br><span class="line">sed -i <span class="string">'s/#Port 22/Port 2222/'</span> /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启ssh 服务</span></span><br><span class="line">service ssh restart &amp;&amp; netstat -tulpn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 root 密码</span></span><br><span class="line">passwd root</span><br><span class="line"></span><br><span class="line"><span class="comment"># SSH Key</span></span><br><span class="line">ssh-keygen</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 ~/.ssh/config，并添加以下内容后，保存并退出，完成 host alias 配置。</span></span><br><span class="line"><span class="comment"># ！注意：</span></span><br><span class="line"><span class="comment"># 如果是CVM机型，则ip是两台机器`ifconfig eth0`显示的ip</span></span><br><span class="line"><span class="comment"># 如果是黑石RDMA机型，则ip是两台机器`ifconfig bond0`显示的ip</span></span><br><span class="line">Host gpu1</span><br><span class="line"> hostname 172.0.0.1</span><br><span class="line"> port 2222</span><br><span class="line">Host gpu2</span><br><span class="line"> hostname 172.0.0.2</span><br><span class="line"> port 2222</span><br></pre></td></tr></table></figure></p>
<h2><span id="rdma">RDMA</span><a href="#rdma" class="header-anchor"></a></h2><p>上面提到了RDMA，这里简单介绍一下。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在数据中心领域，远程直接内存访问（英语：remote direct memory access，RDMA）是一种绕过远程主机操作系统内核访问其内存中数据的技术，由于不经过操作系统，不仅节省了大量CPU资源，同样也提高了系统吞吐量、降低了系统的网络通信延迟，尤其适合在大规模并行计算机集群中有广泛应用。</span><br></pre></td></tr></table></figure></p>
<p>这段话引自wiki，通过使用RDMA技术，可以进一步提高分布式系统的整体性能。而我们使用的NCCL 进行通信，NCCL 是支持RDMA的。此外，我们使用的ngc 中是包含了RDMA 驱动的，如果image 内未安装，参考<a href="https://cloud.tencent.com/document/product/1573/74101" target="_blank" rel="noopener">容器安装用户态 RDMA 驱动</a></p>
<h2><span id="qi-dong-xun-lian">启动训练</span><a href="#qi-dong-xun-lian" class="header-anchor"></a></h2><p>启动训练时，需要根据节点信息和通信方案调整参数，如在支持RDMA 下的启动命令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mpirun -np 16 -H gpu1:8,gpu2:8 --allow-run-as-root -<span class="built_in">bind</span>-to none -map-by slot -x NCCL_DEBUG=INFO -x NCCL_IB_DISABLE=0 -x NCCL_SOCKET_IFNAME=bond0 -x NCCL_IB_GID_INDEX=3 -x NCCL_NET_GDR_LEVEL=0 -x LD_LIBRARY_PATH -x PATH -mca pml ob1 -mca btl_tcp_if_include bond0 -mca btl ^openib python3 train.py</span><br></pre></td></tr></table></figure></p>
<p>其中 ：<br><code>-H</code> 后面指定节点及显卡数；<br><code>-np</code> 需要根据 <code>-H</code> 调整为其总计worker 数量；<br><code>btl_tcp_if_include</code> RDMA 下为bond0，普通网络则为eth0；<br><code>NCCL_SOCKET_IFNAME</code> 为网络接口，RDMA 下为bond0，普通网络则需切换为 eth0<br><code>NCCL_NET_GDR_LEVEL</code> 为GDR 相关，GDR的概念是运行GPU 与RDMA 直接通信，开启后进一步降低通信延迟。不过GDR需要配合RDMA 一起使用；</p>
<p>NCCL 相关变量含义可以参考<a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html" target="_blank" rel="noopener">Environment Variables</a></p>
<h1><span id="xing-neng-dui-bi">性能对比</span><a href="#xing-neng-dui-bi" class="header-anchor"></a></h1><p>在bert-base 规模的模型上进行测试，其结果如下：</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th style="text-align:center">batch size per GPU</th>
<th style="text-align:center">net</th>
<th style="text-align:center">node</th>
<th style="text-align:center">speed</th>
</tr>
</thead>
<tbody>
<tr>
<td>A100-40g</td>
<td style="text-align:center">16</td>
<td style="text-align:center">vpc</td>
<td style="text-align:center">single</td>
<td style="text-align:center">430 me/step</td>
</tr>
<tr>
<td>V100</td>
<td style="text-align:center">8</td>
<td style="text-align:center">vpc</td>
<td style="text-align:center">single</td>
<td style="text-align:center">485 ms/step</td>
</tr>
<tr>
<td>V100</td>
<td style="text-align:center">8</td>
<td style="text-align:center">vpc</td>
<td style="text-align:center">multi</td>
<td style="text-align:center">617 ms/step</td>
</tr>
<tr>
<td>V100</td>
<td style="text-align:center">8</td>
<td style="text-align:center">rdma</td>
<td style="text-align:center">single</td>
<td style="text-align:center">485 ms/step</td>
</tr>
<tr>
<td>V100</td>
<td style="text-align:center">8</td>
<td style="text-align:center">rdma</td>
<td style="text-align:center">multi</td>
<td style="text-align:center">510 ms/step</td>
</tr>
</tbody>
</table>
<p>可以看到，通过RDMA 进一步降低网络延迟后，多机多卡的加速效果接近线性加速了。如果开启GDR 网络延迟能进一步降低，加速效果应该会更解决线性加速。</p>
<h1><span id="ref">ref</span><a href="#ref" class="header-anchor"></a></h1><p><a href="https://zhuanlan.zhihu.com/p/55142557" target="_blank" rel="noopener">RDMA技术详解（一）：RDMA概述</a></p>
<p><a href="https://zh.m.wikipedia.org/zh-hans/%E8%BF%9C%E7%A8%8B%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE" target="_blank" rel="noopener">远程直接内存访问</a></p>
<p><a href="https://cloud.tencent.com/document/product/1573/74100" target="_blank" rel="noopener">配置容器 SSH 免密访问</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>可爱小猫</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>speed-up</tag>
        <tag>horovod</tag>
      </tags>
  </entry>
  <entry>
    <title>few-shot视角下的fine-tuning</title>
    <url>/2023/02/01/fine-tuning-at-few-shot/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#few-shot">few-shot</a></li>
<li><a href="#fine-tuning">fine-tuning</a><ul>
<li><a href="#supervised-fine-tuning">supervised fine-tuning</a></li>
<li><a href="#instruction-tuning">instruction-tuning</a></li>
<li><a href="#prompt-tuning">prompt tuning</a></li>
<li><a href="#reinforcement-learning-with-human-feedback">Reinforcement learning with human feedback</a></li>
</ul>
</li>
<li><a href="#qi-fa">启发</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>LLM 目前的使用方式主要是zero-shot/few-shot,其是从context中的examle 数量来区分的，如果按这个角度进一步概括目前的fine-tuning 方案，感觉是个有趣的视角。<br><img src="/2023/02/01/fine-tuning-at-few-shot/few-shot.PNG" alt></p>
<h1><span id="few-shot">few-shot</span><a href="#few-shot" class="header-anchor"></a></h1><p>目前LLM 的使用方式主要是zero-shot/few-shot，而通常few-shot 的性能也比zero-shot 要好，而且随着example 的数量的增加，few-shot 的性能也可能进一步提升；zero-shot 时只给出task input 效果可能不佳，通常需要给出对应的task description，而更”精准“ 的task description 通常也能得到更好的zero-shot 效果。<br>所以，提升LLM zero-shot /few-shot 的性能，主要的方式有两个：</p>
<ol>
<li>给出更”好“的task description</li>
<li>给出更多的example</li>
</ol>
<h1><span id="fine-tuning">fine-tuning</span><a href="#fine-tuning" class="header-anchor"></a></h1><h2><span id="supervised-fine-tuning">supervised fine-tuning</span><a href="#supervised-fine-tuning" class="header-anchor"></a></h2><p>对于小模型来说，其in-context learning 能力较弱，性能不理想。为了提升模型的性能，一个可以尝试的方向是增加context 中的example 数量，但是由于模型的context 窗口有限，不支持我们无限制的将更多的example 塞进context 中，所以我们变通一下，采用通过更新参数的方式将example 塞进“context”。此时可以看作是无限example（inf-shot)；</p>
<h2><span id="instruction-tuning">instruction-tuning</span><a href="#instruction-tuning" class="header-anchor"></a></h2><p>通常在supervised fine-tuning 时，我们是将一个任务的example “塞”进context，而instruction-tuning 可以看作是同时将多个任务的example “塞”进context，为了在使用时区分应该当前context 里应该使用哪些example，我们在不同的task 前面增加对应的description，作为判断依据。即：同时将多个任务的多个example 塞进模型的context 中，使用时通过不同的task description 来区分当前context 内的example 应该是哪些。</p>
<h2><span id="prompt-tuning">prompt tuning</span><a href="#prompt-tuning" class="header-anchor"></a></h2><p>由于pretrain model 的任务是预测下一个token，而非处理用户的指令（instruction），为了提升模型zero-shot 的性能，一个可以尝试的方法就是找到模型视角下更好的task description（pattern），prompt-tuning 的思路即通过大量的监督样本，尝试寻找到更适应模型的task description，然后期望这个task description 能提高模型的zero-shot 的性能。</p>
<h2><span id="reinforcement-learning-with-human-feedback">Reinforcement learning with human feedback</span><a href="#reinforcement-learning-with-human-feedback" class="header-anchor"></a></h2><p>prompt-tuning 时，我们尝试找到模型视角下更好的task description，而这个方法显然是不利于交互的，更好的方式是让模型理解人类视角下的task description。rlhf 就是按照这个思路，让模型反过来更好的理解人类视角下的task description，使得交互更方便。</p>
<h1><span id="qi-fa">启发</span><a href="#qi-fa" class="header-anchor"></a></h1><p>从这个视角看，我们发现即使是小模型，也是有一定的in-context learning 的能力的，只是不够强，所以我们需要更多的example 他才能发挥出更好的效果；<br>pretrain+fine-tuning 的模式之所以能work，是因为pretrain 后的model 有in-context learning 的能力，in-context learning 并不要求task 的形式与pretrain 的一致，所以我们才能在pretrain 的基础上根据下游任务的不同，来构造不同的fine-tuning 过程，在few-shot 视角下，其对应的是在context 中”塞“进更多的example；<br>in-context learning 的增强，对应的是所需的样本逐渐减少，从supervised fine-tuning 的大量样本到few-shot 的少量样本最终到zero-shot 的不需要提供样本，只需提供任务描述。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文是笔者最近思考pretrain + fine-tuning 模式为什么能work 时，通过在few-shot 视角下的一个解释。通过该思路，笔者尝试将目前的fine-tuning 主流思路统一起来。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>太空视角下的喜马拉雅山脉</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>In-context learning</tag>
        <tag>fine-tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>分词算法综述</title>
    <url>/2019/10/22/cutwords/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#fen-ci-xian-zhuang">分词现状</a></li>
<li><a href="#ji-yu-ci-dian-de-fen-ci">基于词典的分词</a><ul>
<li><a href="#zui-da-pi-pei-fen-ci-suan-fa">最大匹配分词算法</a></li>
<li><a href="#zui-duan-lu-jing-fen-ci-suan-fa">最短路径分词算法</a><ul>
<li><a href="#dijkstra-suan-fa">Dijkstra算法</a></li>
</ul>
</li>
<li><a href="#n-zui-duan-lu-jing-fen-ci-suan-fa">N-最短路径分词算法</a></li>
<li><a href="#ji-yu-n-gram-model-de-fen-ci-suan-fa">基于n-gram model的分词算法</a></li>
</ul>
</li>
<li><a href="#ji-yu-zi-de-fen-ci">基于字的分词</a><ul>
<li><a href="#hmm-mo-xing">HMM模型</a><ul>
<li><a href="#wei-te-bi-suan-fa">维特比算法</a></li>
</ul>
</li>
<li><a href="#crf">CRF</a><ul>
<li><a href="#hmm-yu-crf-de-guan-xi-shi-shi-me">HMM与CRF的关系是什么？</a></li>
<li><a href="#wei-te-bi-suan-fa-yu-liang-zhe-hmm-crf-de-guan-xi-shi-shi-me">维特比算法与两者（HMM/CRF）的关系是什么？</a></li>
</ul>
</li>
<li><a href="#shen-jing-wang-luo-fen-ci-suan-fa">神经网络分词算法</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="fen-ci-xian-zhuang">分词现状</span><a href="#fen-ci-xian-zhuang" class="header-anchor"></a></h1><p>NLP的底层任务由易到难大致可以分为词法分析、句法分析和语义分析。分词是词法分析（还包括词性标注和命名实体识别）中最基本的任务，可以说既简单又复杂。说简单是因为分词的算法研究已经很成熟了，大部分的准确率都可以达到95%以上，说复杂是因为剩下的5%很难有突破，主要因为三点：</p>
<ul>
<li><ol>
<li>粒度，不同应用对粒度的要求不一样，比如“苹果手机”可以是一个词也可以是两个词。</li>
</ol>
</li>
<li><ol start="2">
<li>歧义，比如“下雨天留人天留我不留”。</li>
</ol>
</li>
<li><ol start="3">
<li>未登录词，比如“skrrr”、“打call”等新兴词语。<br>分词算法根据其核心思想主要分为两种，第一种是基于字典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。归根结底，上述两种方法都可以归结为在图或者概率图上寻找最短路径的问题。</li>
</ol>
</li>
</ul>
<h1><span id="ji-yu-ci-dian-de-fen-ci">基于词典的分词</span><a href="#ji-yu-ci-dian-de-fen-ci" class="header-anchor"></a></h1><h2><span id="zui-da-pi-pei-fen-ci-suan-fa">最大匹配分词算法</span><a href="#zui-da-pi-pei-fen-ci-suan-fa" class="header-anchor"></a></h2><p>最大匹配分词寻找最优组合的方式是将匹配到的最长词组合在一起。主要的思路是先将词典构造成一棵Trie树，也称为字典树。<br><img src="/2019/10/22/cutwords/max.jpeg" alt="image.jpeg"> </p>
<p>Trie树由词的公共前缀构成节点，降低存储空间同时提升查找效率。最大匹配分词将句子与Trie树进行匹配，在匹配到根节点后由下一个字重新开始查找。最大匹配又分为前向匹配和后向匹配算法，如上图中，对语句“他说的确实在理”，前向最大匹配的结果“他/说/的确/实在/理，而反向最大匹配结果”他/说/的/确实/在理“。通过词典最大匹配时间效率虽然高，但是效果很差，实际一般很少使用这种方法。</p>
<h2><span id="zui-duan-lu-jing-fen-ci-suan-fa">最短路径分词算法</span><a href="#zui-duan-lu-jing-fen-ci-suan-fa" class="header-anchor"></a></h2><p>最短路径分词算法首先将一句话中的所有词匹配出来（基于词典），构成词图（有向无环图DAG），然后寻找从起始点到终点的最短路径作为最佳组合方式。<br><img src="/2019/10/22/cutwords/min.jpeg" alt="image.jpeg"> </p>
<p>图中所有词的权重都是相等的，所以每条边的权重都是1.<br>DAG图的最短路径问题，可以描述为 在无向图 $G=(V,E)$ 中，假设每条边 $E[i]$ 的长度为 $w[i]$，找到由顶点 V0 到其余各点的最短路径（单源最短路径）。假设源点为S，节点集合为V，终点为E。对于最短路径$P(S,E)$中的中间节点，其源点到其的最短路径也在$P9SE)$内。即：假如$S-&gt;A-&gt;B-&gt;C-&gt;E$是最短路径，那$S-&gt;A-&gt;B$一定是S到B的最短路径，否则，将会存在一点F，使得$d(S-&gt;F-&gt;B)&lt;d(S-&gt;A-&gt;B)$,而最短路径$P1(S,E)=S-&gt;F-&gt;B-&gt;C-&gt;E$将比$P(S,E)$短，从而与假设矛盾。因此，求解DAG可以利用最优子结构，通过贪心或者动态规划来求解。</p>
<h3><span id="dijkstra-suan-fa">Dijkstra算法</span><a href="#dijkstra-suan-fa" class="header-anchor"></a></h3><p>Dijkstra算法本质是贪心算法，每一步求解最短路径节点，然后递推更新源节点到其他节点的距离。<br>算法步骤：</p>
<ul>
<li>a.初始时，S只包含源点，即$S＝{v}$，v的距离为0。U包含除v外的其他顶点，即:U={其余顶点}，若v与U中顶点u有边，则$&lt;u,v&gt;$正常有权值，若u不是v的出边邻接点，则$&lt;u,v&gt;$权值为∞。</li>
<li>b.从U中选取一个距离v最小的顶点k，把k，加入S中（该选定的距离就是v到k的最短路径长度）。</li>
<li>c.以k为新考虑的中间点，修改U中各顶点的距离；若从源点v到顶点u的距离（经过顶点k）比原来距离（不经过顶点k）短，则修改顶点u的距离值，修改后的距离值的顶点k的距离加上边上的权。</li>
<li>d.重复步骤b和c直到所有顶点都包含在S中。<br><img src="/2019/10/22/cutwords/dij.gif" alt="image.gif"> </li>
</ul>
<p>Dijkstra算法的结果为”他/说/的/确实/在理“，可见最短路径分词可以解决大部分问题，但是当最短路径存在多条时，Dijkstra只保存一条，这种策略即缺乏理论依据也对其他路径不公平。</p>
<h2><span id="n-zui-duan-lu-jing-fen-ci-suan-fa">N-最短路径分词算法</span><a href="#n-zui-duan-lu-jing-fen-ci-suan-fa" class="header-anchor"></a></h2><p>N-最短路径分词是对Dijkstra算法的扩展，他在每一步都保存最短的N条路径（beam search），同时记录当前节点的前驱，最后求得最优解时回溯得到最短路径，该算法结果优于Dijkstra，但是时间与空间复杂度上更大。<br><img src="/2019/10/22/cutwords/nshort.png" alt="image.png"></p>
<h2><span id="ji-yu-n-gram-model-de-fen-ci-suan-fa">基于n-gram model的分词算法</span><a href="#ji-yu-n-gram-model-de-fen-ci-suan-fa" class="header-anchor"></a></h2><p>前文中边的权重都是1，但实际中不同的词出现的频率/概率不同，其成词的概率也就相应不同，因此将求解词图最短路径的问题转化为求解最大概率路径问题。即将句子切分为”最有可能的词的组合“。而计算词出现的概率，就需要语料对”语言“进行统计建模。<br>语言模型是对一句话出现的概率进行建模，根据条件概率：<br>p(他说的确实在理) = p(他)p(说|他)p(的|他说)…p(理|他说的确实在)<br>上述计算过于庞大，一般我们采用n-gram来近似，如2-gram。<br>然后我们将语言模型得到的概率分布应用到词图，可以得到词图的概率图 ：</p>
<p><img src="/2019/10/22/cutwords/dag.jpeg" alt="image.jpeg"></p>
<p>利用上面两种求解DAG最短路径的方法进行求解即可。</p>
<h1><span id="ji-yu-zi-de-fen-ci">基于字的分词</span><a href="#ji-yu-zi-de-fen-ci" class="header-anchor"></a></h1><p>与基于词典的分词不同，基于字的分词事先不对句子进行词匹配，而是将分词看成是给句子中的每个字打上标签的序列标注问题，可以看成是对每个字的分类问题。 比如通过4标签来进行标注（single，单字成词；begin，多字词的开头；middle，三字以上词语的中间部分；end，多字词的结尾。均只取第一个字母。），这样，“为人民服务”就可以标注为“sbebe”了。 4标注不是唯一的标注方式，类似地还有6标注，理论上来说，标注越多会越精细，理论上来说效果也越好，但标注太多也可能存在样本不足的问题，一般常用的就是4标注和6标注。</p>
<h2><span id="hmm-mo-xing">HMM模型</span><a href="#hmm-mo-xing" class="header-anchor"></a></h2><p>HMM模型认为在解决序列标注问题时，存在两种序列，一种是观测序列，是人显性观察到的句子，而标签是隐状态序列，即观测状态为X，隐藏状态序列是Y，因果关系是Y-&gt;X.<br>我们用$λ=λ1λ2…λn$表示输入的句子，用$o= o1o2…on$表示对应的label，那最优的输出是什么呢？从概率的角度，我们希望下面的条件概率最大：</p>
<p>$max P(o∣λ)=max P(o1o2…on∣λ1λ2…λn)$<br>也就是，o有很多种可能，但是最优的是o应该是概率最大的o。而上式的概率是关于2n个变量的条件概率，而且不同的输入有不同的n，精确计算基本不太可能，所以，我们为了简化问题，先提出第一个假设：输出只与对应的输入有关。于是，上式就可以简化为：<br>$maxP(o∣λ)=max P(o1∣λ1)P(o2∣λ2)…P(on∣λn)$<br>现在，上式的求解就简单多了，只要让每个<strong>P(ok|λk)</strong>最大，即可得到最大的$P(o|λ)$.<br>上面的方案是一个简化方案，但是完全没有考虑上下文，这样会出现很多不合理的结果，比如按照我们4标注方法，b后只能接m或者e，而上述方案不考虑上下文，就可能出现bb\bs等错误结果。这些都是不合理的结果。而产生这种结果的原因是我们没有考虑label之间的关系，所以，我们需要换个角度，反过来考虑这个问题。<br>首先，利用贝叶斯公式：<br><img src="/2019/10/22/cutwords/bys.png" alt="image.png"></p>
<p>由于λ是给定的输入，那么P(λ)就是常数，我们忽略他。现在，最大化$P(o|λ)~P(o)P(λ|o)$.此时，我们的模型是考虑了输出之间的关系$P(o)$,就可以避免一些不合理的情况的出现。</p>
<p>$P(o)P(λ|o)=P(o1o2…on)P(λ1λ2…λn|o1o2…on)$<br>根据我们第一个假设，我们得到：<br>$P(λ|o)=P(λ1|o1)P(λ2|o2)…P(λn|on)$<br>对于$P(o)$,有<br>$P(o) = P(o1)P(o2|o1)…P(on|o1o2..on-1)$<br>此时，我们提出第二个假设：输出状态只与前一个状态有关。<br>$P(o) = P(o1)P(o2|o1)…P(on|on-1)$<br>此时，<br>$P(o|λ)~P(λ1|o1)P(o2|o1)P(λ2|o2)P(o3|o2)…P(on|on-1)P(λn|on)$<br>其中，$P(λk|ok)$称为发射概率，$P(on|on-1)$称为转移概率。对于不合理的输出序列，如bb，我们可以通过设置其状态转移概率为0来避开。<br>接下来的HMM模型的训练，就是如何求解发射概率与转移概率了。<br>学习训练主要有极大似然估计和 Baum-Welch(前向后向)两种算法，通常用极大似然估计。如果有一批标注好的语料，这俩个概率的估计就非常简单了，但通常我们没有大批标注好的语料，我们可以通过词典和未标注的语料来估计，如果没有语料，我们可以通过词典来估计发射概率，状态转移概率人为给一个。<br>得到了发射概率和转移概率，此时就是计算给定输入的最大概率的输出序列。设输入序列长度为L, 状态（label)有n种，则有 $n^L$种可能的排列方式，此时计算最大概率的复杂度为$O(Ln^L)$，这个量太大了，不可取，于是，维特比算法登场了。</p>
<h3><span id="wei-te-bi-suan-fa">维特比算法</span><a href="#wei-te-bi-suan-fa" class="header-anchor"></a></h3><p>维特比算法是HMM，CRF中的经典解码算法，其核心是通过DP思想，在每个时间点的每个label上存储到该位置的最大概率，将计算复杂度降低为 $O(L^2n)$。其流程图如下:<br><img src="/2019/10/22/cutwords/vtb.jpeg" alt="image.jpeg"></p>
<p>算法流可能不够直观，下面来通过一个例子来更好的理解。<br><img src="/2019/10/22/cutwords/vtb_demo.gif" alt="image.gif"></p>
<p>图中X是观察序列（输入序列），Y是标签矩阵。T1为每个时间点每个label的最大概率，</p>
<p><img src="/2019/10/22/cutwords/ti.png" alt="image.png"></p>
<p>T2为到达该时间点该label最大概率的前驱label（上个时间点上）</p>
<p><img src="/2019/10/22/cutwords/t2.png" alt="image.png"></p>
<p>到达最后一个时间点后，求解最大概率对应的label，然后通过T2回溯，将对应前驱节点加入path直到到达起始时间点，得到的path即为最优解的逆序，反转即可得到最优路径（最大概率的输出标签序列）。</p>
<h2><span id="crf">CRF</span><a href="#crf" class="header-anchor"></a></h2><p>CRF可以看作一个无向图模型，对于给定的标注序列Y和观测序列X，对条件概率$P(Y|X)$进行建模。首先，我们讨论一下有向图与无向图概率模型。</p>
<p>有向图<br><img src="/2019/10/22/cutwords/dg.png" alt="image.png"> </p>
<p>上图是一个广义的有向图，他们的联合概率数学表示为：<br><img src="/2019/10/22/cutwords/px.png" alt="image.png"> </p>
<p>无向图<br><img src="/2019/10/22/cutwords/ug.png" alt="image.png"> </p>
<p>上图是一个广义的无向图，而求解时，需要将其分解为若干个“小团”的联合概率的乘积，而每个小团必须是“最大团”（子图里任意两个节点是相连的，且没有其他节点能加入使他成为跟大的团），则有：</p>
<p><img src="/2019/10/22/cutwords/py.png" alt="image.png"> </p>
<p>其中<br><img src="/2019/10/22/cutwords/zx.png" alt><br>用来归一化，将结果转化为概率。<br>上图的无向图的联合概率表示为：<br><img src="/2019/10/22/cutwords/pyu.png" alt="image.png"> </p>
<p>其中<img src="/2019/10/22/cutwords/c.png" alt="image.png"></p>
<p>是最大团C上随机变量的联合概率，形式一般取指数函数：<br><img src="/2019/10/22/cutwords/yc.png" alt="image.png"> </p>
<p>上式也叫势函数。</p>
<p>无向图的联合概率分布在因子分解下表示为：<br><img src="/2019/10/22/cutwords/pyz.png" alt="image.png"> </p>
<p>上式中化简为各个团的联合概率的乘积由 Hammersly-Clifford law保证。</p>
<p>广义的CRF的定义是： 满足 <img src="/2019/10/22/cutwords/pyc.png" alt><br> 的马尔科夫随机场叫做条件随机场（CRF），一般我们说的是线性链条件随机场，定义为：<br>$P(Y|X) = P(Yv|X, Yv-1, Yv+1)$<br>下面来看看我们在做序列建模时，对应的图结构。<br><img src="/2019/10/22/cutwords/arc.png" alt="image.png"> </p>
<p>上图是一个序列建模(线性链CRF)时，对应结构示意图，为了求解上图中的概率，我们需要对上图进行因子分解，分解成若干个“最大团”，而上图结构中，每个“最大团”都是由一对（Ii~Oi)组成，即每个“最大团”C与位置I对应，c=i，而且线性链CRF满足<br><img src="/2019/10/22/cutwords/pio.png" alt="image.png"><br>，我们带入无向图的联合概率分布的因子分解，对应的CRF建模公式为：<br><img src="/2019/10/22/cutwords/pioz.png" alt="image.png"> </p>
<p>公式内i是对应当前节点对应位置；k表示第几个特征函数，每个特征函数对应一个特征权重$λ_k$, $Z(o)$的作用是归一化为概率。</p>
<p>对于特征函数，我们可以定义两类特征函数：转移特征和状态特征，我们可以将上式展开为：<br><img src="/2019/10/22/cutwords/unroll.png" alt="image.png"> </p>
<p>其中$tj$为转移特征，对应权重为$λj$, $sl$为状态特征，对应权重为$ul$。一般我们不分开看，将对应特征函数统一起来。</p>
<p>CRF学习过程中，需要提前定义特征函数，然后根据样本来学习对应特征的权重，所以CRF中比较困难的点在于需要定义大量的特征函数，特征函数的质量直接影响最后的模型质量。<br>下面借一个列子（CRF++）来说明一下：<br>首先，我们会定义相应的标签数据：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"tags"</span>: [</span><br><span class="line"><span class="string">"B"</span>,</span><br><span class="line"><span class="string">"E"</span>,</span><br><span class="line"><span class="string">"M"</span>,</span><br><span class="line"><span class="string">"O"</span>,</span><br><span class="line"><span class="string">"S"</span>]</span><br></pre></td></tr></table></figure></p>
<p>然后我们会定义特征模版，通过特征模版，会帮助我们产生相应的特征函数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Unigram</span><br><span class="line">U00:%x[-2,0]</span><br><span class="line">U01:%x[-1,0]</span><br><span class="line">U02:%x[0,0]</span><br><span class="line">U03:%x[1,0]</span><br><span class="line">U04:%x[2,0]</span><br><span class="line">U05:%x[-1,0]/%x[0,0]</span><br><span class="line">U06:%x[0,0]/%x[1,0]</span><br><span class="line"># Bigram</span><br><span class="line">B</span><br></pre></td></tr></table></figure></p>
<p>其中，我们定义了7个U系列模板（Unigram）和一个B系列（Bigram）模板，其中U00:%x[-2,0]代表U系列，索引为00，对应特征是当前位置的左侧第二个位置(-2)token和当前位置的token，最后的B代表只有一个B系列特征，即只有当前位置的前一个位置的输出token(标签)与当前位置的token（标签）组成的Bigram一个特征（转移概率）。接下来看看对应学习后输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;feature_func_weight&quot;:&#123;</span><br><span class="line">&quot;U006: 等&quot;:[</span><br><span class="line">0.901,</span><br><span class="line">-0.003,</span><br><span class="line">0.311,</span><br><span class="line">-0.01,</span><br><span class="line">-0.006</span><br><span class="line">],</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面这个代表的是什么呢？就是对应第“006”的“U”系列特征是“等”字时，对应五个（BEMOS)对应得分，通过模板，会对每个字都生成对应的6个特征，每个特征都有对应的特征函数权重。</p>
<h3><span id="hmm-yu-crf-de-guan-xi-shi-shi-me">HMM与CRF的关系是什么？</span><a href="#hmm-yu-crf-de-guan-xi-shi-shi-me" class="header-anchor"></a></h3><p>在介绍两者区别之前，我们先来讨论一下机器学习中的两种不同的模型：生成式模型与判别式模型。<br>在监督学习下，模型可以分为判别模型和生成模型。判别模型通过features对labels刻画边界，即他直接对P(X|Y)进行建模；而生成模型通过训练样本，确定数据的整体分布情况，即他直接对P(X,Y).</p>
<p><img src="/2019/10/22/cutwords/dis.png" alt="image.png"></p>
<p>我们的目标是对$P(o|λ)$进行求解，在HMM中，我们通过贝叶斯公式，将问题转化为：<br><img src="byshmm.png" alt="image.png"></p>
<p>分母是常数，被我们忽略了，本质上是对分子进行建模，即对$P(o,λ)$进行建模，所以是一个生成模型。而CRF，直接对$P(o|λ)$进行建模，是一个判别模型。<br>除此之外，CRF可以解决HMM能解决的一切问题，还能解决许多HMM解决不了的问题。为了说明这一点，我们来看看他们的数学表达形式。HMM模型的数学表达为：<br><img src="/2019/10/22/cutwords/pls.png" alt="image.png"></p>
<p>如果我们将HMM模型取对数，可以得到：<br><img src="/2019/10/22/cutwords/logp.png" alt="image.png"></p>
<p>而CRF的未归一化的log形式（特征函数）的数学表达为：<br><img src="/2019/10/22/cutwords/score.png" alt="image.png"><br>如果将HMM中的log形式看作是CRF中的特征权重，则两者具有相同的形式，即每一个HMM都对应某一个形式的CRF。</p>
<h3><span id="wei-te-bi-suan-fa-yu-liang-zhe-hmm-crf-de-guan-xi-shi-shi-me">维特比算法与两者（HMM/CRF）的关系是什么？</span><a href="#wei-te-bi-suan-fa-yu-liang-zhe-hmm-crf-de-guan-xi-shi-shi-me" class="header-anchor"></a></h3><p>维特比算法是一类问题的解法，而不是固定只与HMM\CRF搭配。CRF与HMM都有两个过程，一个是学习阶段，一个是解码阶段。学习阶段确定模型参数，解码阶段求解最大概率路径。解码过程中，HMM与CRF（线性链CRF）从模型定义上看，每个位置的得分都是依赖前一个位置的信息（标签信息），所以整体是一个递推过程，而这个过程在求解最大概率路径时，如果是穷举，在上文也讨论了，复杂度为$O(L*n**L)$，而维特比算法将复杂度降低为$O(L**2*n)$。所以维特比算法是刚好适用于HMM/CRF解码过程的一种算法。</p>
<h2><span id="shen-jing-wang-luo-fen-ci-suan-fa">神经网络分词算法</span><a href="#shen-jing-wang-luo-fen-ci-suan-fa" class="header-anchor"></a></h2><p>对于序列建模这类问题，RNN有着天然的优势，可以很方便的处理变长输入和序列输入，目前对于序列标注问题，公认效果最好的模型是BiLSTM+CRF,对应的模型结构为：<br><img src="/2019/10/22/cutwords/lstm.png" alt="image.png"></p>
<p>相比于其他模型，BiLSTM可以学习到上下文相关信息，可以很好的解决序列输入问题，但BiLSTM只学习到了输入的上下文信息，对应标注的上下文信息却完全没有用到，所以正如HMM中我们讨论的一样，会出现很多不合理的标签序列，如$b\b\b$这种，所以，在后面接一个CRF来避免这种情况的出现。</p>
<p><strong>参考</strong><br><a href="https://www.cnblogs.com/biyeymyhjob/archive/2012/07/31/2615833.html" target="_blank" rel="noopener">https://www.cnblogs.com/biyeymyhjob/archive/2012/07/31/2615833.html</a><br><a href="https://baike.baidu.com/item/%E8%BF%AA%E6%9D%B0%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95/4049057?fr=aladdin#reference-[1]-1712262-wrap" target="_blank" rel="noopener">https://baike.baidu.com/item/%E8%BF%AA%E6%9D%B0%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95/4049057?fr=aladdin#reference-[1]-1712262-wrap</a><br><a href="https://wenku.baidu.com/view/da6e0921af45b307e8719702.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/da6e0921af45b307e8719702.html</a><br><a href="https://zhuanlan.zhihu.com/p/63087935" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63087935</a><br><a href="https://www.cnblogs.com/pinard/p/7048333.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/7048333.html</a><br><a href="https://www.zhihu.com/question/35866596/answer/236886066" target="_blank" rel="noopener">https://www.zhihu.com/question/35866596/answer/236886066</a><br><a href="https://www.cnblogs.com/pinard/p/7048333.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/7048333.html</a><br><a href="https://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/" target="_blank" rel="noopener">https://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/</a><br><a href="https://www.zhihu.com/question/20279019" target="_blank" rel="noopener">https://www.zhihu.com/question/20279019</a><br><a href="http://taku910.github.io/crfpp/" target="_blank" rel="noopener">http://taku910.github.io/crfpp/</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>摄于北京欢乐谷</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Survey</tag>
      </tags>
  </entry>
  <entry>
    <title>装机指北</title>
    <url>/2020/05/05/make-a-computer/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#ying-jian-pian">硬件篇</a><ul>
<li><a href="#xian-qia">显卡</a></li>
<li><a href="#cpu">CPU</a></li>
<li><a href="#zhu-ban">主板</a></li>
<li><a href="#san-re">散热</a></li>
<li><a href="#cun-chu">存储</a></li>
<li><a href="#dian-yuan-yu-ji-xiang">电源与机箱</a></li>
</ul>
</li>
<li><a href="#an-zhuang">安装</a></li>
<li><a href="#ruan-jian-an-zhuang">软件安装</a><ul>
<li><a href="#xi-tong-an-zhuang">系统安装</a></li>
<li><a href="#an-zhuang-nvidia-qu-dong">安装Nvidia驱动</a></li>
<li><a href="#cuda-de-an-zhuang-yu-xie-zai">CUDA的安装与卸载</a><ul>
<li><a href="#xie-zai-cuda">卸载cuda</a></li>
<li><a href="#an-zhuang-cuda">安装cuda</a></li>
</ul>
</li>
<li><a href="#an-zhuang-cudnn">安装cudnn</a></li>
<li><a href="#python-huan-jing">Python环境</a><ul>
<li><a href="#conda-pei-zhi">conda配置</a></li>
<li><a href="#an-zhuang-jupyter">安装jupyter</a></li>
</ul>
</li>
<li><a href="#an-zhuang-tensorflow-gpu">安装tensorflow-gpu</a></li>
</ul>
</li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>  今年回来，第一件事就是要打造一个自己的实验环境，而这也是我第一次自己从硬件开始，所以前前后后差不多折腾了大半个月，总算是搞定了。为了纪念这次从0开始打造自己的深度学习实验环境，所以写了这篇指北。</p>
<h1><span id="ying-jian-pian">硬件篇</span><a href="#ying-jian-pian" class="header-anchor"></a></h1><p>  关于硬件，最主要的就是显卡、CPU、主板和散热了，接下来一个一个介绍经验。</p>
<h2><span id="xian-qia">显卡</span><a href="#xian-qia" class="header-anchor"></a></h2><p>  显卡的选择主要参考两个维度：1.显存大小；2.浮点计算能力。<br>  显卡主要分AMD与Nvidia系列。因为Nvidia有CUDA加持，加速计算，自然是Nvidia系列了，Nvidia显卡目前针对PC有三个系列：Quadro、GeForce和Tesla。其中Quadro系列是专业绘图，GeoForce是专业游戏显卡（可绘图可计算），而Tesla是专业计算卡。综合考虑价格与 state-of-the-art 的模型对硬件的最低要求，最终选择了2080Ti。关于GPU的选型，可以参考<a href="https://zhuanlan.zhihu.com/p/61411536" target="_blank" rel="noopener">深度学习GPU对比</a><br>  确定了型号，就是出品厂家选择了，主要区别就是公版非公版。公版就是Nvidia自己设计自己出的，而非公就是第三方厂商出的，包括evga，技嘉，微星等等。有一篇2080ti进行深度学习时的显卡性能测评的文章，找不到地址了，结论是evga &gt; Nvidia &gt; others 。evga与Nvidia的主要区别有两个：1.价格上evga略高 10% 左右；2. Nvidia 采用双风扇风冷，evga 采用单风扇风冷-水冷混合。最终选择了公版。</p>
<h2><span id="cpu">CPU</span><a href="#cpu" class="header-anchor"></a></h2><p>  CPU主要有两个系列：Intel 和 AMD 。也是因为主要用来做计算，所以肯定首选 AMD。目前ADM系列顶配是 3990X，但是价格大概在三万左右，太感人了。第二的是 3700X ，京东上一千三上下，那就是他了。</p>
<h2><span id="zhu-ban">主板</span><a href="#zhu-ban" class="header-anchor"></a></h2><p>  CPU确定了，主板型号基本就定了。 主要参考如下图：<br>  <img src="/2020/05/05/make-a-computer/board.png" alt="cpu-主板对应型号参考"><br>  看了一些测评，最终选择了微星<a href="https://item.jd.com/8259910.html" target="_blank" rel="noopener">B450</a></p>
<h2><span id="san-re">散热</span><a href="#san-re" class="header-anchor"></a></h2><p>  散热目前两个方式：风冷和水冷。两者的区别主要参考<a href="https://www.zhihu.com/question/57695465/answer/440467918" target="_blank" rel="noopener">风冷与水冷区别</a><br>  水冷在散热上还是要强一些的（240以上），所以打算试水一款水冷。主要推荐两款：<a href="https://item.m.jd.com/product/100003859323.html?wxa_abtest=o&amp;utm_user=plusmember&amp;ad_od=share&amp;utm_source=androidapp&amp;utm_medium=appshare&amp;utm_campaign=t_335139774&amp;utm_term=Wxfriends&amp;from=singlemessage&amp;isappinstalled=0" target="_blank" rel="noopener">乔思伯光影240</a> <a href="https://item.m.jd.com/product/6454809.html?wxa_abtest=o&amp;utm_user=plusmember&amp;ad_od=share&amp;utm_source=androidapp&amp;utm_medium=appshare&amp;utm_campaign=t_335139774&amp;utm_term=Wxfriends&amp;from=singlemessage&amp;isappinstalled=0" target="_blank" rel="noopener">九州风神水元素240T</a>。最后选择了九州风神水元素，因为买那天乔思伯涨价了～</p>
<h2><span id="cun-chu">存储</span><a href="#cun-chu" class="header-anchor"></a></h2><p>  存储上打算采用 32G + 1T ssd。内存自然上<a href="https://item.jd.com/8391349.html" target="_blank" rel="noopener">金士顿骇客神条</a>, ssd主要参考性价比，最终选择<a href="https://item.jd.com/100002580230.html" target="_blank" rel="noopener">三星1T SSD</a></p>
<h2><span id="dian-yuan-yu-ji-xiang">电源与机箱</span><a href="#dian-yuan-yu-ji-xiang" class="header-anchor"></a></h2><p>  由于目前暂时只插一张显卡，所以电源在 600W 以上即可。选一个品牌比较好的，那就是<a href="https://item.jd.com/6828141.html" target="_blank" rel="noopener">安钛克750</a> 了<br>  至于机箱，主要参考能不能够合理安放显卡、主板以及后续可能的散热。对于我当前的配置，只要是中塔的基本都够插显卡。买个安静低调的，那就他了：<a href="https://item.jd.com/100004999668.html" target="_blank" rel="noopener">爱国者M2</a><br>  最终的配置清单如下：<br>  <img src="/2020/05/05/make-a-computer/list.png" alt="list"></p>
<h1><span id="an-zhuang">安装</span><a href="#an-zhuang" class="header-anchor"></a></h1><p>  安装前，强烈建议多看几期安装教程视频，我主要看的是 B站 的<a href="https://www.bilibili.com/video/BV1vx41187cm" target="_blank" rel="noopener">跟装机猿搞装机</a> 系列。最困难的可能是水冷散热的安装了。由于平台不同，安装方式不同，推荐看对应水冷厂商给的安装视频。我主要参考<a href="https://www.bilibili.com/video/BV1EE411h7R1" target="_blank" rel="noopener">水元素安装教程</a><br>  安装过程大致总结一下：</p>
<ol>
<li>拿出主板和CPU，按照说明书将CPU装到主板上。</li>
<li>将水冷拿出，拿出硅脂，涂抹在CPU上，将冷头上的塑料膜撕掉，安装好支撑后，将水冷头贴在CPU上，安装固定。</li>
<li>拆开机箱侧板，将主板装上，并将水冷的风扇固定在机箱上。注意：风扇上没有挡板的是出风口，一般原则是从机箱内往外吹，注意风向。</li>
<li>插入内存条，固定固态硬盘</li>
<li>将线按说明一个一个插上</li>
<li>寻找机箱上显卡位置，可能需要扣掉挡板铁片。将显卡插入主板，同时固定在机箱上</li>
<li>放入电源，插好对应线<br>主要注意的主要是：1、水冷头上的膜一定要撕去，否则CPU上的热不能很好的传导出来。2、装之前大概看一下机箱各个位置，有些时候因为安装顺序会导致没地方下手，需要卸了重组。</li>
</ol>
<h1><span id="ruan-jian-an-zhuang">软件安装</span><a href="#ruan-jian-an-zhuang" class="header-anchor"></a></h1><h2><span id="xi-tong-an-zhuang">系统安装</span><a href="#xi-tong-an-zhuang" class="header-anchor"></a></h2><p>  系统选Linux的稳定版，所以装了CentOS 7。<br>  1.首先下载centos的镜像文件，外网可能会慢，选择<a href="http://mirrors.aliyun.com/centos/7/isos/x86_64/" target="_blank" rel="noopener">阿里源</a><br>  2.下载相应的刻录工具，插入u盘，将镜像刻录进u盘（刻录时会先格式化u盘，如果是windows系统，u盘默认是FAT32,这种格式下是没法放大于4G的单文件，所以需要进行格式转换：cmd下执行 convert e: /fs:ntfs)<br>  3.插入u盘，开机，引导开机进入u盘（大部分是自动进来），选择 test &amp; install<br>  4.一般安装时，对应的命令行中的目录是错误的，执行会报错：<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ERROR，could not insert <span class="string">'floppy'</span></span><br></pre></td></tr></table></figure></p>
<p>  而无法进入安装界面<br>  此时需要查找u盘对应名字，修改命令：<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span>:/dev &amp; ls</span><br></pre></td></tr></table></figure></p>
<p>  找到一个 <code>s##数字 </code>的串，我的是sdb4 ,然后重启，进入后先按 e 进入编辑模式，修改命令行<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">linuxefi/images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS\x207\x20x86_64 xdriver=vesa nomodeset quiet</span><br><span class="line">linuxefi/images/pxeboot/vmlinuz inst.stage2=hd:/dev/sdb4 xdriver=vesa nomodeset quiet</span><br></pre></td></tr></table></figure></p>
<p>  由于我的显卡是2080Ti,系统自带的nouveau驱动不匹配，需要禁用暂时，所以在后面加上 <code> nouveau.modeset=0 </code><br>  最终修改后完整的命令行为：<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">linuxefi/images/pxeboot/vmlinuz inst.stage2=hd:/dev/sdb4 xdriver=vesa nomodeset quiet nouveau.modeset=0</span><br></pre></td></tr></table></figure></p>
<p>  此时保存后退出，选择test &amp; install 即可进入安装界面<br>  进入后安装，安装时，注意软件选择中选择带网络的，剩下的就是按提示一步一步来即可。</p>
<h2><span id="an-zhuang-nvidia-qu-dong">安装Nvidia驱动</span><a href="#an-zhuang-nvidia-qu-dong" class="header-anchor"></a></h2><p>  1.检查显卡是否正常<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lspci | grep -i nvidia</span><br></pre></td></tr></table></figure></p>
<p>  2.检查驱动版本<br>  添加EIRepo源<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</span><br><span class="line">$ rpm -Uvh http://www.elrepo.org/elrepo-release-7.6-5.el7.elrepo.noarch.rpm</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">3.安装显卡驱动检查包</span><br><span class="line">```bash</span><br><span class="line">$ yum install nvidia-detect</span><br></pre></td></tr></table></figure></p>
<p>  检查驱动版本<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ nvidia-detect -v</span><br></pre></td></tr></table></figure></p>
<p>  此时会得到对应的版本信息，注意那个数字<br>  4.安装编译环境<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) dkms</span><br><span class="line">$ yum -y update //注意这是升级系统</span><br><span class="line">$ yum -y install gcc kernel-devel kernel-headers dkms</span><br></pre></td></tr></table></figure></p>
<p>  5.禁用vouveau<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim /etc/modprobe.d/blacklist-nouveau.conf</span><br><span class="line">    blacklist nouveau</span><br><span class="line">    options nouveau modeset=0</span><br></pre></td></tr></table></figure></p>
<p>  6.重新建立initramfs image文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak</span><br><span class="line">$ dracut /boot/initramfs-$(uname -r).img $(uname -r)</span><br></pre></td></tr></table></figure></p>
<p>  7.reboot</p>
<h2><span id="cuda-de-an-zhuang-yu-xie-zai">CUDA的安装与卸载</span><a href="#cuda-de-an-zhuang-yu-xie-zai" class="header-anchor"></a></h2><h3><span id="xie-zai-cuda">卸载cuda</span><a href="#xie-zai-cuda" class="header-anchor"></a></h3>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo yum remove <span class="string">"*cublas*"</span> <span class="string">"cuda*"</span></span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#removing-cuda-tk-and-driver" target="_blank" rel="noopener">removing-cuda</a></p>
<h3><span id="an-zhuang-cuda">安装cuda</span><a href="#an-zhuang-cuda" class="header-anchor"></a></h3>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget path/to/your-version/install.rpm</span><br><span class="line">sudo rpm -i cuda-repo-rhel7-10-1-local-10.1.105-418.39-1.0-1.x86_64.rpm</span><br><span class="line">sudo yum clean all</span><br><span class="line">sudo yum install cuda</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://developer.nvidia.com/cuda-10.1-download-archive-base?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=CentOS&amp;target_version=7&amp;target_type=rpmlocal" target="_blank" rel="noopener">cuda-download</a><br>注意：一定不要通过浏览器去下载，因为会非常非常非常慢，但是wget大概几分钟就搞定了</p>
<h2><span id="an-zhuang-cudnn">安装cudnn</span><a href="#an-zhuang-cudnn" class="header-anchor"></a></h2><p>  1.下载相应的包 libcudnn*.rpm</p>
<ol start="2">
<li><p>安装包</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -ivh libcudnn7-*.x86_64.rpm</span><br><span class="line">rpm -ivh libcudnn7-devel-*.x86_64.rpm</span><br><span class="line">rpm -ivh libcudnn7-doc-*.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>3.验证</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span>  <span class="variable">$HOME</span>/cudnn_samples_v7/mnistCUDNN</span><br><span class="line"><span class="variable">$make</span> clean &amp;&amp; make</span><br><span class="line">$ ./mnistCUDNN</span><br></pre></td></tr></table></figure>
<p>if: Test passed! 则验证通过<br>参考：<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-linux" target="_blank" rel="noopener">cudnn-install</a></p>
<h2><span id="python-huan-jing">Python环境</span><a href="#python-huan-jing" class="header-anchor"></a></h2><p>python环境采用Anaconda + jupyter notebook</p>
<h3><span id="conda-pei-zhi">conda配置</span><a href="#conda-pei-zhi" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n py3 python=3</span><br><span class="line">activate py3</span><br><span class="line">conda install ipykernel -n py3</span><br><span class="line">python -m ipykernel install --user --name py3 --display-name <span class="string">'py3'</span></span><br></pre></td></tr></table></figure>
<p>删除环境</p>
<figure class="highlight bash"><figcaption><span>jupyter kernelspec remove py3 ```</span></figcaption><table><tr><td class="code"><pre><span class="line">  pip 安装 与修改源</span><br><span class="line">```bash </span><br><span class="line">  conda install pip -n py3</span><br><span class="line">  vim ~/.pip/pip.conf</span><br><span class="line"></span><br><span class="line">  [global]</span><br><span class="line">  index-url=https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<h3><span id="an-zhuang-jupyter">安装jupyter</span><a href="#an-zhuang-jupyter" class="header-anchor"></a></h3><figure class="highlight bash"><figcaption><span>pip install jupyter```</span></figcaption><table><tr><td class="code"><pre><span class="line">  jupyter 作为后台服务器</span><br><span class="line">  1. 添加密码</span><br><span class="line">```bash </span><br><span class="line">  ipython</span><br><span class="line">  from jupyter.auth import passwd</span><br><span class="line">  passwd()</span><br></pre></td></tr></table></figure>
<p>此时会让你输入两次密码，输入后得到一串hash码，保存下来， 回到bash, 添加配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ jupyter notebook --generate-config</span><br><span class="line">$ vim ~/.jupyter/jupyter-notebook-config.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># edit</span></span><br><span class="line"></span><br><span class="line">c.NotebookApp.ip=<span class="string">'*'</span>                                  <span class="comment"># * 代表所有iP都能访问 ，也可以指定ip</span></span><br><span class="line">c.NotebookApp.password = u<span class="string">'sha1:ce...'</span>       <span class="comment"># 刚才复制的那个密文</span></span><br><span class="line">c.NotebookApp.open_browser = False       <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line">c.NotebookApp.port =8888                         <span class="comment">#指定一个端口</span></span><br><span class="line">   </span><br><span class="line">c.NotebookApp.notebook_dir = <span class="string">'/home/user/user1'</span>  <span class="comment">#指定工作空间</span></span><br><span class="line">c.PAMAuthenticator.encoding = <span class="string">'utf8'</span>         <span class="comment">#指定utf-8编码，解决读取中文路径或者文件乱码问题</span></span><br></pre></td></tr></table></figure>
<p>后台运行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ nohup jupyter notebook --allow-root &gt; jupyter.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>  关闭后台运行<br>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># ps -axu | grep jupyter</span></span><br><span class="line"><span class="comment"># kill -9 pid</span></span><br></pre></td></tr></table></figure></p>
<h2><span id="an-zhuang-tensorflow-gpu">安装tensorflow-gpu</span><a href="#an-zhuang-tensorflow-gpu" class="header-anchor"></a></h2><p>  tensorflow-gpu对cuda有版本要求，所以在安装cuda前需要提前查看，确定自己版本。参考官网<a href="https://www.tensorflow.org/install/source" target="_blank" rel="noopener">install</a><br>  <img src="/2020/05/05/make-a-computer/gpu.png" alt="tensorflow-gpu vs cuda"><br>  还记得cuda安装时，给出的卸载方法吗？就是因为装tensorflow-gpu后发现gpu不带动的，后来才发现是cuda版本太高了～<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pip install tensorflow-gpu==2.1</span><br></pre></td></tr></table></figure></p>
<p>  验证<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ipython</span><br><span class="line">  import tensorflow as tf</span><br><span class="line">  tf.config.list_physical_devices(<span class="string">'GPU'</span>)</span><br></pre></td></tr></table></figure></p>
<p>  没有报错则正常</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>  以上就是从零开始搭建搭建一套自己的深度学习实验平台的个人经验，整个过程从硬件到最终的软件适配都走了很多坑，希望能给你一些借鉴。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>  部分硬件盒子</p>
]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>tensorflow-gpu</tag>
        <tag>装机</tag>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Distillation (1) &amp;#58; 模块替换之bert-of-theseus-上篇</title>
    <url>/2020/08/09/bert-of-theseus/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#mo-xing-ya-suo">模型压缩</a><ul>
<li><a href="#jian-zhi">剪枝</a></li>
<li><a href="#liang-hua">量化</a></li>
<li><a href="#zhi-shi-zheng-liu">知识蒸馏</a></li>
<li><a href="#quan-chong-gong-xiang">权重共享</a></li>
<li><a href="#quan-chong-fen-jie">权重分解</a></li>
<li><a href="#mo-xing-ya-suo-de-bi-yao-xing">模型压缩的必要性</a></li>
<li><a href="#bert-of-theseus">Bert of theseus</a></li>
<li><a href="#ju-ti-liu-cheng">具体流程</a></li>
<li><a href="#shi-yan-xiao-guo">实验效果</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>


<blockquote>如果忒修斯的船上的木头被逐渐替换，直到所有的木头都不是原来的木头，那这艘船还是原来的那艘船吗？<br><br>-普鲁塔克</blockquote>

<p>最近遇到一个需要对算法加速的场景，了解到了一个比较简洁实用的方法：<a href="https://arxiv.org/abs/2002.02925" target="_blank" rel="noopener">Bert-of-theseus</a>,了解了原理后参考代码实验后，验证了其有效性，所以总结一下。</p>
<h1><span id="mo-xing-ya-suo">模型压缩</span><a href="#mo-xing-ya-suo" class="header-anchor"></a></h1><p>模型在设计之初都是过参数化的，这是因为模型的参数量与复杂度代表着模型的容量与学习能力，但当我们实际使用时，我们需要更好的部署他（低资源），更快的响应（快速推理），常常需要进行模型压缩。<br>模型压缩就是<code>简化大的模型，得到推理快资源占用低的小模型</code>，而想”即要马而跑又不用吃草”通常是很难的，所以压缩后的模型常常也会有不同程度的牺牲，如模型性能下降。<br>此外，模型压缩是作用在推理阶段，带来的常常是训练时间的增加。<br>模型压缩又分为三种方式：一种是<code>剪枝(Pruning)</code>与<code>量化(Quantization)</code>,一种是<code>知识蒸馏(Knowledge Distillation)</code>,还有一种是<code>权重共享（Sharing）与因数分解（Factorization）</code>。该部分内容推荐一篇博客：<a href="http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html" target="_blank" rel="noopener">All The Ways You Can Compress BERT</a></p>
<h2><span id="jian-zhi">剪枝</span><a href="#jian-zhi" class="header-anchor"></a></h2><p>剪枝技术是通过将大模型中一些”不重要”的连接剪断，得到一个”稀疏”结构的模型。剪枝又分为”结构性剪枝”与”非结构性剪枝”.剪枝可以作用在权重粒度，也可以作用在attention heads / layer粒度上。不过剪枝技术感觉会逐步被<cdoe>NAS（Neural Architecture Search）取。</cdoe></p>
<h2><span id="liang-hua">量化</span><a href="#liang-hua" class="header-anchor"></a></h2><p>量化不改变模型的网络结构，而是改变模型的参数的数据格式，通常模型在建立与训练时使用的是 float32 格式的，量化就是将格式转换为 <code>low-bit</code>, 如 float16 甚至二值化，如此即提速又省显存。</p>
<h2><span id="zhi-shi-zheng-liu">知识蒸馏</span><a href="#zhi-shi-zheng-liu" class="header-anchor"></a></h2><p>知识蒸馏是训练一个小模型(student)来学习大模型(teacher)，由于大模型是之前已经fine-tuning的，所以此时学习的目标已经转换为对应的logit而<br>不再是one-hot编码了，所以student有可能比teacher的性能更好。这样即小又准的模型实在太好了。不过为了达到这样的效果，通常设计小模型时不<br>光要学习大模型的输出，还要学习各个中间层结果，相关矩阵等，这就需要仔细设计模型的结构与loss及loss融合方案了。一种简单的方法是只学习大模型的logit，这与对label做embedding有点类似，不过我没做过实验还。</p>
<h2><span id="quan-chong-gong-xiang">权重共享</span><a href="#quan-chong-gong-xiang" class="header-anchor"></a></h2><p>将部分权重在多个层中共享以达到压缩模型的效果，如ALBERT中共享self-attention中的参数。</p>
<h2><span id="quan-chong-fen-jie">权重分解</span><a href="#quan-chong-fen-jie" class="header-anchor"></a></h2><p>将权重矩阵进行因数分解，形成两个低秩的矩阵相乘的形式，从而降低计算量，如ALBERT中通过将Embedding分解降低参数量。</p>
<h2><span id="mo-xing-ya-suo-de-bi-yao-xing">模型压缩的必要性</span><a href="#mo-xing-ya-suo-de-bi-yao-xing" class="header-anchor"></a></h2><p>看了上面模型压缩的方法，每一个都有种”脱裤子放屁”的感觉，与其训练一个大模型，再费力把它变小，为何不直接开始就弄个小的呢？<br>首先，模型在设计之初是都是会或多或少的过参数化，因为模型的参数量与复杂度代表着模型的容量与学习能力；<br>其次，开始就用一个小模型，那这个小模型也是需要设计的，不能随便拿来一个，而设计一个性能高参数规模小的小模型难度是非常大的，往往是模型小了性能也低了；<br>第三点，大模型压缩后与小模型虽然参数规模相当，但是对应的模型空间并不相同。<br>此外，为了更好的部署，如手机或FPGA等，得到精度更高模型更小(distillation)或者利用硬件加速(low-bit)，模型压缩都是值得试一试的手段。<br>更详细的讨论，可以参考<a href="https://www.zhihu.com/question/303922732" target="_blank" rel="noopener">为什么要压缩模型，而不直接训练一个小的CNN</a></p>
<h2><span id="bert-of-theseus">Bert of theseus</span><a href="#bert-of-theseus" class="header-anchor"></a></h2><p>Bert of theseus 方法属于上面提到的知识蒸馏，知识蒸馏中我们提到，在蒸馏时，我们不光要学习teacher的输出，对中间层我们也希望他们直接尽量相似，<br>那想象一个这种状态对应对理想情况：<code>中间层的结果一致，最终的结果一致</code>,既然我们的期望中间结果一致，那也就意味着两者可以互相替换。<br>正如开头提到的忒修斯之船一样。所以核心思想是：<br><code>与其设计复杂的loss来让中间层结果相似不如直接用小模型替换大模型来训练</code><br>通过复杂loss来达到与中间层结果相似可以看作是一种整体渐进式的逼近，让小模型一点点去学习，而直接替换可以看作是一种简单粗暴的方式，<br>但是他不需要设计各种loss，优化目标也是同一个，就只有一个下游任务相关的loss，突出一个<code>简洁</code>。<br>这就好比高中上学一样，即使花高价也要让孩子去一所好高中，因为学校的”氛围”能让孩子的学习成绩进步，其实是因为周围的孩子带着一起学，<br>弱鸡也能学的比平时更多一点。bert-of-theseus也是类似的道理，跟着大佬（teacher）总比单独fine-tuning效果好。</p>
<h2><span id="ju-ti-liu-cheng">具体流程</span><a href="#ju-ti-liu-cheng" class="header-anchor"></a></h2><p>如果直接将小模型替换大模型，那其实是在对小模型进行微调，与大模型就脱离了，也达不到对应的效果，所以作者采用了一种概率替换的方式。<br>首先呢，想象我们现在已经训练好了一个6层的BERT，我们成为<code>Predecessor（前辈）</code>, 而我们需要训练一个三层的bert，<br>他的结果近似12层BERT的效果，我们成为<code>Successor(传承者)</code>,那 bert-of-theseus的模型结构如<a href="https://spaces.ac.cn/archives/7575" target="_blank" rel="noopener">下图</a>所示：</p>
<p><img src="/2020/08/09/bert-of-theseus/bert-of-theseus.png" alt="bert-of-theseus"></p>
<p>在bert-of-theseus中，首先固定predecessor的权重，然后将6层的Bert分为3个block，每个block与successor的一层对应，训练过程分为两个stage：<br>首先用successor中的层概率替换predecessor中对应的block，在下游任务中直接fine-tuning（只训练successor），<br>然后将successor从bert-of-theseus中分离出来，单独在下游任务中进行fine-tuning，直到指标不再上升。<br>所谓替换，就是输出的替换，在进入下一层前在predecessor和successor的输出中二选一。<br>替换概率作者也给出了两种方式，一种是固定 0.5,一种是线性从0-1,如下图所示：<br><img src="/2020/08/09/bert-of-theseus/figure2.png" alt></p>
<h2><span id="shi-yan-xiao-guo">实验效果</span><a href="#shi-yan-xiao-guo" class="header-anchor"></a></h2><p>实验代码主要参考<a href="https://github.com/bojone/bert-of-theseus" target="_blank" rel="noopener">bert-of-theseus</a>, 实验主要做了三组，一组文本分类两组ner-crf，结果如下：</p>
<p>文本分类：CLUE的iflytek数据集</p>
<p>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 60.11\% &amp; 58.99\%  &amp; 57.96\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 59.6\%  &amp; 59.3\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p>
<p>ner-crf: 公司数据<br>$$<br>\begin{array}{c|c|c}<br>\hline<br>&amp; \text{直接微调} &amp; \text{BERT-of-Theseus}\\<br>\hline<br>\begin{array}{c}\text{层数} \\ \text{效果}\end{array} &amp; \begin{array}{ccc}\text{完整12层} &amp; \text{前6层} &amp; \text{前3层}<br>\\ 97.5\% &amp; 97.0\%  &amp; 96.1\%\end{array} &amp; \begin{array}{cc}\text{6层} &amp; \text{3层} \\ 97.3\%  &amp; 96.6\% \end{array}\\<br>\hline<br>\end{array}<br>$$</p>
<p>可以看到，相比直接那前几层微调，bert-of-theseus的效果确实更好，此外，我还尝试了线性策略的替换概率，效果上差别不大。<br>实验代码：<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_ifytek_bert_of_theseus.py" target="_blank" rel="noopener">classification_ifytek_bert_of_theseus</a><br><a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/sequence_labeling_ner_bert_of_theseus.py" target="_blank" rel="noopener">sequence_labeling_ner_bert_of_theseus</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://github.com/JetRunner/BERT-of-Theseus" target="_blank" rel="noopener">论文原作者配图</a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Distillation</tag>
        <tag>speed-up</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Distillation (2) &amp;#58; 知识迁移</title>
    <url>/2020/08/31/bert-01/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#distilling-knowledge">Distilling Knowledge</a><ul>
<li><a href="#distilling-the-knowledge-in-a-neural-network">Distilling the Knowledge in a Neural Network</a><ul>
<li><a href="#distilling">Distilling</a></li>
</ul>
</li>
<li><a href="#distill-bert">Distill BERT</a></li>
<li><a href="#tinybert">TinyBERT</a><ul>
<li><a href="#you-dian">优点</a></li>
<li><a href="#que-dian">缺点</a></li>
</ul>
</li>
<li><a href="#distilbert">DistilBERT</a><ul>
<li><a href="#you-dian-1">优点</a></li>
</ul>
</li>
<li><a href="#mobilebert">MobileBERT</a><ul>
<li><a href="#you-dian-2">优点</a></li>
<li><a href="#que-dian-1">缺点</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#lun-wen-zong-jie">论文总结</a></li>
<li><a href="#xiang-fa">想法</a></li>
<li><a href="#shi-yan">实验</a><ul>
<li><a href="#teacher-to-student">Teacher-to-Student</a></li>
<li><a href="#student-to-student">student-to-student</a></li>
<li><a href="#normal-noise-training">normal-noise-training</a></li>
</ul>
</li>
<li><a href="#shi-yan-jie-guo">实验结果</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>上篇讨论了<a href="https://xv44586.github.io/2020/08/31/bert-01/">bert-of-theseus</a>，算是一个开篇，本文继续讨论关于模型蒸馏（Distilling Knowledge）及关于BERT模型的知识蒸馏。<br>模型蒸馏的最重要的一个特点就是降低资源使用和加速模型推理速度，而小模型往往性能较低，本文总结一些如何通过蒸馏来使小模型具有更好的性能。</p>
<h1><span id="distilling-knowledge">Distilling Knowledge</span><a href="#distilling-knowledge" class="header-anchor"></a></h1><h2><span id="distilling-the-knowledge-in-a-neural-network">Distilling the Knowledge in a Neural Network</span><a href="#distilling-the-knowledge-in-a-neural-network" class="header-anchor"></a></h2><p>这篇是2015年Hinton发表的,也是我看到的最早提出Knowledge Distillation的<a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">论文</a>。<br>在这篇论文中，Hinton指出one-hot 的label只指示了true label 的信息，但是没有给出negative label 之间、negative 与 true label之间<br>的相对关系，比如：比如现在的任务是给定一个词（比如：苹果），然后判断词对应的类别（电视/手机/水果/汽车），假如现在我们有两个样本：<br>（苹果，[0,0,1,0]）和 （小米，[0,1,0,0]), 而one-hot 形式的label并不能告诉我们，苹果中 label是水果的概率高出label是拖拉机的概率，<br>稍低于是手机的概率，而小米中label是电视的概率稍低于是手机的概率，但是同时要高于是汽车和水果的概率，这些相对关系在one-hot 形式的label中<br>是无法得到的，而这些信息非常重要，有了这些信息，我们可以更容易的学习任务。于是提出了Teacher-Student模式，<br>即用一个大的复杂的模型（也可以是ensemble后的）来先学习，然后得到label的相对关系（logits），然后将学习到的知识迁移到一个小模型（Student）。</p>
<h3><span id="distilling">Distilling</span><a href="#distilling" class="header-anchor"></a></h3><p>具体迁移过程是Student 在进行training 时，除了学习ground truth 外，还需要学习label 的probability（softmax output），但是不是直接学习<br>softmax output，而是学习<code>soften labels</code>，所谓soften labels 即经过<code>Temperature</code> 平滑后的 probability，具体形式：<br>$$<br>q_{i} = \frac{exp(z_{i}/T)}{\sum_{j}^{}exp(z_{j}/T)}<br>$$<br>其中T 越大，对应的probability 越平滑，如下图所示。而平滑probability 可以看作是对soften label的一种正则化手段。<br> <img src="/2020/08/31/bert-01/soften.png" alt></p>
<p>更直观的实验请查阅<a href="https://github.com/xv44586/Knowledge-Distillation-NLP/blob/master/Knowledge_Distillation_From_Scratch.ipynb" target="_blank" rel="noopener">Knowledge Distillation From Scratch</a></p>
<h2><span id="distill-bert">Distill BERT</span><a href="#distill-bert" class="header-anchor"></a></h2><p>看到的第一篇针对BERT 模型做蒸馏的是<a href="http://arxiv.org/abs/1903.12136" target="_blank" rel="noopener">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a>,<br>在这篇论文中，作者延续Hinton 的思路在BERT 上做实验，首先用BERT-12 做Teacher，然后用一个单层Bi-LSTM 做Student，loss 上除了<br>ground truth 外，也选择了使用teacher 的logits，包括Temperature 平滑后的soften labels 的CrossEntropy和 logits 之间的MSE，<br>最后实验验证MSE效果优于CE。<br>此外，由于是从头开始训练Student，所以只用任务相关数据会严重样本不足，所以作者提出了三种NLP的任务无关的data augment策略：</p>
<ol>
<li><strong>mask：随机mask一部分token作为新样本，让teacher去生成对应logits ;</strong></li>
<li><strong>根据POS标签去替换，得到 ”What do pigs eat?” -&gt; “ How do pigs ear?”</strong></li>
<li><strong>n-gram采样：随机选取n-gram，n取[1-5]，丢弃其余部分。</strong></li>
</ol>
<p>在<a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>中曾指出 logits 之间的CrossEntropy是可以看作<br>是MSE 的近似版本，不过这里作者的结论是MSE 更好，此外，由于Hinton 实验时是巨大数据量，所以不存在样本不足的情况，而普通实验时都会遇到<br>迁移时训练样本不足，需要做数据增强的问题。</p>
<h2><span id="tinybert">TinyBERT</span><a href="#tinybert" class="header-anchor"></a></h2><p>TinyBERT 出自<a href="http://arxiv.org/abs/1909.10351" target="_blank" rel="noopener">TinyBERT: Distilling BERT for Natural Language Understanding</a>,由于Transformer 结构<br>在NLP 任务中的强大能力，作者选择用与BERT 同结构的方式做Student，此外，为了提高KD后模型性能，做了更细致的工作：</p>
<ol>
<li><strong>Student选择一个更窄更浅的transformer;</strong></li>
<li><strong>将KD也分为两个阶段：pre-train 和 fine-tuning，并且在两个阶段上都进行KD;</strong></li>
<li><strong>使用了更多的loss：Embedding之间的MSE，Attention Matrix中的logits之间的MSE，Hidden state之间的MSE以及最后的分类层的CE;</strong></li>
<li><strong>为了提高下游任务fine-tuning后的性能，使用了近义词替换的策略进行数据增强.</strong></li>
</ol>
<h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><ol>
<li><strong>6层transformer基本达到了bert-12的性能，并且hidden size更小，实际是比bert-6更小的;</strong></li>
<li><strong>因为有pre-train KD，所以可以拿来当bert 一样直接在下游fine-tuning.</strong></li>
</ol>
<h3><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h3><ol>
<li><strong>由于hidden size的不同，所以为了进行MSE，需要用一个参数矩阵W 来调节，这个参数只在训练时使用，训练完后丢弃，这个矩阵没有任何约束，觉得不优雅;</strong></li>
<li><strong>其次，student model的每一层都需要去学习teacher model的对应的block的输出，如何对不同的层如何设计更好的权重也是一个费力的事；</strong></li>
<li><strong>虽然student的结构也是transformer，但是由于hidden size 不同，没法使用teacher的预训练结果，但是我觉得这里其实可以用降维的方式用<br>teacher的预训练结果，可能不需要pretraining的阶段了也说不定。</strong></li>
</ol>
<h2><span id="distilbert">DistilBERT</span><a href="#distilbert" class="header-anchor"></a></h2><p>DistilBERT 出自<a href="http://arxiv.org/abs/1910.01108" target="_blank" rel="noopener">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>,<br>论文中作者通过调查发现BERT 中的hidden size 的对计算效率的改变比hidden layer nums 的影响小，说白了就是让模型变矮比让模型变瘦效率更高，<br>所以作者使用了一个更矮的BERT来做Student 来迁移BERT 中的知识。由于DistilBERT 是一个与BERT 同结构只是层数更小，所以DistilBERT 可以用<br>BERT 的预训练的权重进行初始化，此外，DistilBERT 是一个与任务无关的模型，即与BERT 一样，可以对很多下游任务进行fine-tuning。<br>由于DistilBERT 与 BERT 的前几层一致，所以loss 的选择上就更多一些，作者选择了triple loss：<br>MLM loss + embedding cosin loss + soften labels cross entropy .s</p>
<h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><p>DistilBERT 做到了与BERT 一样，完全与任务无关，不需要添加额外的Distillation 阶段（添加后结果会更好)。</p>
<h2><span id="mobilebert">MobileBERT</span><a href="#mobilebert" class="header-anchor"></a></h2><p>MobileBERT 出自<a href="http://arxiv.org/abs/2004.02984" target="_blank" rel="noopener">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a>,<br>作者同样采用一个transformer 作为基本结构，但作者认为深度很重要，宽度较小对模型损坏较小，所以整体架构是保持模型深度不变，<br>通过一个矩阵来改变feature size，即bottleneck，在通过在block的前后插入两个bottleneck，来scale feature size。由于<br>MobileBERT太窄太深，所以不好训练，作者提出新的方式，通过一个同深但是更宽的同架构的模型来训练 作为teacher，然后用MobileBERT迁移。<br>loss 设计上主要包括三部分：feature map之间的MSE，Attention logits之间的KL，以及pre-training MLM + pre-training-NSP + pre-training-KD<br>训练策略上，有三种方式：</p>
<ol>
<li><strong>将KD作为附加预训练的附加任务，即一起训练；</strong></li>
<li><strong>分层训练，每次训练一层，同时冻结之前的层；</strong></li>
<li><strong>分开训练，首先训练迁移，然后单独进行pre-training.</strong></li>
</ol>
<p>此外，为了提高推理速度，将gelu 替换为更快的 relu ，LayerNormalization 替换为 更简单的NoNorm，也做了量化的实验。</p>
<h3><span id="you-dian">优点</span><a href="#you-dian" class="header-anchor"></a></h3><ol>
<li><strong>首先mobileBERT容量更小，推理更快，与任务无关，可以当bert来直接在下游fine-tuning，而之前的KD大多数时候需要与任务绑定并使用数据<br>增强，才能达到不错的性能；</strong></li>
<li><strong>论文实验非常详实，包括如何选择inter-block size, intra-block size, 不同训练策略如何影响等;</strong></li>
<li><strong>训练策略上，除了之前的一起训练完，实验了两种新的训练方式，而最终的一层一层的训练与skip connection 有异曲同工的作用：每层都学一小部分<br>内容，从而降低学习的难度；</strong></li>
<li><strong>替换了gelu 和 LayerNormalization,进一步提速.</strong></li>
</ol>
<h3><span id="que-dian">缺点</span><a href="#que-dian" class="header-anchor"></a></h3><ol>
<li>要训练一个IBBERT作为teacher，而这个模型容量与BERT-Large差不多，增加了训练难度.</li>
</ol>
<h1><span id="lun-wen-zong-jie">论文总结</span><a href="#lun-wen-zong-jie" class="header-anchor"></a></h1><p>以上论文的迁移过程其实可以总结为两类：</p>
<ol>
<li><strong>soft label迁移，即主要迁移Teacher 模型最后分类层的logits 及相应的soft label；</strong></li>
<li><strong>feature迁移，即除了最后分类层外，还迁移Teacher 模型中的output/attention/embedding等特征。</strong></li>
</ol>
<p>Student 的选择上，除了自定义外，还可以选择跟Teacher 同结构，而为了降低参数量，可以选择将模型变矮/变窄/减小hidden size 等方式。<br>而为了蒸馏后的模型能更加的general，适应更多的task，就需要迁移更多的信息，设计上也越复杂。</p>
<h1><span id="xiang-fa">想法</span><a href="#xiang-fa" class="header-anchor"></a></h1><p>实际工作上，大多数时候我们都是需要一个task 来做模型，而以上论文中告诉我们，迁移的信息越多，Student 的性能越好。而针对具体task ，我觉得<br>比较简洁有效的一种方式是采用更矮的Teacher 来作为Student ，这样可以直接将Teacher 中的前几层的信息完全迁移过来，然后在object 上，<br>加入迁移Teacher 在train data 上的logits ，这样就可以比较有效的进行蒸馏了。<br>除此之外，让我们换个角度看看为什么logits 能增强Student 模型的性能呢？除了迁移的角度外，其实logits 提供了label<br>更多的信息（不同类别的相对关系），而这个额外信息只要优于随机分布，就能对模型提供更多的约束信息，从而增强模型性能，即当前的模型可以看<br>作是分别拟合ground truth 和 logits的两个模型的<code>ensemble</code>，只不过是两个模型共享参数。<br>上面我们提到只要logits 优于随机，对Student 模型来说就会有所提升，那logits 由谁产生的其实并不重要。所以，我们除了可以用Teacher 产生的<br>logits来增强Student 模型外，我们还可以增强Teacher 模型，或者直接用Student 先学习一下，产生logits，再用Student 去迁移上次产生的logits。<br>想到这里，我不禁的有个大胆的想法：<code>既然我可以一边生成logits， 一边学习logits，那我不是可以持续这个过程，直到模型完全拟合train data，<br>生成的logits退化为one-hot，那此时的模型是不是能得到一个非常大的提升呢？</code></p>
<h1><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h1><p>实验的基本设置是用12层bert 作为Teacher model ，用3层bert 作为Student model 。soften labels 采用Temperature 平滑后的结果，此外，<br>Student model 除了学习 soften labels 的外，也需要学习ground truth。</p>
<h2><span id="teacher-to-student">Teacher-to-Student</span><a href="#teacher-to-student" class="header-anchor"></a></h2><p>Teacher model 在train data 上训练，然后在train data 上生成对应的soften labels，Student model 学习ground truth 和 soften labels。 </p>
<h2><span id="student-to-student">student-to-student</span><a href="#student-to-student" class="header-anchor"></a></h2><p>既然soften labels 是一种对labels 的一种平滑估计，那我们可以用任何方式去估计他，所以这里我们就用student 去做一个估计：<br>student model 在train data 上进行训练，然后在train data 上生成对应的soften labels ，将 student model 利用bert 预训练结果重新初始化，<br>然后去学习ground truth 和 soften labels.</p>
<h2><span id="normal-noise-training">normal-noise-training</span><a href="#normal-noise-training" class="header-anchor"></a></h2><p>既然是对labels 的一个估计，那假如给一个随机的估计，只要保证生成的logits 中true label 对应的值最大，就能对Student 模型进行一定程度的提升：<br>直接在train label 上添加一个normal noise ，然后重新进行平滑后归一，作为soften labels让student model 去学习。</p>
<h1><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h1><p>$$<br>\begin{array}{c|c|c}<br>\hline \\<br>\text{teacher standalone} &amp; \text{student standalone} &amp; \text{teacher-to-student} &amp; \text{teacher-to-teacher} &amp; \text{student-to-student} &amp; \text{normal-noise-student}\\<br>\hline \\<br>60.21\% &amp; 58.14\% &amp; 60.14\% &amp; 60.14\% &amp; 61.07\% &amp; 59.5\%<br>\end{array}<br>$$</p>
<p>从结果中可以看到：</p>
<ol>
<li><strong>优于随机的logits 对Student 模型有一定的提升，估计越准确，提升越高；</strong></li>
<li><strong>越大的模型性能越好;</strong><br>3.<strong>迭代进行logits 的生成与训练不能进一步提高模型性能，原因主要是新的logits 分布相比之前的对模型的提升非常小，此外这个分布也比较容易拟<br>合，所以无法进一步提升。</strong></li>
</ol>
<p>完整实验代码地址<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/distilling_knowledge_bert.py" target="_blank" rel="noopener">distilling_knowledge_bert</a></p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文主要针对目前针对BERT 的知识蒸馏进行了总结，并提出了针对具体任务时可行的简洁方案，同时在新的视角下探讨了知识蒸馏有效的一些原因，<br>并通过实验进行了验证，发表顺序上上篇<a href="https://xv44586.github.io/2020/08/09/bert-of-theseus/">bert-of-theseus</a> 更晚一些，有兴趣的可以再去看一下上一篇。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>芝麻街</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>样本不均衡之难易不均衡</title>
    <url>/2020/10/14/focal-loss/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#cross-entropy">Cross Entropy</a></li>
<li><a href="#yang-ben-lei-bie-bu-jun-heng">样本类别不均衡</a></li>
<li><a href="#focal-loss">Focal Loss</a><ul>
<li><a href="#ru-he-que-ding-alpha-yu-gamma">如何确定$\alpha$ 与 $\gamma$</a></li>
<li><a href="#shi-yan">实验</a></li>
</ul>
</li>
<li><a href="#ghm">GHM</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>上篇<a href="https://xv44586.github.io/2020/09/25/fastbert/">看样本下菜的FastBERT</a>提到样本有难易，通过利用样本的这个特性，可以在推理上进行加速，除了在推理上可以利用，在训练时也可以利用，本篇就来说怎么在训练时更充分的利用样本有难易的特性。</p>
<h1><span id="cross-entropy">Cross Entropy</span><a href="#cross-entropy" class="header-anchor"></a></h1><p>对于分类问题，通常我们选择交叉熵作为损失。本文均针对二分类进行说明，多分类的情况可以横向扩展。对于二分类问题来说，其损失CE：</p>
<p>$$<br>CE = \left\{\begin{matrix}<br> -log(p)&amp; y\_true=1  \\<br> -log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p>
<h1><span id="yang-ben-lei-bie-bu-jun-heng">样本类别不均衡</span><a href="#yang-ben-lei-bie-bu-jun-heng" class="header-anchor"></a></h1><p>当我们遇到一个正负样本不均衡的情况，如1:1000时，直接训练后效果往往不好，其倾向于将更多的样本预测为类别多的类，而产生的原因是：由于我们训练时使用的 CE:<br>$CE_W = CE_positive + CE_negative$, 其中CE_positive 与 CE_negative 分别代表正负样本的loss，而由于此时的样本不均衡，loss主要有类别多的样本贡献，主导了优化方向，所以模型会偏向数量多的方向，如当前全部预测为正样本，那解决这个问题最简单直接的办法就是在loss上增加一个权重α来均衡一下两方的loss，从而让模型更“公平”的对待不同类别样本,即：</p>
<p>$$<br>CE_W = \left\{\begin{matrix}<br> -\alpha log(p)&amp; y\_true=1  \\<br> -(1-\alpha)log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p>
<h1><span id="focal-loss">Focal Loss</span><a href="#focal-loss" class="header-anchor"></a></h1><p>除了在类别上可能存在这种不均衡外，样本在难易程度上往往也会有难易之分。如当训练一个情感分类器时，“不喜欢xx”就比“谁不喜欢xx呢”要容易训练一些. 为了衡量这种“难易”特征，我们定义一个代表预测值与真实label 之间差距的参数$p_t$:</p>
<p>$$<br>p_t = \left\{\begin{matrix}<br> 1-p,&amp; y\_true=1  \\<br> p,&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p>
<p>即</p>
<p>$$<br>p_t = \begin{vmatrix}<br>pred - y_{true}<br>\end{vmatrix}<br>$$</p>
<p>pt越大则说明预测值与其label 相差越大，也即样本越“难训练”，最后我们对整个样本的pt 统计往往得到一个U型分布，如下图所示：<br><img src="/2020/10/14/focal-loss/a1.png" alt><br>即“易训练”样本是“难训练”样本的指数级。虽然此时“易训练”样本由于得到了很好的训练，其loss 很小，当由于其数量庞大，任然可能主动整个训练。<br>所以为了解决难易不均衡的问题，我们采用与样本不均衡一样的方法：对不同样本添加一个权重来平衡，即此时的loss FL:</p>
<p>$$<br>FL = \left\{\begin{matrix}<br> -\alpha \beta(p\_t) log(p)&amp; y\_true=1  \\<br> -(1-\alpha)\beta(1-p\_t)log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p>
<p>而前面我们说难易样本的loss 呈指数级差距，所以此时的$\beta(p_t)$ 我们也定义为指数函数，最终的 FL:</p>
<p>$$<br>FL = \left\{\begin{matrix}<br> -\alpha(1 - p)^{\gamma} log(p)&amp; y\_true=1  \\<br> -(1-\alpha)(p)^{\gamma}log(1-p),&amp; y\_true=0<br>\end{matrix}\right.<br>$$</p>
<p>此时，$\alpha$ 用来平衡样本不均衡，$(1-p)^{\gamma}$ 用来均衡难易样本。通过平衡难易样本对应损失，让模型更“关注”那些难分的样本。</p>
<p>以上就是<a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">focal loss</a> 的主要思想，虽然最后我们得到的loss形式上与focal loss一样，但其中参数的含义与focal loss中的内容却有一些不同，主要在于focal loss 中实验证明，由于对难易样本降权后正样本（量少的类）对应的loss反而更易主动优化方向，所以用 $\alpha$ 来降权，而我们上面提到的alpha 主要是用来均衡正负样本，这里读者可以自行判断理解。此外，苏剑林通过硬截断过渡到软阶段也得到了类似的loss，推荐大家也看看：<a href="https://spaces.ac.cn/archives/4733" target="_blank" rel="noopener">从loss的硬截断、软化到focal loss </a></p>
<h2><span id="ru-he-que-ding-alpha-yu-gamma">如何确定$\alpha$ 与 $\gamma$</span><a href="#ru-he-que-ding-alpha-yu-gamma" class="header-anchor"></a></h2><p>在focal loss论文内，作者是通过搜索一个范围来确定两个参数的最优解，最后给出的结果是 $\alpha = 0.25$, $\gamma=2.$，而通过上面我们提到的两个参数的含义，这里给出一个确定参数范围的方案：<br>1.首先，我们通过统计正负样本，来确定$\alpha$的大致范围；<br>2.通过CE_W我们可以训练一个基础的分类器，通过这个分类器，我们对训练集进行预测，生成对应的prob，然后通过统计$p_t$，我们认为$p_t&lt;=0.1$ 的为主要的“易分样本”，${p_t&gt;=0.9}$ 为主要“难分样本”，由于是指数衰减，所以两者的loss 差距为 $9^\gamma$, 即此时$9^\gamma=C_易/C_难$, 解出此时的$\gamma$ 即可得到其大致的范围。</p>
<h2><span id="shi-yan">实验</span><a href="#shi-yan" class="header-anchor"></a></h2><p>实验时，通过构造一个正负样本8:1的数据集进行实验，在通过权重平衡正负样本不均衡后，对应的pt分布如下图：<br><img src="/2020/10/14/focal-loss/a1.png" alt><br><img src="/2020/10/14/focal-loss/p1.png" alt><br><img src="/2020/10/14/focal-loss/n1.png" alt><br>而focal loss 训练后的pt 分布为：<br><img src="/2020/10/14/focal-loss/a2.png" alt><br><img src="/2020/10/14/focal-loss/p2.png" alt><br><img src="/2020/10/14/focal-loss/n2.png" alt><br>可以看到，在focal loss 下，右侧偏差大的样本基本都被移到了左侧，说明“难样本”大幅度减少变为了“易样本”。<br>实验代码地址：<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_focal_loss.py" target="_blank" rel="noopener">classification use focal loss</a></p>
<h1><span id="ghm">GHM</span><a href="#ghm" class="header-anchor"></a></h1><p>现在让我们来讨论一下focal loss存在的问题：</p>
<ol>
<li>首先，让模型过多的关注那些特别难分的样本没有什么问题，但是这个前提：样本紧凑。而当数据中存在离群点时，那此时就会发生：本来模型已经收敛了，但是由于这些离群点还是会误判，一直存在在pt的最右侧，而让模型再过多的去关注这些点，这明显是不合适的；</li>
<li>对于focal loss中的两个参数$\alpha$ 和$\gamma$ ，虽然我们能估算一个大致范围，但是由于两者是相互影响的，所以实际使用时还是需要通过实验去寻找最优解，这也为训练增加了一定的难度。</li>
</ol>
<p>现在再让我们回过头来重新审视一下我们的原始问题：样本有难易之分，所以训练时存在难易样本不均衡，而”易分”样本占比过高导致主导优化方向。那此时让我们往后再思考一步，当我们对易分样本降权后，对应的pt分布图中最左侧的柱子会降低，而由于模型得到了更好的优化方向，模型的性能提高，所以最右侧的柱子也会降低，两边减少的样本会同时向”中间”扩散，最后得到一个比原始pt分布曲线更”平滑”的分布曲线，正如上文中focal loss对应的pt分布图。<br>而focal loss 由于过度关注”难分样本”，导致存在离群点时不理想的问题。而离群点有一个特点就是：量相对正常样本非常少（否则就是一个”小群”了），利用这个特点，我们就能对focal loss 进行改进了。改进的思路就是利用离群点少的特点，从难易样本的量上来平衡难易样本的loss。<br>具体做法：我们将$p_t$ 按间隔$\varepsilon$均等的分为K个区间，然后统计不同区间内的样本数量$num_k$，然后针对每个区间内的loss 我们用参数$\beta(i)$ 来平滑：</p>
<p>$$<br>L_{GHN-C} = \sum_{1}^{N}\beta(i)L_{CE}(p_i,  \hat p_i)<br>$$</p>
<p>其中：<br>$\beta(i)$对应$pt_i$所属区间的样本$num_k$在整体样本 $N$ 中占比的倒数。<br>而在实现时，由于通常我们都是采取mini-batch 的方式训练，无法在每个batch内事先得到全局统计量进行$\beta(i)的计算，一种近似的办法是利用动量，逐步近似求。<br>以上就是<a href="http://arxiv.org/abs/1811.05181" target="_blank" rel="noopener">GHM</a> 在分类情况下的loss，原始论文中的pt 分布对比图中也能看到，利用GHM确实更平滑。<br><img src="/2020/10/14/focal-loss/gn.jpeg" alt> </p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文介绍了两种针对样本难易不均衡问题的loss：focal loss 与 GHM，并通过实验进一步验证了其有效性，在一些样本不均衡的场景下均可尝试使用。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p><a href="https://arxiv.org/pdf/1811.05181.pdf" target="_blank" rel="noopener">Gradient Harmonized Single-stage Detector</a>中配图。</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>样本不均衡</tag>
        <tag>Loss</tag>
      </tags>
  </entry>
  <entry>
    <title>如何提升bert在下游任务中的性能</title>
    <url>/2020/11/24/fine-tune/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#further-pre-training">Further Pre-training</a><ul>
<li><a href="#er-jie-duan-vs-san-jie-duan-vs-si-jie-duan">二阶段 vs 三阶段 vs 四阶段</a><ul>
<li><a href="#san-jie-duan">三阶段</a></li>
<li><a href="#si-jie-duan">四阶段</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#ru-he-further-pre-training">如何further pre-training</a><ul>
<li><a href="#how-to-mask">how to mask</a></li>
<li><a href="#when-to-stop">when to stop</a></li>
<li><a href="#how-to-fine-tuning">how to fine-tuning</a><ul>
<li><a href="#optimizer">optimizer</a></li>
<li><a href="#learning-rate">learning rate</a></li>
<li><a href="#multi-task">multi-task</a></li>
<li><a href="#which-layer">which layer</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#self-knowledge-distillation">Self-Knowledge Distillation</a></li>
<li><a href="#zhi-shi-zhu-ru">知识注入</a></li>
<li><a href="#shu-ju-zeng-qiang">数据增强</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>随着Transformer 在NLP中的表现，Bert已经成为主流模型，然而大家在下游任务中使用时，是不是也会发现模型的性能时好时坏，甚至相同参数切换一下随机种子结果都不一样，又或者自己不管如何调，模型总达不到想象中的那么好，那如何才能让Bert在下游任务中表现更好更稳呢？本文以文本分类为例，介绍几种能帮你提高下游任务性能的方法。</p>
<h1><span id="further-pre-training">Further Pre-training</span><a href="#further-pre-training" class="header-anchor"></a></h1><p>最稳定也是最常用的提升下游任务性能的手段就是继续进行预训练了。</p>
<h2><span id="er-jie-duan-vs-san-jie-duan-vs-si-jie-duan">二阶段 vs 三阶段 vs 四阶段</span><a href="#er-jie-duan-vs-san-jie-duan-vs-si-jie-duan" class="header-anchor"></a></h2><p>首先回顾一下，Bert 是如何使用的呢？我们设通用泛化语料为$D_g$，下游任务相关的数据为$D_t$, Bert 即在通用语料$D_g$ 上训练一个通用的Language Model， 然后利用这个模型学到的通用知识来做下游任务，也就是在下游任务上做fine-tune，这就是<code>二阶段模式</code>。大多数情况下我们也都是这么使用的：下载一个预训练模型，然后在自己的数据上直接fine-tune。</p>
<h3><span id="san-jie-duan">三阶段</span><a href="#san-jie-duan" class="header-anchor"></a></h3><p>在论文<a href="http://arxiv.org/abs/1801.06146" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a>中，作者提出了一个通用的范式ULMFiT：</p>
<ol>
<li><em>在大量的通用语料上训练一个LM（Pretrain）；</em></li>
<li><em>在任务相关的小数据上继续训练LM（Domain transfer）；</em></li>
<li><em>在任务相关的小数据上做具体任务（Fine-tune）。</em></li>
</ol>
<p>那我们在使用Bert 时能不能也按这种范式，进行三阶段的fine-tune 从而提高性能呢？答案是：<code>能！</code><br>比如邱锡鹏老师的论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>和<a href="arXiv:2004.10964 [cs]" target="_blank" rel="noopener">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a>中就验证了，在任务数据$D_t$ 继续进行pretraining 任务，可以提高模型的性能。<br>那如果我们除了任务数据没有别的数据时，怎么办呢？简单，任务数据肯定是相同领域的，此时直接将任务数据看作相同领域数据即可。所以，在进行下游任务之前，不妨先在任务数据上继续进行pre-training 任务继续训练LM ，之后再此基础上进行fine-tune。</p>
<h3><span id="si-jie-duan">四阶段</span><a href="#si-jie-duan" class="header-anchor"></a></h3><p>我们在实际工作上，任务相关的label data 较难获得，而unlabeled data 却非常多，那如何合理利用这部分数据，是不是也能提高模型在下游的性能呢？答案是：<code>也能！ </code>  </p>
<ol>
<li><em>在大量通用语料上训练一个LM（Pretrain）；</em></li>
<li><em>在相同领域$D_{in_domain}$上继续训练LM（Domain transfer）；</em></li>
<li><em>在任务相关的小数据上继续训练LM（Task transfer）；</em></li>
<li><em>在任务相关数据上做具体任务（Fine-tune）。</em>  </li>
</ol>
<p>而且上述两篇论文中也给出了结论：先Domain transfer 再进行Task transfer 最后Fine-tune 性能是最好的。</p>
<h1><span id="ru-he-further-pre-training">如何further pre-training</span><a href="#ru-he-further-pre-training" class="header-anchor"></a></h1><h2><span id="how-to-mask">how to mask</span><a href="#how-to-mask" class="header-anchor"></a></h2><p>首先，在further pre-training时，我们应该如何进行mask 呢？不同的mask 方案是不是能起到更好的效果呢？<br>在Roberta 中提出，动态mask 方案比固定mask 方案效果更好，此外，在做Task transfer 时，由于数据通常较小，固定的mask 方案通常也容易过拟合，所以further pre-training 时，动态随机mask 方案通常比固定mask 效果更好。<br>而ERNIE 和 SpanBert 中都给出了结论，更有针对性的mask 方案可以提升下游任务的性能，那future pre-training 时是否有什么方案能更有针对性的mask 呢？<br>刘知远老师的论文<a href="http://arxiv.org/abs/2004.09733" target="_blank" rel="noopener">Train No Evil: Selective Masking for Task-Guided Pre-Training</a>就提出了一种更有针对性的mask 方案<code>Selective Mask</code>,进行further pre-training 方案，该方案的整体思路是：</p>
<ol>
<li><em>在$D_t$上训练一个下游任务模型 $Model_0$;</em></li>
<li><em>利用$Model_0$判断token 是否是下游任务中的重要token，具体计算公式为：$S(w_i) = P(y_t|s) - P(y_t|s^{‘}_{i-1}W_i)$, 其中$s$为完整句子（序列），$s^{‘}$为一个初始化为空的buffer，每次将句子中的token 往buffer中添加，如果加入的token 对当前任务的表现与完整句子在当前任务的表现差距小于阈值，则认为该token 为重要token，并从buffer 中剔除；</em></li>
<li><em>利用上一步中得到的token label，训练一个二分类模型$Model_b$，来判断句子中的token 是否为重要token；</em></li>
<li><em>利用$Model_b$，在domain 数据上进行预测，根据预测结果进行mask ；</em></li>
<li><em>进行Domain transfer pre-training；</em></li>
<li><em>在下游任务进行Fine-tuning。</em><br>上述方案验证了更有针对性的mask 重要的token，下游任务中能得到不错的提升。综合下来，<code>Selective Mask &gt; Dynamic Mask &gt; Static Mask</code>   </li>
</ol>
<p>虽然selective mask 有提升，但是论文给出的思路太过繁琐了，本质上是判断token 在下游任务上的影响，所以这里给出一个笔者自己脑洞的一个方案：通过$Model_0$在unlabeled 的Domain data 上直接预测，然后通过不同token 下结果的熵的波动来确定token 对下游任务的影响。这个方案我没有做过实验，有兴趣的可以试试。  </p>
<h2><span id="when-to-stop">when to stop</span><a href="#when-to-stop" class="header-anchor"></a></h2><p>在further pretraining 时，该何时停止呢？是否训练的越久下游任务就提升的越多呢？答案是否定的。在进行Task transfer 时，应该训练多少步，论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>进行了实验，最后得出的结论是<code>100k</code>步左右，下游任务上提升是最高的，这也与我自己的实验基本吻合，训练过多就会过拟合，导致下游任务上提升小甚至降低。</p>
<p><img src="/2020/11/24/fine-tune/step.png" alt></p>
<p>此外，由于下游任务数据量的不同，进行多少步结果是最优的也许需要实验测试。这里给出一个更快捷稳妥的方案：借鉴PET本质上也是在训练MLM 任务，我们可以先利用利用PET做fine-tuning，然后将最优模型作为预训练后的模型来进行分类任务fine-tuning，这种方案我实验后的结论是与直接进行Task transfer性能提升上相差不大。不了解PET的可以查看我之前博文<a href="https://xv44586.github.io/2020/10/25/pet/">PET-文本分类的又一种妙解</a>.</p>
<h2><span id="how-to-fine-tuning">how to fine-tuning</span><a href="#how-to-fine-tuning" class="header-anchor"></a></h2><p>不同的fine-tuning 方法也是影响下游任务性能的关键因素。</p>
<h3><span id="optimizer">optimizer</span><a href="#optimizer" class="header-anchor"></a></h3><p>关于优化方案上，Bert 的论文中建议使用与bert 预训练时一致的方案进行fine-tuning，即使用weighted decay修正后的Adam，并使用warmup策略 搭配线性衰减的学习率。不熟悉的同学可以查看我之前的博文<a href="https://xv44586.github.io/2020/08/01/optimizer-in-bert/">optimizer of bert</a></p>
<h3><span id="learning-rate">learning rate</span><a href="#learning-rate" class="header-anchor"></a></h3><p>不合适的learning rate可能会导致<code>灾难性遗忘</code>,通常learning rate 在$[-e^{-5}, 1e^{-4}]$之间，更大的learning rate可能就会发生灾难性遗忘，不利于优化。</p>
<p><img src="/2020/11/24/fine-tune/lrt.png" alt></p>
<p>此外，对transformer 逐层降低学习率也能降低发生灾难性遗忘的同时提升一些性能。</p>
<h3><span id="multi-task">multi-task</span><a href="#multi-task" class="header-anchor"></a></h3><p>Bert在预训练时，使用了两个task：NSP 和 MLM，那在下游任务中，增加一个辅助的任务是否能带来提升呢？答案是否定的。如我之前尝试过在分类任务的同时，增加一个相似性任务：让样本与label desc的得分高于样本与其他样本的得分，但是最终性能并没有得到提升。具体的实验过程请看博文<a href="https://xv44586.github.io/2020/09/13/classification-label-augment/">模型增强之从label下手</a>。<br>此外，论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>也任务multi-task不能带来下游任务的提升。</p>
<h3><span id="which-layer">which layer</span><a href="#which-layer" class="header-anchor"></a></h3><p>Bert的结构上是一个12层的transformer，在做文本分类时，通常我们是直接使用最后一层的<code>[CLS]</code>来做fine-tuning，这样是最优的吗？有没有更好的方案？<br>论文<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>中针对这个问题也做了实验，对比了不同的layer不同的抽取策略，最终结论是所有层拼接效果最好，但是与直接使用最后一层差距不大。<br><img src="/2020/11/24/fine-tune/layer.png" alt></p>
<p>而论文<a href="http://arxiv.org/abs/2008.06460" target="_blank" rel="noopener">Hate Speech Detection and Racial Bias Mitigation in Social Media based on BERT model</a>中，作者通过组合多种粒度的语义信息，即将12层的<code>[CLS]</code>拼接后，送人CNN，在Hate Speech Detection 中能带来<code>8个点</code>的提升！<br><img src="/2020/11/24/fine-tuning/cnn.png" alt></p>
<p>所以在fine-tuning时，也可以想一想到底是哪种粒度的语义信息对任务更重要。</p>
<h1><span id="self-knowledge-distillation">Self-Knowledge Distillation</span><a href="#self-knowledge-distillation" class="header-anchor"></a></h1><p>self-knowledge distillation（自蒸馏）也是一种常用的提升下游任务的手段。做法是先在Task data上fine-tuning 一个模型，然后通过模型得到Task data 的soft labels，然后使用soft labels 代替hard label 进行fine-tuning。更多细节可以查看之前的博文<a href="https://xv44586.github.io/2020/08/31/bert-01/">Knowledge Distillation之知识迁移</a> </p>
<h1><span id="zhi-shi-zhu-ru">知识注入</span><a href="#zhi-shi-zhu-ru" class="header-anchor"></a></h1><p>通过注入外部知识到bert中也能提升Bert的性能，常用的方式主要有两种：</p>
<ol>
<li><em>在bert embedding 层注入：通过将外部Embedding 与Bert token-embedding 拼接（相加）进行融合，然后进行transformer一起作用下游；</em></li>
<li><em>在transformer的最后一层，拼接外部embedding，然后一起作用下游。</em><br>如<a href="http://arxiv.org/abs/1909.08402" target="_blank" rel="noopener">Enriching BERT with Knowledge Graph Embeddings for Document Classification</a>中，通过在<br>transformer的最后一层中拼接其他信息，提高模型的性能。<br><img src="/2020/11/24/fine-tune/kg.png" alt></li>
</ol>
<h1><span id="shu-ju-zeng-qiang">数据增强</span><a href="#shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>NLP中数据增强主要有两种方式：一种是保持语义的数据增强，一种是可能破坏语义的局部扰动增强。保持语义通常采用回译法，局部扰动的通常使用EDA，更多细节可以查看之前博文<a href="https://xv44586.github.io/2020/11/10/eda/">NLP中的数据增强</a></p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文总结了使用bert 时，当前主要的提升Bert 在下游任务上的性能的方法，遇到相关问题时，可以尝试一下。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>重新认识矩阵</title>
    <url>/2021/01/12/matrix/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#shi-me-shi-1">什么是1</a></li>
<li><a href="#xiang-liang">向量</a></li>
<li><a href="#ju-zhen">矩阵</a></li>
<li><a href="#ju-zhen-cheng-fa">矩阵乘法</a></li>
<li><a href="#xiang-si-ju-zhen">相似矩阵</a></li>
<li><a href="#xing-lie-shi">行列式</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>最近有篇很火的论文bert-flow，其中的flow是出了GAN和VAE之外的第三种生成模型，我竟然是第一次听说，所以引起了我的好奇心，然而看flow模型时，发现里面有一个很重要的概念就是行列式，数学渣太久没碰过他所以再看到有些陌生，于是就找了一些文章来重新学习一下，后来发现了孟岩的认识矩阵系列博客与苏神的新认识矩阵系列博客，看完感觉对矩阵的认识refresh了一下，所以做一些总结记录。</p>
<h1><span id="shi-me-shi-1">什么是1</span><a href="#shi-me-shi-1" class="header-anchor"></a></h1><p>首先我们来讨论一下，什么是<code>1</code>，所谓1，就是参照物，或者是基，而其他的数字都是在参考他得到的，比如2,2就是2 个1，也就是1<em>2.<br>而1又是一个抽象的东西，你没法直观感受什么是1，只有在参考系下，才能直观感受到什么是1，如1瓶水，1米长，而1 与一米的关系就像类与对象一样。所以在参考系下，2米的含义就是2个1米，1米 </em> 2，即在以1米为基的坐标系下，他在2的“位置”。而5瓶水，对应的含义是在以1瓶水为基的参考系下，<br>他处在5的“位置”。而不同的坐标系，在不同的情况下也有优劣之分，如同样的酒，当用来统计销售额时，我们用“瓶”来统计就比用“杯”来统计方便，而当用来劝酒时，用“杯”就明显比用“瓶”要合适一些。也就是，酒还是那么多酒，但是当你用不同的方式来度量时，得到的结果也是不同的，是有优劣之分的。</p>
<h1><span id="xiang-liang">向量</span><a href="#xiang-liang" class="header-anchor"></a></h1><p>对于向量$A(a, b)$,其对应的含义是在直角坐标系（以$\vec{i}=(1,0),\vec{j}=(0,1)$为基）中，A是在x轴上为a,y轴上为b的点。<br>比如，在以x轴为斤y轴为瓶的直角坐标系（以$\vec{i}=(一斤,0瓶), \vec{j}=(0斤,1瓶)$为基）中，三瓶500毫升的肥仔水对应的向量就是B(3,3).所以，<code>所谓向量即在线性空间内，选定一组基后，用来刻画一个对象</code>。</p>
<h1><span id="ju-zhen">矩阵</span><a href="#ju-zhen" class="header-anchor"></a></h1><p>那何为矩阵呢？比如对于矩阵<br>$$A=\begin{pmatrix}<br>a &amp; c \\<br>b &amp; d<br>\end{pmatrix}$$<br>我们能观察到什么呢？首先，矩阵也是由向量（列向量）组成的，对于当前矩阵A，也就对应着$\vec{i}=(a,b),\vec{j}=(c,d)$ 两个向量，然后将他们按一定的顺序排列就组成了矩阵；此外，这个矩阵又代表了由这两个向量为基组成的坐标系（线性空间），在这个空间内，所有的对象都能通过这两个基来进行刻画，即对应于一个“向量”。这里如何更直观的理解呢？还是考虑之前的例子，同样是酒，我们可以用“杯”来度量，也可以用“瓶”来度量，假如我们用“杯”作为最初的基，<br>那矩阵对应的就是“瓶”作为基构成的坐标系，而构成矩阵的向量又是在以“杯”为基下度量出来的，其对应的向量中的值的含义就是“瓶”的基在“杯”这个基下的表示，<br>同样的酒，假如在“杯”坐标系下为10（杯），通过这个矩阵，就变为了2（瓶），<code>即矩阵是线性空间里变换（运动）的描述。</code></p>
<h1><span id="ju-zhen-cheng-fa">矩阵乘法</span><a href="#ju-zhen-cheng-fa" class="header-anchor"></a></h1><p>那何为矩阵乘法呢？比如现在有矩阵<br>$$<br>A=\begin{pmatrix}<br>a &amp; c \\<br>b &amp; d<br>\end{pmatrix}<br>$$<br>向量$x=(e,f)$,对于$Ax$这个矩阵与向量的乘法，这个是什么含义呢？上面我们已经提到了，矩阵是由列向量组成的，而列向量又可以看做是对应坐标系下的一组基，那根据之前我们提到向量与基的关系，就是在基确定后，用来刻画其空间内对象的，对应的就是乘法，那这个矩阵与向量的乘法，我们也可以用这种方式来看待，即矩阵与向量的乘法代表的是在矩阵对应的列向量为基组成的空间内，由向量刻画的对象。这也是为什么矩阵乘法要求对应维度要相等，<br>对应的就是需要用所有的基，才能正确刻画在这组基对应坐标系下的位置（对象）。而矩阵与向量乘法就是用新的基来刻画对象，<code>也即矩阵与向量的乘法代表施加变换。</code></p>
<p>那为什么矩阵的乘法公式里为对应行与列相乘并求和呢？这是因为最终的结果，我们是需要变换到对应的直角坐标系下的，因为我们书写出来的所有向量，都默认是在直角坐标系下的结果，所以我们需要将结果向量在每个轴上进行分解，然后合并得到最终的结果，对应的就是行与列相乘后求和。</p>
<p>那何为矩阵与矩阵的乘法呢？如$A * B$,同理，矩阵与矩阵的乘法也可以看做是连续变换后，构造的新的坐标系。矩阵是由向量组成的，而矩阵与向量乘法的含义是在新的基下刻画的对象表示，所以矩阵与矩阵的乘法，对应B这个变换在施加了A这个变换后，形成的新的变换，而施加这个变换，对应B中的列向量都是在A坐标下刻画得到的。</p>
<p>那何为线性方程呢？比如$Ax = y$, 对应的线性方程是什么含义呢？这个方程左侧有向量有矩阵，而右侧缺只有向量，有点不和谐，我们变换一下，让他们形式一样，$Ax = Iy$,这样，等式成立，两边形式也一样。那写成这个变换，我们就能猜到这个方程中等式的意义了：在直角坐标系($I$)下的y向量，在A坐标系下该如何刻画(x)呢？那再来看看这个方程的解，即$x = A^{-1}y$，而这个式子可以理解为将施加的变换“逆”着再变回去，就还原了原始位置。</p>
<p>普通乘法有交换律、结合律、分配律，那矩阵乘法是否也满足呢？对于交换律，矩阵乘法是不满足的，想象一下，5“瓶”酒换成“杯”可能是25，而5“杯”酒换成“瓶”，可能也就是1了，所以两个变换交换位置，得到的是不同的变换；而结合律与分配律是满足的，结合律可以看做是将变换分成“几步”走，即先进行“子变换a”,在进行“子变换b”，几步走与一步到位的结果是一致的；而分配律也是类似的思路。</p>
<h1><span id="xiang-si-ju-zhen">相似矩阵</span><a href="#xiang-si-ju-zhen" class="header-anchor"></a></h1><p>那何为相似矩阵呢？假设有一个矩阵A，对应$y=Ax$, 而现在有一个新的坐标系P，对应有${y}’=B{x}’$,在P坐标系下，x与y对应着$P{x}’=x$, $P{y}’=y$,代入后得到：$P{x}’=AP{y}’ = P(P^{-1}AP){x}’$，即在P坐标系下，从${x}’$ 到${y}’$的变换用矩阵$B=P^{-1}AP$来表示，这就是相似矩阵，<a href="https://spaces.ac.cn/archives/1777" target="_blank" rel="noopener">即同一个线性变换在不同坐标系下的一个测量结果而已。</a></p>
<h1><span id="xing-lie-shi">行列式</span><a href="#xing-lie-shi" class="header-anchor"></a></h1><p>行列式是在通过<a href="https://zhuanlan.zhihu.com/p/37111386" target="_blank" rel="noopener">高斯消元法解线性方程组时引入的数学工具</a>, 对应的定义为：<br>$$<br>D = \sum(-1)^t a_{1p_n}a_{2p_n}…a_{np_n}<br>$$<br>其中$t$为排列$a_{1p_n}a_{2p_n}…a_{np_n}$的逆序数，$\sum$ 为对所有可能的排列求和。<br>这个计算方式看起来好奇怪，还有一个逆序数，无法直观的理解，那怎么才能有个直观印象，让我们知道为什么这里是这么计算的呢？<br>说实话我虽然几年前在网易云课堂重新学了一遍线性代数，但是今天又基本全忘记了，所以这次，我希望找到一些更“直观”的东西。<br>如果按照矩阵就是对应坐标系的变换，也就是对应基的变换的思路，那矩阵的行列式应该是对应着变换的某种度量（实际是对应坐标系的缩放，也就是基的体积变换）。顺着这个思路，苏神的一篇博文<a href="https://kexue.fm/archives/2208" target="_blank" rel="noopener">新理解矩阵5：体积=行列式</a>中，通过两者的性质相同，证明了行列式的几何意义，就是其对应的n维平行n维体的体积。<br>知道了这个，我们就可以尝试通过计算平行n维体的体积来“直观”感受一下了。<br><img src="/2021/01/12/matrix/det.png" alt></p>
<p>这里我们讨论二维空间，如上图所示，其中$\overrightarrow{A}$与$\overrightarrow{B}$分别代表两个向量，而其围成的面积S 为(O,A,I,B)四个点所围成的图形。而这个图形又可以看成两个部分的组合：一部分为与四边形(O,D，P,G)重叠的部分，另一部分为剩下的部分。而剩下的部分又可以通过$PI$切分为两个三角形：$\bigtriangleup BPI$ $\bigtriangleup API$，<br>我们分别做两个三角形的高（蓝色线）$PJ=b, PK=c$，此时我们将$\bigtriangleup BPI$ 与 $\bigtriangleup ODA$ 进行对比，两者的高都是$b$,而底的差为$c$，也就是$S\bigtriangleup ODA - S\bigtriangleup BPI = S\bigtriangleup OCM$， 类似的，我们可以求出$S\bigtriangleup OBG - S\bigtriangleup API = S\bigtriangleup OFM$,于是，我们就可以得到一个结论：<br>$$<br>S_{\square OAIB} = S_{\square ODPG} - S_{\square OCMF} = ad - bc = \begin{Vmatrix}<br>a &amp;b \\<br>c &amp; d<br>\end{Vmatrix} = det(A,B)<br>$$</p>
<p>而两个四边形的位置都是由向量在坐标轴上投影后确定的，也就是对应着行列式中的“排列相乘”，而面积不足需要被“减掉”的小面积就对应逆序数，所以完整的过程就是“通过排列求长方形的面积，然后用大面积减去小面积”。<br>以上的思路可以推广到N维空间，思想是一样的。 而理解了矩阵的行列式就是平行N维体的体积后，对应行列式的计算公式也就显得相当“直观”了。如，对行列式的某一列乘上$\alpha$，则对应的行列式的值也乘上$\alpha$，这里就是对应其中的一个基向量放大了$\alpha$倍，所以体积放大了$\alpha$倍。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文主要是参考了许多文章后，发现将矩阵看做是变换的表述这个视角下，很多相关问题都变得非常直观又容易理解，最后经过思考做的部分总结，如果对矩阵感兴趣或者总觉得不容易理解，推荐孟岩的<a href="http://blog.csdn.net/myan/article/details/647511" target="_blank" rel="noopener">理解矩阵系列</a>、苏神的<a href="[https://kexue.fm/archives/1765">新理解矩阵系列</a>、<a href="https://www.matongxue.com/madocs/247/" target="_blank" rel="noopener">马同学的矩阵与行列式系列</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1>]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title>对比学习心路历程</title>
    <url>/2021/07/06/cl2rdrop/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#qian-yan">前言</a></li>
<li><a href="#dui-bi-xue-xi-fan-shi">对比学习范式</a></li>
<li><a href="#wu-jian-du-dui-bi-xue-xi">无监督对比学习</a></li>
<li><a href="#you-jian-du-dui-bi-xue-xi">有监督对比学习</a></li>
<li><a href="#zong-jie">总结</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="qian-yan">前言</span><a href="#qian-yan" class="header-anchor"></a></h1><p>在之前的<a href="https://xv44586.github.io/2021/01/20/ccf-qa-2/">博客</a>里，笔者介绍了在有监督任务（文本匹配）上，增加对比学习来提高模型性能的实验，而当时尝试后却发现，在新增了对比学习的任务后，模型的性能并没有得到对应的提升，经过简单分析后，笔者也尝试给出了一些可能的问题与解决方案。虽然笔者的实验都失败了，但是笔者依然认为对比学习是一个非常好的方向，所以也在持续关注，这篇就算是后续的填坑与总结吧。</p>
<h1><span id="dui-bi-xue-xi-fan-shi">对比学习范式</span><a href="#dui-bi-xue-xi-fan-shi" class="header-anchor"></a></h1><p>之前也提到过，这里在简单称述一下对比学习目前的主要范式。对比学习主要是通过对比，拉近相似样本之间的距离，推远不相似样本之间的距离。而相似样本的构造，又可以分为有监督与无监督两种：</p>
<ul>
<li>有监督对比学习：通过将监督样本中的相同label的样本作为正样本，不同label的样本作为负样本，来进行对比学习；</li>
<li>无监督对比学习：由于没有监督信号（label），此时，我们对同一个样本构造两个view，让同一样本构造的两个view互为正样本，而其他样本构造的view则全部为负样本，以此来进行对比学习。而由同一个样本构造两个view，又是数据扩增的过程，所以也可以称作是数据扩展对比学习。而不管那种范式，通常对比学习都是在batch内进行。</li>
</ul>
<h1><span id="wu-jian-du-dui-bi-xue-xi">无监督对比学习</span><a href="#wu-jian-du-dui-bi-xue-xi" class="header-anchor"></a></h1><p>笔者曾经在实验中，通过对样本进行eda（随机替换、随机删除、随机重复和随机互换），来构造不同view，由此进行对比学习。实验失败后，笔者也提到失败的可能原因主要有两点：batch size太小且样本上直接进行操作，获得的新样本与原来的语义可能由较大的差别甚至是完全的反义。如：“我不会再爱你” –&gt; “我会再爱你”,而强行让这两个样本的语义距离相互靠近，效果自然不会好；而batch size太小的问题，一来是换大的GPU，二来可以尝试在算法层面进行优化，如将Adam 替换为AdaFactor，re-compute等，这不是本文的重点，所以就不再细说了。<br>而针对直接对样本进行修改会导致语义不一致的问题，其主要原因是NLP中，样本的输入是one-hot形式的，我们的相似样本应该是语义上相似，对应的修改后的语义距离应该尽可能“小”，而直接对one-hot形式进行修改，对应的距离恒定是$\sqrt{2}$ ，怎么看也不小。一种解决方法是借鉴对抗训练在NLP中的方式，将修改放在Embedding 层，由于one-hot与Embedding的对应关系，就能够获得“相似语义”的样本了。对应思路的论文有美团今年的<a href="http://arxiv.org/abs/2105.11741" target="_blank" rel="noopener">ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</a>,总的来说，这篇paper就是将EDA的思路放在embedding 层，然后构建不同view后进行对比学习，最后获得的语义表示超过了当前的sota，在多个数据集上获得了提升。不过值得吐槽的是，美团也学坏了，竟然学会了谷歌那套在更多数据上进行训练然后跟只有原始数据的结果进行对比，可能是论文本身没什么亮点，只能通过“大的提升”来吸引眼球了吧~<br>而做数据扩增时，最常用的两个方式随机删除与随机互换，而位置编码在transformer 中起到的作用不是关键性的，且直接互换位置也会带来语义变化较大的风险，那只使用随机删除策略做数据扩增，进行对比学习效果如何呢？答案是非常好！女神陈丹琦最近的论文<a href="https://arxiv.org/abs/2104.08821" target="_blank" rel="noopener">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a>,只使用dropout ，对同一个样本构造两个不同的view，然后进行对比学习，最终的效果在非监督学习任务下8个点提升起步，在监督任务下4个点提升起步，此外，<a href="https://github.com/bojone/SimCSE" target="_blank" rel="noopener">苏神</a>在中文任务上也进行了实验，再次验证了该方法的有效性。</p>
<p><img src="/2021/07/06/cl2rdrop/simcse.png" alt="效果对比"></p>
<p>而simcse具体是如何工作的呢？其实做法相当简单，即将一个样本进入模型两次，然后通过dropout 两次，获得两个不同的view，互为正样本。而dropout 由于是在feature 维度进行随机mask，所以就得到了与在embedding层随机mask相同的效果，此外，由于在transformer 中，dropout 多次使用，也进一步的增加了两个view 的差异。读完论文，看着效果，直叹“大道至简”，而对比在embedding 做扰动与直接使用dropout，从效果上看dropout 也是一种更加有效的数据扩增手段。那这个思路是不是可以扩展到有监督任务呢？<br>其实读完SimCSE后，笔者就觉得dropout 是一种更加有效的数据扩增手段，自然可以扩展到有监督任务学习中。这里的有监督任务学习有别与有监督对比学习，请读者注意区分。而在有监督任务学习中的方案也是比较直观：将样本进入模型两遍，然后在做监督任务的同时，增加一个对比学习，不过笔者实验时被之前无监督任务的思路所束缚了，一直使用encoder output 的logits来表征语义，所以调来调去效果时好时坏，好的时候也没有超过0.5的提升，所以就放弃了。<br>而最近的论文<a href="http://arxiv.org/abs/2106.14448" target="_blank" rel="noopener">R-Drop: Regularized Dropout for Neural Networks</a>却成功的将这个思路延续了下来，在读完其<a href="https://github.com/dropreg/R-Drop" target="_blank" rel="noopener">代码</a>后，才发现原来是自己姿势不对~<br>R-Drop中的完整思路是这样的：首先，我们通过将样本重复的输入到模型，然后通过dropout，获取不同的view，而dropout的目的是为了将集成模型的思路延续到深度学习中，本质目的是增加模型的鲁棒性。所以，不同的view获得的最终的输出，我们也希望其尽可能的一致。而论文中的任务都是分类任务，所以模型最终的输出是一个概率分布，衡量两个概率分别的差异通常使用kl散度，所以最终的loss增加了一个对应view之间的kl-divergence。<br>对比笔者之前的思路与R-Drop 中的思路，首先R-Drop 中是直接作用在最终的probs上，而笔者是希望encoder output logits 之间能“同性相吸，异性排斥”，而由于在做分类任务时，在logits 后都会接一个dense层来压缩维度，这个dense 层就会大大抑制前面的对比学习的效果，即前面的logits可以差异很大，但通过最后的dense层，却能得到一样的预测结果（label相同），而直接作用在最终的输出层，才会得到想要的效果；其次在R-Drop 中，不再需要与其他样本做对比，即只要相同view 的输出足够“接近”即可，而不需要其与其他的view 尽可能的“远”，考虑到同一个batch 内，存在同样label的样本的概率是极大的，而要求同时预测对label又要其结果之间尽可能不同是不合理的，所以取消掉与其他view之间的对比也是合理与必要的；而由于不需要去其他view做对比，对比学习中大batch size的要求也就不再需要，即在小的batch size 下该方法依然有效，真可谓“方法简单效果好”，在所有的NLP任务中都值得尝试。<br>前面提到笔者之前的思路的最大问题是对比的不是最后一层的输出，而同一batch内，又不可避免的会出现相同label，让相同label的logits之间相互“远离”似乎不太合理，但从对抗训练或者模型的鲁棒性上考虑，让同一样本的不同view之间相互接近而与其他view之间相互远离，可以达到让样本在一个范围内的输出尽可能的保持一致，从而能使模型更加的鲁棒，自然会增加一些泛化能力。这个思路笔者也在<a href="https://github.com/xv44586/toolkit4nlp/blob/master/examples/classification_tnews_contrastive_learning_dropout.py" target="_blank" rel="noopener">之前的实验</a>上进行了修改尝试，结果显示确实能提高一些性能，但是与只拉近相同view之间的kl-divergence相比，提升就不够看了。</p>
<h1><span id="you-jian-du-dui-bi-xue-xi">有监督对比学习</span><a href="#you-jian-du-dui-bi-xue-xi" class="header-anchor"></a></h1><p>有监督对比学习是指相同label之间互为正样本，不同label间为负样本，与上文提到的有监督任务是不同的。而有监督任务中也可以使用无监督对比学习的思路，如上文提到的R-Drop，而有监督对比学习自从<a href="https://arxiv.org/abs/2011.01403" target="_blank" rel="noopener">Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning</a>后，笔者还未关注到有什么新的进展，所以还是推荐在有监督任务上使用R-Drop思路。</p>
<h1><span id="zong-jie">总结</span><a href="#zong-jie" class="header-anchor"></a></h1><p>本文主要总结了从笔者之前对比学习实验时留下的问题，到最近一些论文中提出的解决思路，虽然整个心路历程与原作者们的肯定不一样，但希望能给读者提供对比学习到目前为止的一些进展脉络。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>图片来自大佬刑无刀（陈开江）的朋友圈：据说人工智能中有80%的部分是人工😄</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>SimCSE</tag>
        <tag>R-Drop</tag>
      </tags>
  </entry>
  <entry>
    <title>训练加速篇（2）-horovod</title>
    <url>/2022/05/25/horovod/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#horovod">horovod</a></li>
<li><a href="#da-huan-jing">搭环境</a><ul>
<li><a href="#docker">docker</a></li>
</ul>
</li>
<li><a href="#local">local</a><ul>
<li><a href="#nccl">NCCL</a><ul>
<li><a href="#yan-zheng-yi-xia">验证一下</a></li>
</ul>
</li>
<li><a href="#mpirun">mpirun</a></li>
<li><a href="#nvidia-tensorflow">nvidia-tensorflow</a></li>
<li><a href="#horovod-1">horovod</a></li>
</ul>
</li>
<li><a href="#huan-jing-zhong-cai-guo-de-keng">环境中踩过的坑</a><ul>
<li><a href="#nccl">nccl</a></li>
<li><a href="#horovod-2">horovod</a></li>
</ul>
</li>
<li><a href="#dai-ma-ceng-mian-xiu-gai">代码层面修改</a></li>
<li><a href="#run-code">run code</a></li>
<li><a href="#jia-su-xiao-guo">加速效果</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<h1><span id="horovod">horovod</span><a href="#horovod" class="header-anchor"></a></h1><p><a href="https://github.com/horovod/horovod" target="_blank" rel="noopener">horovod</a>是Uber 团队开发的分布式训练框架，他可以满足让你尽量少的修改代码即可将在单卡训练的脚本横行扩展为多卡并行训练，同时又兼顾训练的加速。目前支持tensorflow/keras/pytorch/mxnet.底层通信主要依赖<a href="https://github.com/NVIDIA/nccl" target="_blank" rel="noopener">NCCL</a>/<a href="https://github.com/facebookincubator/gloo" target="_blank" rel="noopener">Gloo</a>（测试后NCCL是最快的）,支持MPI（CPU 训练更快）。由于其训练加速效果比tensorflow 原生的distributedStrategy 快很多，所以在分布式训练时，推荐使用。<br>下面主要针对tensorflow1.x 下做分布式训练进行说明。</p>
<h1><span id="da-huan-jing">搭环境</span><a href="#da-huan-jing" class="header-anchor"></a></h1><p>环境主要依赖tensorflow1.x/horovod/nccl/mpi ，这里有两种方式搭环境：local/docker，下面分别介绍这两种环境的搭建及踩过的坑。</p>
<h2><span id="docker">docker</span><a href="#docker" class="header-anchor"></a></h2><p>由于谷歌官方的tensorflow1.x 不提供对A100/3090 及更新版本的显卡的支持，这里我们推荐使用nvidia 官方维护的<a href="https://github.com/NVIDIA/tensorflow" target="_blank" rel="noopener">nvidia-tensorflow</a>，其提供了对A100/3090 等显卡的支持。支持pip/conda安装，同时也提供了对应的docker 镜像。<br>在使用镜像时，首先在<a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow/tags" target="_blank" rel="noopener">ngc</a> 中寻找最新镜像并pull 下来。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull nvcr.io/nvidia/tensorflow:22.04-tf1-py3</span><br></pre></td></tr></table></figure></p>
<p>在启动时，建议设置share memory 为单张显卡显存差不多大小，否则有可能由于默认share memory size 太小（64M）导致nccl 通信时资源不足被kill。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -itd --gpus all --net=host --privileged -v /data:/data --name horovod --shm-size=32g --<span class="built_in">ulimit</span> memlock=-1 --<span class="built_in">ulimit</span> stack=67108864 nvcr.io/nvidia/tensorflow:22.04-tf1-py3</span><br></pre></td></tr></table></figure>
<h1><span id="local">local</span><a href="#local" class="header-anchor"></a></h1><p>在本地搭环境，主要需要安装nvidia-tensorflow/horovod/nccl/mpi,此外，由于nvidia-tensorflow 只支持ubuntu20.04，对应的OS 也要调整。</p>
<h2><span id="nccl">NCCL</span><a href="#nccl" class="header-anchor"></a></h2><p>安装NCCL，注意版本，为了兼顾A100，推荐使用v2.8.3-1 这个版本。<br>ref:<a href="https://github.com/NVIDIA/nccl" target="_blank" rel="noopener">nccl</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编译nccl</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/nccl.git</span><br><span class="line"><span class="built_in">cd</span> nccl &amp;&amp; git checkout v2.8.3-1</span><br><span class="line">make -j src.build</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果第一次安装，需要安装一下依赖</span></span><br><span class="line"><span class="comment"># Install tools to create debian packages</span></span><br><span class="line">sudo apt install build-essential devscripts debhelper fakeroot</span><br><span class="line"><span class="comment"># Build NCCL deb package</span></span><br><span class="line">make pkg.debian.build</span><br><span class="line">ls build/pkg/deb/</span><br><span class="line"></span><br><span class="line"><span class="comment"># install</span></span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure></p>
<h3><span id="yan-zheng-yi-xia">验证一下</span><a href="#yan-zheng-yi-xia" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 确认horovod链接的nccl版本路径正确</span></span><br><span class="line">ldd /usr/<span class="built_in">local</span>/lib/python3.8/dist-packages/horovod/tensorflow/mpi_lib.cpython-38-x86_64-linux-gnu.so</span><br></pre></td></tr></table></figure>
<h2><span id="mpirun">mpirun</span><a href="#mpirun" class="header-anchor"></a></h2><p>这里我们安装<a href="https://www.open-mpi.org/" target="_blank" rel="noopener">open-mpi</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">apt-get install libnuma-dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># mpirun</span></span><br><span class="line">wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.5.tar.gz &amp;&amp; tar xvf openmpi-4.0.5.tar.gz</span><br><span class="line"><span class="built_in">cd</span> openmpi-4.0.5/ &amp;&amp; ./configure --prefix=/usr/<span class="built_in">local</span>/openmpi</span><br><span class="line">make -j$(nproc) &amp;&amp; sudo make install</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PATH=/usr/local/openmpi/bin:$PATH'</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH'</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line">mpirun --version</span><br></pre></td></tr></table></figure>
<h2><span id="nvidia-tensorflow">nvidia-tensorflow</span><a href="#nvidia-tensorflow" class="header-anchor"></a></h2><p>这里直接用pip 安装或conda 安装都行：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install nvidia-pyindex</span><br><span class="line">pip install nvidia-tensorflow</span><br></pre></td></tr></table></figure></p>
<h2><span id="horovod">horovod</span><a href="#horovod" class="header-anchor"></a></h2><p>horovod 在安装时，需要安装支持NCCL ，同时建议安装最新版本</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">// 安装horovod</span><br><span class="line">pip show horovod &amp;&amp; pip uninstall horovod</span><br><span class="line">HOROVOD_WITH_MPI=1 HOROVOD_WITHOUT_GLOO=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_WITH_TENSORFLOW=1 HOROVOD_NCCL_LINK=SHARED pip3 install horovod==0.24.3 --no-cache-dir</span><br></pre></td></tr></table></figure>
<p>验证一下horovod 使用了NCCL<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">horovodrun --check-build</span><br></pre></td></tr></table></figure></p>
<p>对应的NCCL 前面勾选则支持NCCL<br><img src="/2022/05/25/horovod/horovod-nccl.png" alt="horovod-nccl"></p>
<h1><span id="huan-jing-zhong-cai-guo-de-keng">环境中踩过的坑</span><a href="#huan-jing-zhong-cai-guo-de-keng" class="header-anchor"></a></h1><p>当前在A100 的机器上进行安装测试，其他型号显卡不一定也有同样的问题。</p>
<h2><span id="nccl">nccl</span><a href="#nccl" class="header-anchor"></a></h2><p>针对A100，ngc 的镜像中依然存在一个坑，其安装的nccl 版本太高，导致使用中会报错，对应的修复方式时对nccl 进行降级。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载编译nccl</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/nccl.git</span><br><span class="line"><span class="built_in">cd</span> nccl &amp;&amp; git checkout v2.8.3-1</span><br><span class="line">make -j src.build</span><br><span class="line"></span><br><span class="line"><span class="comment"># install</span></span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<h2><span id="horovod">horovod</span><a href="#horovod" class="header-anchor"></a></h2><p>nvcr.io/nvidia/tensorflow:22.04-tf1-py3 中的horovod 不是最新版本的，在使用梯度累计时，老版本里<a href="https://github.com/horovod/horovod/issues/2806" target="_blank" rel="noopener">有问题</a>，对应的修复在新版本的horovd 中。所以，不管是否使用梯度累计，都建议升级一下horovod。PS: 升级时参考上面安装horovod 的办法 ，注意对NCCL 的支持。</p>
<h1><span id="dai-ma-ceng-mian-xiu-gai">代码层面修改</span><a href="#dai-ma-ceng-mian-xiu-gai" class="header-anchor"></a></h1><p>代码层面需要引入horvod，然后将optimizer 用horovod 进行wrap，此外，evaluate 相关的内容，尽量使用一个节点即可。<br>简单的demo：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment"># import horovod</span></span><br><span class="line"><span class="keyword">import</span> horovod.tensorflow.keras <span class="keyword">as</span> hvd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Horovod: initialize Horovod.</span></span><br><span class="line">hvd.init()</span><br><span class="line"><span class="comment"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">config.gpu_options.visible_device_list = str(hvd.local_rank())</span><br><span class="line">K.set_session(tf.Session(config=config))</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="comment"># wrap optimizer</span></span><br><span class="line">optimizer = hvd.DistributedOptimizer(optimizer,</span><br><span class="line">                                    backward_passes_per_step=grad_accum_steps,</span><br><span class="line">                                    average_aggregated_gradients=<span class="literal">True</span>,</span><br><span class="line">                                    sparse_as_dense=<span class="literal">True</span>)</span><br><span class="line">                          </span><br><span class="line">...</span><br><span class="line"><span class="comment"># callback</span></span><br><span class="line">callbacks = [</span><br><span class="line">            <span class="comment"># Horovod: broadcast initial variable states from rank 0 to all other processes.</span></span><br><span class="line">            <span class="comment"># This is necessary to ensure consistent initialization of all workers when</span></span><br><span class="line">            <span class="comment"># training is started with random weights or restored from a checkpoint.</span></span><br><span class="line">            hvd.callbacks.BroadcastGlobalVariablesCallback(<span class="number">0</span>),</span><br><span class="line">            </span><br><span class="line">        ]</span><br><span class="line"><span class="comment"># evaluate only on worker 0 to prevent other workers from corrupting them.  </span></span><br><span class="line"><span class="keyword">if</span> hvd.rank() == <span class="number">0</span>:</span><br><span class="line">    callbacks.append(evaluator)</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>部分限制：</p>
<ol>
<li>由于horovod 是让你在单卡的代码能够平滑的迁移到多卡上，所以这里的batch size 设置的是针对单卡的，但是整个优化过程是在多卡的结果上进行的，所以需要你手动调整自己的optimizer 以适应最终的batch size(batch_size_node * node_num)</li>
<li>需要使用梯度累计以达到更大batch size 的效果时，需要使用horovod 提供的方式，即：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = hvd.DistributedOptimizer(optimizer,</span><br><span class="line">                                    backward_passes_per_step=grad_accum_steps,  <span class="comment"># 累计步数</span></span><br><span class="line">                                    average_aggregated_gradients=<span class="literal">True</span>,</span><br><span class="line">                                    sparse_as_dense=<span class="literal">True</span>                        <span class="comment"># 累计步数大于1必须设为True</span></span><br><span class="line">                                    )</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>而直接使用bert4keras 中的实现方式会报错，具体原因未知。</p>
<ol start="3">
<li>callback 中只支持原生的keras 的操作，其他自定义的操作都会报错，如bert4keras 中的 <code>Transformer.save_weights_as_checkpoint()</code> 就不支持。</li>
</ol>
<h1><span id="run-code">run code</span><a href="#run-code" class="header-anchor"></a></h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mpirun -np 8 --allow-run-as-root -<span class="built_in">bind</span>-to none -map-by slot -x NCCL_DEBUG=INFO -mca btl_tcp_if_include eth0 -x LD_LIBRARY_PATH -x PATH python train_hvd.py</span><br></pre></td></tr></table></figure>
<p>其中 $-np$ 后面的参数是显卡数量，根据使用情况自行调整。</p>
<h1><span id="jia-su-xiao-guo">加速效果</span><a href="#jia-su-xiao-guo" class="header-anchor"></a></h1><p>对不同的OS 不同的环境进行了训练速度对比，效果如下：</p>
<table>
<thead>
<tr>
<th>local system</th>
<th style="text-align:center">run where</th>
<th style="text-align:center">distributed</th>
<th style="text-align:center">speed</th>
</tr>
</thead>
<tbody>
<tr>
<td>centos</td>
<td style="text-align:center">docker</td>
<td style="text-align:center">tf-mirroredStrategy</td>
<td style="text-align:center">520 ms/step</td>
</tr>
<tr>
<td>centos</td>
<td style="text-align:center">docker</td>
<td style="text-align:center">horovod-without-nccl</td>
<td style="text-align:center">440 ms/step</td>
</tr>
<tr>
<td>centos</td>
<td style="text-align:center">docker</td>
<td style="text-align:center">horovod-nccl</td>
<td style="text-align:center">285 ms/step</td>
</tr>
<tr>
<td>ubuntu20.04</td>
<td style="text-align:center">local</td>
<td style="text-align:center">tf-mirroredStrategy</td>
<td style="text-align:center">500 ms/step</td>
</tr>
<tr>
<td>ubuntu20.04</td>
<td style="text-align:center">local</td>
<td style="text-align:center">horovod nccl</td>
<td style="text-align:center">265 ms/step</td>
</tr>
<tr>
<td>ubuntu20.04</td>
<td style="text-align:center">docker</td>
<td style="text-align:center">tf-mirroredStrategy</td>
<td style="text-align:center">500 ms/step</td>
</tr>
<tr>
<td><code>ubuntu20.04</code></td>
<td style="text-align:center"><code>docker</code></td>
<td style="text-align:center"><code>horovod-nccl</code></td>
<td style="text-align:center"><code>240 ms/step</code></td>
</tr>
</tbody>
</table>
<p>可以看到，相对于tf 原生的单机多卡的MirroredStrategy，能提速一倍之多，加速相当惊人了。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>可爱猫猫</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>speed-up</tag>
        <tag>horovod</tag>
      </tags>
  </entry>
  <entry>
    <title>tokenizers 总结</title>
    <url>/2022/09/08/tokenizers/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#tokenizer">tokenizer</a><ul>
<li><a href="#word-level">word level</a></li>
<li><a href="#char-level">char level</a></li>
<li><a href="#subword-level">subword level</a></li>
<li><a href="#bpe">BPE</a></li>
<li><a href="#bytes-bpe">Bytes BPE</a></li>
<li><a href="#wordpiece">WordPiece</a></li>
<li><a href="#unigram">Unigram</a></li>
<li><a href="#sentencepiece">SentencePiece</a></li>
<li><a href="#train-from-scratch">train from scratch</a></li>
</ul>
</li>
<li><a href="#tui-jian">推荐</a></li>
<li><a href="#bu-chong-yue-du">补充阅读</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div></p>
<h1><span id="tokenizer">tokenizer</span><a href="#tokenizer" class="header-anchor"></a></h1><p>目前的机器学习模型都是数学模型，其对应的输入要求必须是数字形式（number）的，而我们处理的真实场景往往会包含许多非数字形式的输入（有时候即使原始输入是数字形式，我们也需要转换），最典型的就是NLP 中的文字(string),为了让文字能够作为输入参与到模型的计算中去，我们就需要构建一个映射关系(mapping):将对应的文字映射到一个数字形式上去，而其对应的数字就是token。而对应的这个映射关系，就是我们的tokenizer：他可以将文字映射到其对应的数字上去(encode)，也可以将数字映射回对应的文字上(decode).</p>
<h2><span id="word-level">word level</span><a href="#word-level" class="header-anchor"></a></h2><p>那如何构建这个映射关系呢？最简单的想法：一个词对应一个id 不就好了。也就是”word level”。然后，”词”缺不是那么好分的:对于中文，词到底分成什么粒度，”苹果手机“到底是一个词还是二个词呢？”武汉市/长江/大桥/欢迎/你“与”武汉/市长/江大桥/欢迎/你“应该选择哪个方案呢？此外，对于”虚坤“/”鸡你太美“等这些新词怎么识别呢？对这部分感兴趣的可以看我更早期关于分词总结的博客：<a href="https://xv44586.github.io/2019/10/22/cutwords/">分词算法综述</a>.</p>
<p>中文有分词的问题，那英语总没有了吧，空格+标点就是天然分隔符，然而事情也没那么简单：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Don<span class="string">'t cry!</span></span><br></pre></td></tr></table></figure></p>
<p>按照空格与标点切分，得到的结果是：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&quot;Don&quot;, &quot;&apos;&quot;, &quot;t&quot;, &quot;cry&quot;, &quot;!&quot;]</span><br></pre></td></tr></table></figure></p>
<p>哈，有点怪，我们需要把”Don’t” -&gt; “Do”/“n’t”,这样才更合理一点。即：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&quot;Do&quot;, &quot;n&apos;t&quot;, &quot;cry&quot;, &quot;!&quot;]</span><br></pre></td></tr></table></figure></p>
<p>除了如何切分成词外，还有一个最大的问题是结果集太大。比如TransformerXL 中，使用标点和空格，切分后的词表大小有267K，如此大的词表，不管是对存储还是计算都有压力。</p>
<p>此外，随着文化发展，每天都有大批的新词出现，用词粒度做映射就越来越不理想了。</p>
<h2><span id="char-level">char level</span><a href="#char-level" class="header-anchor"></a></h2><p>既然分词这么麻烦，我不分不好了，我就按“字”(char) 来做最小单元做映射。这样词表就小多了：英文只需要26个字母即可，中文根据2013年<a href="http://www.moe.gov.cn/jyb_xwfb/s5147/201308/t20130828_156423.html" target="_blank" rel="noopener">中华人民共和国教育部《通用规范汉字表》定义“规范汉字”</a>，国家规定的通用规范汉字一共为8105个，相比之下也不算大。</p>
<p>然而char level 的主要问题是切分的太细：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pneumonoultramicroscopicsilicovolcanoconiosis</span><br><span class="line"></span><br><span class="line">（医）肺尘病、矽肺病</span><br></pre></td></tr></table></figure></p>
<p>上面这个单词是我找到的最长的英语单词，他有45个字母组成，而中文中也存在大量的成语/歇后语/专用名词等，如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">只许州官放火不许百姓点灯</span><br></pre></td></tr></table></figure></p>
<p>目前我们NLP 的主要思路是对句子进行，即：<br>$$<br>P(S)=P(w_1,w_2,..w_n)=P(w_1)∗P(w_2|w_1)∗P(w_3|w_1,w_2)∗…∗P(w_n|w_1,w_2,..w_{n−1})<br>$$<br>而切分太细，则对应$S$ 的长度会变长，无疑大大增加建模难度（通过字来学习词的语义），也常常导致模型效果不理想。</p>
<h2><span id="subword-level">subword level</span><a href="#subword-level" class="header-anchor"></a></h2><p>为了缓解两种方式的问题，一个想法是取长补短，即目前的主流方案：subword level.</p>
<p>subword level 的主要出发点是：我们应该尽量保留高频词，对低频词进行逐级切分，以获得一个词表大小合适，又对后续建模友好的方案。</p>
<p>目前subword level 的tokenizer 方法主要有BPE, Bytes BPE, WordPiece, Unigram, SentencePiece,下面简单总结一下各个方法。 </p>
<h2><span id="bpe">BPE</span><a href="#bpe" class="header-anchor"></a></h2><p>bpe 的方案是通过统计词频来确定两个相邻的pair subwords 要不要合并，具体做法：<br>1.统计pre-tokenize 的word 的词频；<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(<span class="string">"hug"</span>, 10), (<span class="string">"pug"</span>, 5), (<span class="string">"pun"</span>, 12), (<span class="string">"bun"</span>, 4), (<span class="string">"hugs"</span>, 5)</span><br></pre></td></tr></table></figure></p>
<p>2.使用词典对word 进行切分：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># base vocabulary: ["b", "g", "h", "n", "p", "s", "u"]</span></span><br><span class="line">(<span class="string">"h"</span> <span class="string">"u"</span> <span class="string">"g"</span>, 10), (<span class="string">"p"</span> <span class="string">"u"</span> <span class="string">"g"</span>, 5), (<span class="string">"p"</span> <span class="string">"u"</span> <span class="string">"n"</span>, 12), (<span class="string">"b"</span> <span class="string">"u"</span> <span class="string">"n"</span>, 4), (<span class="string">"h"</span> <span class="string">"u"</span> <span class="string">"g"</span> <span class="string">"s"</span>, 5)</span><br></pre></td></tr></table></figure></p>
<p>3.统计相邻两个subword pair 词频，将top-k 高的pair 合并生成新的subword，添加进vocabulary，同时，如果当前的subword  只会同pair 一起出现，则同时将vocabulary 中对应subword 删除。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># count pair</span></span><br><span class="line">h + u = 10 + 5 = 15</span><br><span class="line">u + g = 10 + 5 +  = 20</span><br><span class="line">...</span><br><span class="line"><span class="comment"># merge top k</span></span><br><span class="line"><span class="built_in">set</span> k = 1</span><br><span class="line">ug -&gt; vocabulary</span><br><span class="line">base vocabulary: [<span class="string">"b"</span>, <span class="string">"g"</span>, <span class="string">"h"</span>, <span class="string">"n"</span>, <span class="string">"p"</span>, <span class="string">"s"</span>, <span class="string">"u"</span>, <span class="string">"ug"</span>]</span><br><span class="line">​</span><br><span class="line">loop until vocabulary match vocab_size</span><br><span class="line">​</span><br></pre></td></tr></table></figure></p>
<h2><span id="bytes-bpe">Bytes BPE</span><a href="#bytes-bpe" class="header-anchor"></a></h2><p>Bytes BPE 与BPE基本相同，唯一不同的是：BPE 中会存在UNK 的情况，为了解决unk 的问题，一个非常天才的想法是将所有text 先转为<code>bytes</code> ，这样就不会存在unk 的问题，尤其是在多语言中，这种方式可以大大缩减词表大小;此外即使不是目标语言训练的模型也可以拿来使用。通常词表大小包括256 个基本bytes + &lt;end|of|text&gt; + vocab-size,如gpt2 的词表为50257: 256 base bytes tokens + &lt;end|of|text&gt; + 50,000 merges.<br>此外，训练bytes bpe 时，通常我们还会选择先将文本进行normalize，这部分后面会进一步说明。</p>
<h2><span id="wordpiece">WordPiece</span><a href="#wordpiece" class="header-anchor"></a></h2><p>WordPiece 与BPE 也非常相似，区别主要在于merge 的策略：BPE 中选择频率最高的pair 进行合并，WordPiece 则选择使用语言模型来进行选择：<br>$$<br>L = logP(S) = \sum^Nlog(P_i)<br>$$<br>对于两个subword： $t_x$, $t_y$ ，合并后为 $t_z$ ，则合并前后的增益：<br>$$<br>Loss = logP(t_z) - (logP(t_x) + logP(t_y))<br>$$<br>通过计算合并增益是否增大来决定是否合并subword pair.</p>
<h2><span id="unigram">Unigram</span><a href="#unigram" class="header-anchor"></a></h2><p>Unigram 与 上述的方法都略有不同：Unigram 不再是通过合并base vocabulary 中的subword 来新增，他选择在初始化时初始化一个非常大的subword set，通过计算是否需要将一个subword 切分为多个base subword （remove 这个subword）来减小vocabulary size 直到达到vocab size。<br>这里有一个假设：句子之间是独立的，subword 与 subword 之间是独立的。对应的句子的语言模型似然值就是其subword 的概率的乘积。目标是保存vocab size 的同时语言模型似然值最大。<br>$$<br>x^* = argmax_{x \in U}P(\overrightarrow{x})<br>$$<br>整个求解过程是一个简单的EM 或者说一个迭代过程：<br>0.建立一个足够大的种子subword vocabulary，可以用字典树构建可以是所有字符的组合，也可以用bpe 构建；<br>1.（期望E）统计vocabulary 中每个subword 的频率，计算其对应概率值；<br>2.（最大化M）根据其概率，使用维特比算法返回其语言模型似然值最大化下的最佳分割方案；<br>3.计算最佳分割方案下每个新子词的loss，这里的loss 是指将当前subword 从vocabulary 中移除时，对应的语言模型似然值，即<br>$$<br>L = − \sum^Nlog (\sum_{x∈S(x_i)}p(x))<br>$$<br>4.丢弃掉loss 前x% 对应的subword；<br>5.重复2-4阶段，直到vocabulary 达到指定的vocab size。</p>
<h2><span id="sentencepiece">SentencePiece</span><a href="#sentencepiece" class="header-anchor"></a></h2><p>SentencePiece 其实并不是一个新的tokenizer 方法，他其实是一个实现了BPE/Unigram tokenizer 的一个集合，不过他有一些创新的地方。<br>上述方法中有一些问题：<br>1.都有字，子词或词的概念，然而在很多语言中并没有这样的概念；<br>2.都默认需要自己进行pre-tokenize，如英语则利用“空格”作为词的分割符，中文则一般选择jieba 进行pre-tokenize，这个过程不同的语言有自己的一套做法，不统一；<br>3.token 格式不统一。以英文为例，表示token 时会有 ##xx, xx/s 这种，表示subword 是否出现在词的首尾，然而中文中是没有这种概念的；<br>4.解码困难，如BPE解码时需要进行一些标准化，最常见的是去除标点符号。而我们解码后是 [new] [york]两个token，我们并不知道原来的词是 newyork/new york/new-york 中的哪一个.<br>SentencePiece 的做法：<br><a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">SentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol. To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol “▁” (U+2581) as follows.</a></p>
<p>即首先将空格转换为一个标准的字符”▁”,然后将text 转换为unicode，其实这里的unicode 是NFKC-based normalization后的unicode，至于unicode 标准化，可以参考<a href="https://blog.csdn.net/weixin_43866211/article/details/98384017" target="_blank" rel="noopener">unicode文本标准化</a> ，虽然通常我们使用NFKC 标准化，但sentencepiece 内部四种方法都实现了。<br>通过上述的空格转换加normalize，所有的语言经过转换后就有统一的格式了，这样多语言的问题就彻底的与token 切分解偶了，tokenizer 就有了一个完全端到端的解决方案。</p>
<h2><span id="train-from-scratch">train from scratch</span><a href="#train-from-scratch" class="header-anchor"></a></h2><p>训练一个tokenizer model 主要有两个仓库可以参考： <a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">huggingface/tokenizers</a>   和  <a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">google/sentencepiece</a>.<br>其中tokenizers 支持bpe/bytes bpe/unigram/wordpiece, sentencepiece 支持bpe/unigram.两者都支持四种标注化方法。<br>此外，tokenizers 不支持自定义的pre_tokenizer的保存，如中文时我们常用的jieba.lcut ；bytes bpe不支持big dataset 的训练，1T  内存训练100G 文本也会因内存不足被killed，（一个办法是缩小语料训练，因为没有oov 的问题，基本上小语料下训练也能用）。<br>在实际使用时，通常会结合huggingface/transformers 一起使用，这里也看了一下transformers/tokenizers 的实现。原始的GPT2 中的tokenizer 是没有做normalize 的，所以 transformers中的GPT2Tokenizer 也是没有做normalize 的，而通常我们自己训练的bytes bpe 是会加一个normalize 的过程，所以如果是通过huggingface/tokenizers 训练的tokenier，迁移到transformers 时需要注意normalize 是否实现。</p>
<h1><span id="tui-jian">推荐</span><a href="#tui-jian" class="header-anchor"></a></h1><p>当目标语言为中文时，推荐使用WordPiece + jieba 的方案；而是多语言场景时，推荐使用SentencePieceBPE/SentencePieceUnigram.<br>无论哪种合并/切分 subword 的策略，我们的初衷是:<br><code>尽量不切分常用词，而是将不常用词切分为常用的子词.</code><br>而中文中，有明确的字/词概念，却没有子词的概念（如英文中有”app”, “##le”, 中文却没有”苹” “##果”），而转bytes 后对子词更友好，此外，中文通常需要3个bytes（GBK）或者4个bytes（Chinese-Japanese character set），对于一个中文的字，很有可能需要大于1个token 来表示，反而会增加tokenize 后序列的长度，对模型的训练与使用不利；此外，中文中空格也没有切分词/句子 的语义，保留空格反而会由于各种空格的错误使用带来问题，最终的推荐方案就是jieba +  Word Piece/SentencePieceUnigram。<br>而多语言场景下，推荐使用SentencePieceBPE，他提供一个端到端的方案，而不需要再根据不同语言进行不同的pre-tokenize/subword 格式，此外，SentencePiece 都是bytes 粒度的，这样既能大大缩减词表又能避免unk 的情况。</p>
<h1><span id="bu-chong-yue-du">补充阅读</span><a href="#bu-chong-yue-du" class="header-anchor"></a></h1><p><a href="https://huggingface.co/docs/transformers/tokenizer_summary" target="_blank" rel="noopener">tokenizer summary-huggingface</a><br><a href="https://towardsdatascience.com/difference-between-nfd-nfc-nfkd-and-nfkc-explained-with-python-code-e2631f96ae6c" target="_blank" rel="noopener">Difference Between NFD, NFC, NFKD, and NFKC Explained with Python Code-medium</a><br><a href="https://zhuanlan.zhihu.com/p/86965595" target="_blank" rel="noopener">深入理解NLP Subword算法：BPE、WordPiece、ULM-知乎</a><br><a href="https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html#121_expectation-maximization" target="_blank" rel="noopener">natural_language_understanding/subword_units/subword_units-github.io</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>本人真实写照🐶</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>BPE</tag>
        <tag>WordPiece</tag>
        <tag>Unigram</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-4 yes!! but</title>
    <url>/2023/03/25/gpt4/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#gpt-4-yes">GPT-4 yes</a></li>
<li><a href="#gpt-4-cun-zai-de-wen-ti">GPT-4存在的问题</a><ul>
<li><a href="#bu-kai-yuan">不开源</a></li>
<li><a href="#shu-ju-an-quan">数据安全</a></li>
<li><a href="#zi-yuan-xiao-hao-da">资源消耗大</a></li>
</ul>
</li>
<li><a href="#nlp-ke-zuo-de-fang-xiang">NLP可做的方向</a><ul>
<li><a href="#hallucination">hallucination</a></li>
<li><a href="#fu-xian-gpt-4-chatgpt-gpt-3-5-instructgpt">复现GPT-4/ChatGPT/GPT-3.5/InstructGPT</a></li>
<li><a href="#ru-he-ping-gu-llm">如何评估LLM</a></li>
<li><a href="#zhi-chi-chang-wen-ben">支持长文本</a></li>
<li><a href="#bian-xiao-bian-kuai">变小变快</a></li>
<li><a href="#di-cheng-ben-inference">低成本inference</a></li>
<li><a href="#di-cheng-ben-you-hua">低成本优化</a></li>
<li><a href="#you-hua-qi">优化器</a></li>
<li><a href="#geng-ke-kong">更可控</a></li>
<li><a href="#shi-bie-aigc">识别AIGC</a></li>
<li><a href="#dan-yi-ren-wu-ling-yu-shua-bang">单一任务/领域刷榜</a></li>
</ul>
</li>
<li><a href="#he-qu-he-cong">何去何从</a><ul>
<li><a href="#pu-tong-gong-cheng-shi">普通工程师</a></li>
<li><a href="#pu-tong-yong-hu">普通用户</a></li>
</ul>
</li>
<li><a href="#fan-wai">番外</a><ul>
<li><a href="#tong-guo-prompt-gou-jian-ji-zhu-bi-lei-shen-qing-prompt-zhuan-li">通过prompt 构建技术壁垒/申请prompt 专利</a></li>
<li><a href="#hui-bu-hui-shi-ye">会不会失业</a></li>
</ul>
</li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
<li><a href="#buy-me-a-coffee">Buy me a coffee</a></li>
</ul>
<!-- tocstop -->
</div>

<p>这篇博客简单讨论下在GPT-4 如此强大的技术冲击下，我们NLPer该何去何从。<br>首先说下我的结论：GPT-4 非常强大，但是还没有到完全取代我们工作的地步，我们依然有很多能做的方向。</p>
<h1><span id="gpt-4-yes">GPT-4 yes</span><a href="#gpt-4-yes" class="header-anchor"></a></h1><ol>
<li>更可靠了（胡说八道进一步降低）</li>
<li>性能更好：比GPT-3.5 又提升了一大截</li>
<li>reverse inverse scaling prize:一些随着模型变大性能下降的任务在GPT-4上不再出现类似现象（曾经没法通过增大模型规模提升性能的任务现在也解决了）</li>
<li>能够用图像做prompt：增加图像信息能进一步提升性能（看图说话，类似BLIP2，这个对盲人太友好了）</li>
<li>进一步closeAI</li>
</ol>
<h1><span id="gpt-4-cun-zai-de-wen-ti">GPT-4存在的问题</span><a href="#gpt-4-cun-zai-de-wen-ti" class="header-anchor"></a></h1><h2><span id="bu-kai-yuan">不开源</span><a href="#bu-kai-yuan" class="header-anchor"></a></h2><p>由于GPT-4 完全不公布任何技术细节，所以他为什么有如此强大的能力，我们只能猜，想要研究它变得困难重重。</p>
<h2><span id="shu-ju-an-quan">数据安全</span><a href="#shu-ju-an-quan" class="header-anchor"></a></h2><p>ChatGPT 的火爆让大家突然忘了曾经非常看重的数据安全问题，preview 版是有可能会参与下次迭代的；而商用api 即使强调不会用于模型训练，敏感业务数据你敢用吗？</p>
<h2><span id="zi-yuan-xiao-hao-da">资源消耗大</span><a href="#zi-yuan-xiao-hao-da" class="header-anchor"></a></h2><p>即使是GPT-3 也有175B 参数，训练/推理都是极其消耗资源的，从GPT-4 的价格上涨了50% 来看，GPT-4 的推理消耗资源也上升了50% 左右。</p>
<h1><span id="nlp-ke-zuo-de-fang-xiang">NLP可做的方向</span><a href="#nlp-ke-zuo-de-fang-xiang" class="header-anchor"></a></h1><p>这也是最近讨论比较热烈的一个问题，回答这个问题前，不妨先思考一下理想的NLP 模型应该具有哪些特征。我认为比较理想的模型是：安全可靠/支持长文本/小/快/私有化部署。所以我仅从个人出发，给出一些我比较关注的方向。</p>
<h2><span id="hallucination">hallucination</span><a href="#hallucination" class="header-anchor"></a></h2><p>目前LLM 最大的问题就是hallucination(一本正经的胡说八道)。目前主流两种思路：alignment/多模态。</p>
<ul>
<li>alignment<br>至于如何做alignment ，学术界主要是instruction-tuning为主，OpenAI 的路线是RLHF，然而普通玩家我是完全不推荐做RL的，只要仔细阅读InstructGPT/GPT-4 paper中关于reward model 部分就能劝退了。所以对于我们普通玩家，是否有别的路径？</li>
<li>多模态<br>GPT4 的paper 上看效果是不错的，我没做过，不多说了。</li>
</ul>
<h2><span id="fu-xian-gpt-4-chatgpt-gpt-3-5-instructgpt">复现GPT-4/ChatGPT/GPT-3.5/InstructGPT</span><a href="#fu-xian-gpt-4-chatgpt-gpt-3-5-instructgpt" class="header-anchor"></a></h2><p>不开源只能复现，目前主要有<a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener">facebookresearch/llama</a>/<a href="https://huggingface.co/bigscience/bloom" target="_blank" rel="noopener">bigscience/bloom</a>,此外还有不开源但是可以使用API 访问的百度文心一言/ChatGLM 等。</p>
<h2><span id="ru-he-ping-gu-llm">如何评估LLM</span><a href="#ru-he-ping-gu-llm" class="header-anchor"></a></h2><p>我们说百度文心一言性能不行时，到底如何不行？这里就牵扯到如何量化的评估LLM 的性能。曾经自动化的方案及benchmark 的参考意义随着LLM 的能力提升显得越来越弱，现在急需新的数据集/评估方案。目前的工作有<a href="https://github.com/openai/evals" target="_blank" rel="noopener">openai/evals</a>/<a href="https://github.com/stanford-crfm/helm" target="_blank" rel="noopener">stanford-crfm/HELM</a></p>
<h2><span id="zhi-chi-chang-wen-ben">支持长文本</span><a href="#zhi-chi-chang-wen-ben" class="header-anchor"></a></h2><p>更长的输入对某些任务是有利的，如何让模型支持更长的输入呢？主要的思路有两个：</p>
<ul>
<li>训练时使用较短文本，推理时外推更长的位置信息，使模型获得处理长文本的能力，如bloom 中使用的<a href="https://arxiv.org/pdf/2108.12409.pdf" target="_blank" rel="noopener">ALiBI</a></li>
<li>调整模型结构，如最近的工作：<a href="https://arxiv.org/pdf/2303.09752.pdf" target="_blank" rel="noopener">CoLT5:Faster Long-Range Transformers with Conditional Computation</a><br>PS: GPT-4 的输入从GPT-3.5 的4K(or 8K?) 提升到了30K，如何做的呢？</li>
</ul>
<h2><span id="bian-xiao-bian-kuai">变小变快</span><a href="#bian-xiao-bian-kuai" class="header-anchor"></a></h2><p>相同架构的模型通常变小就会变快，让模型变小的方法主要是蒸馏/量化/train 小模型，这个方向目前工作有<a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank" rel="noopener">stanford_alpaca</a>/<a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener">bitsandbytes</a>，中文上也有<a href="https://github.com/THUDM/ChatGLM-6B" target="_blank" rel="noopener">ChatGLM-6B</a>/<a href="https://github.com/LianjiaTech/BELLE" target="_blank" rel="noopener">BELLE</a>等。</p>
<h2><span id="di-cheng-ben-inference">低成本inference</span><a href="#di-cheng-ben-inference" class="header-anchor"></a></h2><p>如何在低成本设备上使用这些模型？如单张GPU 上跑大模型或普通CPU 上跑模型。这个方向的工作也有<a href="https://github.com/FMInference/FlexGen" target="_blank" rel="noopener">FlexGen</a>/<a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener">llama.cpp</a> 等。</p>
<h2><span id="di-cheng-ben-you-hua">低成本优化</span><a href="#di-cheng-ben-you-hua" class="header-anchor"></a></h2><p>低成本fine-tuning 主要有两个方向：parameter-efficient / sample-efficient.</p>
<ul>
<li>parameter-efficient  的思路目前主要有prompt-tuning/prefix-tuning/LoRA/Adapter 等，参考<a href="https://github.com/huggingface/peft" target="_blank" rel="noopener">huggingcae/peft</a></li>
<li>sample-efficient 可以帮助我们如何更有效的构造训练集，最近的工作有<a href="http://arxiv.org/abs/2303.08114" target="_blank" rel="noopener">Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs</a></li>
</ul>
<h2><span id="you-hua-qi">优化器</span><a href="#you-hua-qi" class="header-anchor"></a></h2><p>优化器决定了我们训练时需要的资源。虽然我们通常使用Adam 优化器，但是其需要2倍额外显存，而google 好像用Adafactor 更多一点，最近他们又出了一个新工作<a href="https://arxiv.org/abs/2302.06675" target="_blank" rel="noopener">Lion</a>.</p>
<h2><span id="geng-ke-kong">更可控</span><a href="#geng-ke-kong" class="header-anchor"></a></h2><p>如从可控生成角度看，目前可控主要通过control token（prompt）来实现，有没有更好的办法来实现更“精细”的控制，就如controlnet 之于stable diffusion。</p>
<h2><span id="shi-bie-aigc">识别AIGC</span><a href="#shi-bie-aigc" class="header-anchor"></a></h2><p>如何判别内容是人写的还是模型生成的呢？随着模型的性能越来越强，识别AIGC 也越来越困难。目前的工作也有watermark/<a href="https://gptzero.me/" target="_blank" rel="noopener">GPTZero</a> 等。不过我感觉还没什么特别有效的方案目前。<br>对此我有个简单的思路：将AI 生成的与非AI 生成的看作是两种不同的语言，如code 与英语一样，虽然都是相同符合构成，但是对应不同语言。使用大量的AI 生成的内容（或人机交互数据）pretrain 一个”AI 语言模型“，再来进行识别。</p>
<h2><span id="dan-yi-ren-wu-ling-yu-shua-bang">单一任务/领域刷榜</span><a href="#dan-yi-ren-wu-ling-yu-shua-bang" class="header-anchor"></a></h2><p>我认为在某个任务/领域上通过小模型挑战大模型依然有意义，LLM 虽然强大，但是依然有太多我们不知道的能力，通过小模型刷榜也许能提供一些思路，就像PET 本意是调战GPT-3，却打开了LLM 的新思路。</p>
<h1><span id="he-qu-he-cong">何去何从</span><a href="#he-qu-he-cong" class="header-anchor"></a></h1><h2><span id="pu-tong-gong-cheng-shi">普通工程师</span><a href="#pu-tong-gong-cheng-shi" class="header-anchor"></a></h2><p>这种新的革命性的技术我们普通工程师通常都不是第一线的，我第一次真正使用bert 也是在其出来两年后了。即使今天，也有很多场景/公司不使用bert这个技术。<br>换个角度，即使我们想参与，我想能参与训练/fine-tuning 一个10B 规模模型的工程师都相当少，更别提更大的了。所以到底是“左右逢源”还是“举步维艰”，让子弹飞一会儿吧。</p>
<h2><span id="pu-tong-yong-hu">普通用户</span><a href="#pu-tong-yong-hu" class="header-anchor"></a></h2><p>普通用户我觉得应该就是多读书，提高自己的鉴别能力了。”生活中不缺少美，而是缺少发现美的眼睛。”</p>
<h1><span id="fan-wai">番外</span><a href="#fan-wai" class="header-anchor"></a></h1><h2><span id="tong-guo-prompt-gou-jian-ji-zhu-bi-lei-shen-qing-prompt-zhuan-li">通过prompt 构建技术壁垒/申请prompt 专利</span><a href="#tong-guo-prompt-gou-jian-ji-zhu-bi-lei-shen-qing-prompt-zhuan-li" class="header-anchor"></a></h2><p>随着alignment 的进一步优化，LLM 通常越来越理解自然语言，所以我认为prompt-trick 越来越不重要，而清晰准备的用prompt 描述你的需求越来越重要。所谓技术壁垒也许就是如何更清晰有效的描述需求了，但也很难形成技术壁垒。<br>至于专利，软件著作权保护的是制作软件这个技术本身，而非你使用软件时的姿势，所以我想单独的prompt 应该也不会形成专利，但是作为你某个技术的一部分，还是有可能的。</p>
<h2><span id="hui-bu-hui-shi-ye">会不会失业</span><a href="#hui-bu-hui-shi-ye" class="header-anchor"></a></h2><p>我认为不会失业，但是会转变一部分人的工作方式。在计算这件事上，人类早已被计算机远远的甩在后面，而计算机的出现也带来了大量的新工作。尤其是LLM 现阶段的表现是“懂开车的人才能开车”，所以需要更多更懂某个业务，更熟练使用LLM 工具的人。</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>放张动漫图，据说能缓解焦虑</p>
<h1><span id="buy-me-a-coffee">Buy me a coffee</span><a href="#buy-me-a-coffee" class="header-anchor"></a></h1><p>如果觉得这篇文章不错，对你有帮助，欢迎打赏一杯蜜雪冰城。</p>
<p><img src="/img/sponsor.JPG" alt="赞赏"></p>
<script src="https://giscus.app/client.js" data-repo="xv44586/giscus" data-repo-id="R_kgDOIC6Ipg" data-category="Announcements" data-category-id="DIC_kwDOIC6Ips4CRkmo" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async>
</script>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>GPT-4</tag>
      </tags>
  </entry>
  <entry>
    <title>from softmax to crf</title>
    <url>/2019/12/26/from-softmax-to-crf/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#xu-lie-biao-zhu">序列标注</a><ul>
<li><a href="#mo-xing">模型</a></li>
<li><a href="#shu-xue-xing-shi">数学形式</a></li>
</ul>
</li>
<li><a href="#yan-shi-jian-zhou-softmax">沿时间轴Softmax</a></li>
<li><a href="#crf">CRF</a></li>
<li><a href="#xian-xing-lian-crf">线性链CRF</a></li>
<li><a href="#qiu-jie">求解</a></li>
<li><a href="#demo">demo</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
</div>

<p>又做NER相关东西， 用到了CRF，所以想给组里人从头一步一步的将CRF讲一遍，希望大家看完能明白CRF的数学模型已经工程上的使用。<br>网上关于CRF大多数都是将他与HMM及概率图模型一起对比着讲，但是我觉得这需要一些背景知识，鉴于上次分享发现大家并没有什么背景知识，<br>所以这次希望能尽量减少背景知识就能让人搞懂CRF。</p>
<h1><span id="xu-lie-biao-zhu">序列标注</span><a href="#xu-lie-biao-zhu" class="header-anchor"></a></h1><h2><span id="mo-xing">模型</span><a href="#mo-xing" class="header-anchor"></a></h2><p>通常CRF出现在序列标注任务中，所以我们先来看看序列标注主要是做什么的。<br>序列标注是NLP中一个重要的任务，它包括分词、词性标注、命名实体识等。下面用一个分词的例子来简单说明。（<a href="https://spaces.ac.cn/archives/5542/comment-page-1#comments" target="_blank" rel="noopener">原文</a>)<br>假设我们现在用$bmes$四标签来进行分词，其中b 代表begin即词的开头， m代表middle即词内，e代表end即词的结尾，s代表single即单独成词。<br>现在我们有一个字符串序列 “今天天气不错”，如果对应的分词结果为“今天/天气/不/错”，则其标签序列为“bebess”。由于在序列标注中，我们认为正确的标签序列是唯一的，<br>所以我们的目标就是在所有可能的标签序列中（如bbbbbb,ssssss)挑选出真实的标签序列（bebess), 即最大化概率$P(bebess|今天天气不错）$。<br><img src="/2019/12/26/from-softmax-to-crf/seg.png" alt="4tag分词网络示意图"><br>即在上图中，所有从左至右的连线中，选出黄色的那条。</p>
<h2><span id="shu-xue-xing-shi">数学形式</span><a href="#shu-xue-xing-shi" class="header-anchor"></a></h2><p>我们假设输入序列是$X=[x_1, x_2, x_3, …, x_n]$,对应的输出序列是 $Y = [y_1, y_2, …, y_n]$,<br>label的集合为$L = [l_1, l_2, … , l_k]$. 任务目标是让真实的输出序列的概率最大，即：<br>$$<br>Max(P(y_1,y_2,..y_n|X))<br>$$</p>
<h1><span id="yan-shi-jian-zhou-softmax">沿时间轴Softmax</span><a href="#yan-shi-jian-zhou-softmax" class="header-anchor"></a></h1><p>直接对上述模型进行求解比较困难，所以我们先将问题简化，然后在对简化后的问题进行求解。<br>首先，我们引入朴素假设：即标签之间独立不相关，对应的目标就简化为：<br>$$<br>Max(P(y_1|X)P(y_2|X)…P(y_n|X))<br>$$<br>为了对$P(y_i|X)$进行建模，通常我们先通过RNNs（LSTM，BiLSTM, etc)来捕获输入X的全局信息，获得隐藏状态序列$\bar{x_1}, \bar{x_2}, …, \bar{x_n}$,<br>此时的$\bar{x_i}$可以看作是$x_i$ 通过X获取的特征，由于RNNs可以捕获全局信息，所以我们认为特征$\bar{x_i}$之间互不相关，对应的<br>$$P(y_i|X) = P(y_i|\bar{x_i})$$<br>我们的目标：<br>$$<br>Max(P(y_1|X)P(y_2|X)…P(y_n|X))<br>= Max(P(y_1|\bar{x_1})P(y_2|\bar{x_2})..P(y_n|\bar{x_n}))<br>= Max(\prod(P(y_i|\bar{x_i})))<br>$$<br>此时只需要$Max(P(y_i|\bar{x_i}))$进行求解，即沿时间轴一步一步的对RNNs的隐藏输出通过softmax来最大化对应目标标签概率。<br><img src="/2019/12/26/from-softmax-to-crf/softmax.png" alt="沿时间轴softmax"></p>
<h1><span id="crf">CRF</span><a href="#crf" class="header-anchor"></a></h1><p>因为在上一个方案中，我们做了朴素建设，将输出序列看作是相互独立的一元模型，这样会引入一些问题，如在分词中（bmes），m- 标签不能出现在s- 后面，<br>s- 标签不能出现在b- 和m-后面等，所以即使在上述方案中，至少也需要人为的设置一个“转移矩阵”，将不合理的转移方式得分置为0，来避免不合理方案的出现。<br>而上述方案出现错误的原因，本质上是因为我们的朴素假设：标签之间相互独立。为了解决上述方案的问题，我们至少需要在输出端显式考虑标签的关联性，即输出标签与上下文相关。<br><img src="/2019/12/26/from-softmax-to-crf/crf_example.png" alt="显式考虑输出端上下文"><br>显式考虑输出端上下文相关</p>
<p>现在我们回到原始目标上来，原始目标是$Max(P(y_1,y_2,..y_n|X)) $, 上个方案中我们是因为直接对$P(y_1, y_2,…y_n|x_1, x_2, …,x_n)$直接建模很难，<br>所以才做出了假设，简化目标。现在让我们换个思路，为了求解上述概率，我们还可以穷举出输出序列所有的可能结果$Y_1, Y_2, …, Y_{k^n}$,<br>然后如果能计算出当前输入X对应每种可能的输出序列的“值”， 则我们可以通过Softmax计算出真实输出序列的概率，即得到$P(Y_{true}|X)$。<br><code>假设一：我们可以学习一个打分函数f，通过函数f可以得到输出序列关于输入的得分，即$score_i = f(Y_i, X)$</code><br>此时，我们的目标就转化为 $P(Y|X) = \frac{exp(f(y_1, y_2,…y_n; X))}{Z(X)}$.<br>其中$Z(X) = \sum_{i=1}^{k^n}(exp(f(Y_i, X)))$。即此时的概率P是一个指数分布。<br>此时我们的方案是<code>1</code>个<code>$k^n$</code>多分类问题，即我们是对一个完整的输出序列为单位来计算概率（路径积分），而上一个方案中，是<code>n</code>个<code>k</code>分类问题，<br>这是两个方案的不同点之一。<br>在方案一中我们也说过，直接对完整序列建模比较困难，此时我们直接对$f(Y_i, X)$求解也会面临相同的困难，为了避免方案一的问题，我们不再使用一元模型，改为二元模型。<br><code>引入一阶马尔可夫假设，且其关联性是加性的。即当前输出标签只与前一个输出标签相关，其总得分是对所有得分求和。</code><br>此时的目标就转化为<br>$$<br>P(y_1, y_2, …, y_n|X)<br>= P(y_1|X)P(y_2|y_1;X)…P(y_n|y_{n-1}|X)<br>= P(y_1|X)\frac{P(y_1,y_2|X)}{P(y_1|X)P(y_2|X)} P(y_2|X) … \frac{P(y_{n-1}, y_n|X)}{P(y_{n-1}|X)P(y_n|X)} P(y_n|X)<br>$$<br>假设一中我们假设P是一个指数分布，所以此时我们引入两个函数e 和 t：e对$P(y_i|X)$建模, t 对$\frac{P(y_1,y_2|X)}{P(y_1|X)P(y_2|X)}$建模，即：<br>$$<br>P(y_i|X) = exp(e(y_i, X))<br>\frac{P(y_{i-1},y_1|X)}{P(y_{i-1}|X)P(y_i|X)} = P(y_i|X)exp(t(y_{i-i}, y_i; X))<br>$$</p>
<p>此时的目标就化简为：<br>$$<br>P(y_1, y_2, …, y_n|X)<br>= \frac{exp(f(y_1, y_2,…y_n; X))}{Z(X)}<br>= \frac{1}{Z(X)} exp(e(y_i,X) + t(y_1, y_2; X) + e(y_2, X) + … + t(y_{n-1}, y_n; X) + e(y_n, X))<br>$$<br>此时我们只需要对每个标签和相邻标签打分，然后将所有打分求和，即可得到总得分，然后对目标进行求解。</p>
<h1><span id="xian-xing-lian-crf">线性链CRF</span><a href="#xian-xing-lian-crf" class="header-anchor"></a></h1><p>虽然上面已经做了大量简化，但是求解时依然比较困难，主要是在求解t中，因为需要同时对输入X与标签$y_i$, $y_{i-1}$同时考虑，而在e中，<br>已经将输入与输出的关联进行了建模，此时我们引入线性链假设：假设t与输入X无关，则此时$ t(y_{i-1}, y_i;X) = t(y_{i-1}, y_i)$，那打分函数f简化为：<br>$$<br>f(y_1, y_2, …, y_n;X)<br>= e(y_1,X) + t(y_1, y_2) + e(y_2, X) + … + t(y_{n-1},y_n;X) + e(y_n,X)<br>$$<br>此时t就是一个待训练的参数矩阵，而e则可以通过RNNs来建模，概率分布也变为：</p>
<p>$$<br>P(y_1, y_2,…,y_n|x_1, x_2,…,x_n)<br>= exp(e(y_1, X) + \sum_{i=1}^{n-1}[t(y_i, y_{i+1}) + e(y_{i+1}, X)])) \frac{1}{Z(X)}<br>$$</p>
<h1><span id="qiu-jie">求解</span><a href="#qiu-jie" class="header-anchor"></a></h1><p>为了求解模型，我们用最大似然法， 即：</p>
<p>$$loss = -logP(y_1, y_2, …, y_n|x_1, x_2, …, x_n)$$<br>将上式代入：</p>
<p>$$loss = logZ(X) - (e(y_1, X) + \sum_{i=1}^{n-1}[t(y_i, y_{i+1}) + e( y_{i+1} , X)]) $$</p>
<p>减号后面的项通过添加一个待训练的参数矩阵循环计算即可得到结果，难算的前面的归一化因子Z(X)。前面我们也说了，我们此时是以路径为单位，<br>则此时Z(X) 需要我们穷举所有可能的路径比对其打分求和，而此时的路径有 $k^n$条，是指数级的，直接算效率太低，几乎不可能。<br>在假设二中，我们引入了一阶马尔可夫假设，当前标签只与前一个标签有联系，因此我们可以递归的计算归一化因子，这使得原来是指数级的计算量降低为线性级别。<a href="https://spaces.ac.cn/archives/5542/comment-page-1#comments" target="_blank" rel="noopener">原文</a><br>（这点是求解归一化因子的关键，最初我在推导时一直卡在这点上）<br>具体的: 将计算到时刻t的归一化因子记为Zt，并将它安装标签分为k个部分，即：</p>
<p>$$Zt = Zt^1 + Zt^2 + … + Zt^k$$<br>其中$Zt^i$表示以标签i为终点的所有路径的得分指数和，此时，我们写出递归公式：</p>
<p>$$Z_{t+1}^1 = (Zt^1 T_{11} + Zt^2 T_{21} + … + Zt^k T_{k1})E_{t+1}(1|X)$$</p>
<p>$$…  $$</p>
<p>$$Z_{t+1}^k = (Zt^1 T_{1k} + Zt^2 T{2k} + … + Zt^k T_{kk})E_{t+1}(k|X)$$</p>
<p>其中T是矩阵t的各个元素取指数形式，即$T_{ij} = exp(t_{ij})$, E是e的指数形式，即$E_{ij} = exp(e_{ij})$, 而$e_{ij}$是指时刻i时RNNs对label j的打分。<br><img src="/2019/12/26/from-softmax-to-crf/logz.png" alt="logz"></p>
<p>上式带有指数形式，我们取对数来简化计算过程。<br>$$<br>log(Z_{t+1}^1) = log((Zt^1 T_{11} + Zt^2 T_{21} + … + Zt^k T_{k1})E_{t+1}(1|X) )  \\<br>=log(exp(log(Zt^1) + t_{11}) + exp(log(Zt^2) + t_{21}) + … + exp(log(Zt^k) + t_{k1}) exp(e_{t+1}(1|X)))  \\<br>= log(exp(log(Zt^1) + t_{11} + e_{t+1}(1|X)) + exp(log(Zt^2) + t_{21} + e_{t+1}(1|X)) + … + exp(log(Zt^k) + t_{k1} + e_{t+1}(1|X)))\\<br>= log(\sum_k(log(Zt^k) + t_{k1} + e_{t+1}(1|X)))<br>$$</p>
<p>上面的过程比较曲折，对有些同学可能不太好理解，我们用一个简单的例子来帮助理解。<br>我们假设我们现在有一个输入$X=[w_0,w_1, w_2]$, 标签集合为$L=[l_1, l_2]$. RNNs对e完成了建模，即：</p>
<table>
<thead>
<tr>
<th>&nbsp;</th>
<th>l1</th>
<th>l2     </th>
</tr>
</thead>
<tbody>
<tr>
<td>w0</td>
<td>$e_{01}$</td>
<td>$e_{02}$ </td>
</tr>
<tr>
<td>w1</td>
<td>$e_{11}$</td>
<td>$e_{12}$ </td>
</tr>
<tr>
<td>w2</td>
<td>$e_{21}$</td>
<td>$e_{22}$ </td>
</tr>
</tbody>
</table>
<p>其中$e_{ij}$代表第i个字是第j个标签的得分。<br>转移矩阵t:</p>
<table>
<thead>
<tr>
<th>&nbsp;</th>
<th>l1</th>
<th>l2 </th>
</tr>
</thead>
<tbody>
<tr>
<td>l1</td>
<td>t11</td>
<td>t12  </td>
</tr>
<tr>
<td>l2</td>
<td>t21</td>
<td>t22  </td>
</tr>
</tbody>
</table>
<p>其中$t_{ij}$代表第i个标签转换为第j个标签的得分。<br>接下来我们按从$w_0$到$w_2$的方向一步一步来进行计算。首先，我们引入两个变量: states, cur，其中:  </p>
<ul>
<li>states代表上一个时刻计算的最终结果，即对应$log(Z_t^i)$</li>
<li><p>cur代表当前时刻各个标签的得分，即对应$e_t^i$</p>
</li>
<li><p><code>$w_0$:</code></p>
</li>
<li>states = None</li>
<li>$cur = [e_{01}, e_{02}]$<br>此时:<br>$$log(Z) = exp(e_{01}) + exp(e_{02})$$</li>
</ul>
<ul>
<li><code>$w_0$ –&gt; $w_1$:</code></li>
<li>states = $[e_{01}, e_{02}]$</li>
<li>cur = $[e_{11}, e{12}]$</li>
</ul>
<ol>
<li>扩展states:<br>$<br>states = $$\begin{pmatrix}<br>e_{01}&amp;e_{01}\\<br>e_{02}&amp;e_{02}\\<br>\end{pmatrix}$$<br>$</li>
</ol>
<ol start="2">
<li>扩展cur:<br>$<br>cur = $$\begin{pmatrix}<br>e_{11}&amp;e_{12}\\<br>e_{11}&amp;e_{12}\\<br>\end{pmatrix}$$<br>$</li>
</ol>
<ol start="3">
<li>将cur, states 与转移矩阵t 求和:</li>
</ol>
<p>$<br>score = $$\begin{pmatrix}<br>e_{11}&amp;e_{12}\\<br>e_{11}&amp;e_{12}\\<br>\end{pmatrix}$$<br>$+$ $$\begin{pmatrix}<br>e_{01}&amp;e_{01}\\<br>e_{02}&amp;e_{02}\\<br>\end{pmatrix}$$<br>$+$ $$\begin{pmatrix}<br>t_{11}&amp;t_{12}\\<br>t_{21}&amp;t_{22}\\<br>\end{pmatrix}$$<br>$=$ $$\begin{pmatrix}<br>e_{01} + e_{11} + t_{11} &amp; e_{01} + e_{12} + t_{12}\\<br>e_{02} + e_{11} + t_{21} &amp; e_{02} + e_{12} + t_{22}\\<br>\end{pmatrix}$$<br>$</p>
<ol start="4">
<li>对score取指数形式然后求和，得到新的states:<br>$$<br>states = [log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21})), log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))]<br>$$<br>其中，states中的每个元素即对应着式中$logZ_{t}^i$, 此时的$log(Z)= \sum_k(exp(log(Z^k)))$:</li>
</ol>
<p>$$<br>log(Z_{0,1}) = log(exp(log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}))) + exp(log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22})))) \\<br>= log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}) + exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))<br>$$</p>
<p>当序列长度为2，标签有两个时，所有可能的标签序列为$(label_1-&gt;label_1, label_2-&gt;label_1, label_1-&gt;label_2, label_2-&gt;label_2)$,而对应的序列得分，即对应上式中的项，即：</p>
<ul>
<li>$ S_1: label_1-&gt;label_1:$<br>$ S_1 = e_{01} + e_{11} + t_{11} $</li>
<li>$S_2: label_2-&gt;label_1:$<br>$ S_2 = e_{02} + e_{11} + t_{21}$</li>
<li>$S_3 = label_1-&gt;label_2:$<br>$ S_3 = e_{01} + e_{12} + t_{12}$</li>
<li>$S_4 = label_2-&gt;label_2:$<br>$ S_4 = e_{02} + e_{12} + t_{22}$</li>
</ul>
<ul>
<li><code>$w_0$ -&gt; $w_1$ -&gt; $w_2$:</code></li>
<li>$states = [log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21})), log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))] $</li>
<li>$cur = [e_{21}, e_{22}]$</li>
</ul>
<p>与上面的做法一样，也分为4步：</p>
<ol>
<li>扩展states:<br>$<br>states = $$\begin{pmatrix}<br>log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}))&amp;log(exp(e_{01} + e_{11} + t_{11}) + exp(e_{02} + e_{11} + t_{21}))\\<br>log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))&amp;log(exp(e_{01} + e_{12} + t_{12}) + exp(e_{02} + e_{12} + t_{22}))\\<br>\end{pmatrix}$$<br>$</li>
</ol>
<ol start="2">
<li>cur：<br>$<br>cur = $$\begin{pmatrix}<br>e_{21} &amp;e_{22}\\<br>e_{21} &amp;e_{22}\\<br>\end{pmatrix}$$<br>$</li>
</ol>
<ol start="3">
<li>将states，cur与转义矩阵t求和：</li>
</ol>
<p>$$<br>scores = \begin{pmatrix}<br>log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) &amp; log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) \\<br>log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22})) &amp; log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))\\<br>\end{pmatrix} \\+<br>\begin{pmatrix}<br>e_{21} &amp;e_{22}\\<br>e_{21} &amp;e_{22}\\<br>\end{pmatrix} \\+<br>\begin{pmatrix}<br>t_{11}&amp; t_{12}\\<br>t_{21}&amp;t_{22}\\<br>\end{pmatrix} \\=<br>\begin{pmatrix}<br>log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{21} + t_{11} &amp;log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{22} + t_{12}\\<br>log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22})) + e_{21} + t_{21} &amp;log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+x_{12}+t_{22})) + e_{22} + t_{22}\\<br>\end{pmatrix}<br>$$</p>
<ol start="4">
<li>对score取指数形式然后求和，得到新的states:</li>
</ol>
<p>$$ states = [\\<br>log(exp(log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{21} + t_{11})+exp(log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22})) + e_{21} + t_{21})), \\<br>log(exp(log(exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21})) + e_{22} + t_{12})+ exp(log(exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+x_{12}+t_{22})) + e_{22} + t_{22})))] \\<br> = [log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{21} + t_{11}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{21}+t_{21})),\\<br>log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{22} + t_{12}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{22}+t_{22}))]<br>$$</p>
<p>现在，我们用states计算一下$(Z_2)$:</p>
<p>$$<br>log(Z_{(0-&gt;1-&gt;2)}) = log(exp(states[0]) + exp(states[1])) \\<br>=log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{21} + t_{11}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{21}+t_{21})) \\+<br>log((exp(e_{01}+e_{11}+t_{11}) + exp(e_{02}+e_{11}+t_{21}))exp(e_{22} + t_{12}) + (exp(e_{01}+e_{12}+t_{12}) + exp(e_{02}+e_{12}+t_{22}))exp(e_{22}+t_{22})) \\<br>= log(exp(e_{01}+e_{11}+t_{11}+e_{21}+t_{11}) + exp(e_{02}+e_{11}+t_{21}+e_{21}+t_{11}) \\ +<br>exp(e_{01}+e_{12}+t_{12} +e_{21}+t_{21}) + exp(e_{02}+e_{12}+t_{22}+e_{21}+t_{21}) \\+<br>exp(e_{01}+e_{11}+t_{11} +e_{22}+t_{12}) + exp(e_{01}+e_{11}+t_{11}+e_{22}+t_{12}) \\+<br>exp(e_{01}+e_{12}+t_{12} +e_{22}+t_{22}) + exp(e_{01}+e_{12}+t_{12}+e_{22}+t_{21}))<br>$$</p>
<p>上式也就是我们要求的最终结果$log(Z)$,其中指数内对应着所有路径的得分。<br>到此，上例中的整个归一化因子的计算过程也就完成了，而CRF中最难的部分也就解决了。</p>
<h1><span id="demo">demo</span><a href="#demo" class="header-anchor"></a></h1><p>搞懂了理论部分，下面写一个demo来验证一下。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mask_label=False, **kwargs)</span>:</span></span><br><span class="line">        self.mask_label = <span class="number">1</span> <span class="keyword">if</span> mask_label <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        super(CRF, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.num_label = input_shape[<span class="number">-1</span>] - self.mask_label</span><br><span class="line">        self.trans = self.add_weight(name=<span class="string">'crf_trans'</span>,</span><br><span class="line">                                     shape=(self.num_label, self.num_label),</span><br><span class="line">                                     trainable=<span class="literal">True</span>,</span><br><span class="line">                                     initializer=<span class="string">'glorot_uniform'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">path_score</span><span class="params">(self, inputs, labels)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param inputs: (batch_size, timesteps, num_label), obtained from rnn(lstm, bilstm. etc.)</span></span><br><span class="line"><span class="string">        :param labels: one-hot, (batch_size, timesteps, num_label) , real target series</span></span><br><span class="line"><span class="string">        :return:  path score</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        point_score = K.sum(K.sum(inputs * labels, <span class="number">2</span>), <span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        label_pre = K.expand_dims(labels[:, :<span class="number">-1</span>], <span class="number">3</span>)</span><br><span class="line">        label_next = K.expand_dims(labels[:, <span class="number">1</span>:], <span class="number">2</span>)</span><br><span class="line">        label_trans = label_pre * label_next</span><br><span class="line">        trans = K.expand_dims(K.expand_dims(self.trans, <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">        trans_score = K.sum(K.sum(label_trans * trans, [<span class="number">2</span>, <span class="number">3</span>]), <span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> point_score + trans_score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log_norm_pre</span><span class="params">(self, inputs, states)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        expand previous states and inputs, sum with trans</span></span><br><span class="line"><span class="string">        :param inputs: (batch_size, num_label), current word emission scores</span></span><br><span class="line"><span class="string">        :param states: (batch_size, num_label), all paths  score of previous word</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        states = K.expand_dims(states[<span class="number">0</span>], <span class="number">2</span>)</span><br><span class="line">        inputs = K.expand_dims(inputs, <span class="number">1</span>)</span><br><span class="line">        trans = K.expand_dims(self.trans, <span class="number">0</span>)</span><br><span class="line">        scores = states + trans + inputs</span><br><span class="line">        output = K.logsumexp(scores, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, [output]</span><br><span class="line">        <span class="comment"># states = K.expand_dims(states[0], 2)  # (batch_size, output_dim, 1)</span></span><br><span class="line">        <span class="comment"># trans = K.expand_dims(self.trans, 0)  # (1, output_dim, output_dim)</span></span><br><span class="line">        <span class="comment"># output = K.logsumexp(states + trans, 1)  # (batch_size, output_dim)</span></span><br><span class="line">        <span class="comment"># return output + inputs, [output + inputs]</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, y_true, y_pre)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param inputs: (batch_size, timesteps, num_label)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># mask = 1 - y_true[:, 1: -1] if self.mask_label else None</span></span><br><span class="line">        <span class="comment"># # y_true, y_pred = y_true[:, :, :self.num_label], y_pre[:, :, :self.num_label]</span></span><br><span class="line">        <span class="comment"># real_path_score = self.path_score(y_pre, y_true)</span></span><br><span class="line">        <span class="comment"># init_states = [y_pre[:, 0]]</span></span><br><span class="line">        <span class="comment"># log_norm, _ = K.rnn(self.log_norm_pre, initial_states=init_states, inputs=y_pre[:, 1:], mask=mask)  # log(Z)</span></span><br><span class="line">        <span class="comment"># log_norm_score = K.logsumexp(log_norm, 1, keepdims=True)</span></span><br><span class="line">        <span class="comment"># return log_norm_score - real_path_score</span></span><br><span class="line">        mask = <span class="number">1</span> - y_true[:, <span class="number">1</span>:, <span class="number">-1</span>] <span class="keyword">if</span> self.mask_label <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        y_true, y_pre = y_true[:, :, :self.num_label], y_pre[:, :, :self.num_label]</span><br><span class="line">        init_states = [y_pre[:, <span class="number">0</span>]]  <span class="comment"># 初始状态</span></span><br><span class="line">        log_norm, _, _ = K.rnn(self.log_norm_pre, y_pre[:, <span class="number">1</span>:], init_states, mask=mask)  <span class="comment"># 计算Z向量（对数）</span></span><br><span class="line">        log_norm = K.logsumexp(log_norm, <span class="number">1</span>, keepdims=<span class="literal">True</span>)  <span class="comment"># 计算Z（对数）</span></span><br><span class="line">        path_score = self.path_score(y_pre, y_true)  <span class="comment"># 计算分子（对数）</span></span><br><span class="line">        <span class="keyword">return</span> log_norm - path_score  <span class="comment"># 即log(分子/分母)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span>  <span class="comment"># crf 只是loss，不改变inputs</span></span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(self, y_true, y_pred)</span>:</span></span><br><span class="line">        mask = <span class="number">1</span> - y_true[:, :, <span class="number">-1</span>] <span class="keyword">if</span> self.mask_label <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        y_true, y_pred = y_true[:, :, :self.num_label], y_pred[:, :, :self.num_label]</span><br><span class="line">        isequal = K.equal(K.argmax(y_true, <span class="number">2</span>), K.argmax(y_pred, <span class="number">2</span>))</span><br><span class="line">        isequal = K.cast(isequal, <span class="string">'float32'</span>)</span><br><span class="line">        <span class="keyword">if</span> mask == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> K.mean(isequal)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> K.sum(isequal * mask) / K.sum(mask)</span><br></pre></td></tr></table></figure></p>
<p>结果：<br><img src="/2019/12/26/from-softmax-to-crf/result.png" alt="result"></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>初雪下的红果果</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>ccf问答匹配比赛（下）：如何只用“bert”夺冠</title>
    <url>/2021/01/20/ccf-qa-2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#qian-yan">前言</a><ul>
<li><a href="#sai-ti">赛题</a></li>
<li><a href="#qa-pair">QA pair</a></li>
<li><a href="#qa-point">QA Point</a></li>
<li><a href="#pattern-exploiting-training-pet">Pattern-Exploiting Training (PET)</a></li>
<li><a href="#concat">Concat</a></li>
<li><a href="#focal-loss">focal loss</a></li>
<li><a href="#dui-kang-xun-lian">对抗训练</a></li>
<li><a href="#post-training">post training</a></li>
</ul>
</li>
<li><a href="#post-training">Post Training</a><ul>
<li><a href="#mlm">MLM</a></li>
<li><a href="#nsp">nsp</a></li>
<li><a href="#model-adaptive">model-adaptive</a></li>
<li><a href="#geng-xin-ci-shu">更新次数</a></li>
<li><a href="#zui-zhong-jie-guo">最终结果</a></li>
</ul>
</li>
<li><a href="#rong-ru-zhi-shi">融入知识</a></li>
<li><a href="#dui-bi-xue-xi">对比学习</a><ul>
<li><a href="#fei-jian-du-dui-bi-xue-xi">非监督对比学习</a></li>
<li><a href="#jian-du-dui-bi-xue-xi">监督对比学习</a></li>
<li><a href="#shi-yan-jie-guo">实验结果</a></li>
</ul>
</li>
<li><a href="#shu-ju-zeng-qiang">数据增强</a><ul>
<li><a href="#eda">EDA</a></li>
<li><a href="#wei-biao-qian">伪标签</a></li>
<li><a href="#shi-yan-jie-guo-1">实验结果</a></li>
</ul>
</li>
<li><a href="#zi-zheng-liu">自蒸馏</a></li>
<li><a href="#shuffle-jie-ma">shuffle 解码</a></li>
<li><a href="#mo-xing-rong-he">模型融合</a></li>
<li><a href="#shi-yan-zong-jie">实验总结</a></li>
<li><a href="#bi-sai-jie-guo">比赛结果</a></li>
<li><a href="#dai-ma">代码</a></li>
<li><a href="#zui-hou">最后</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>ccf问答匹配比赛也结束了一段时间了，这篇算是一个下篇吧，总结一下后期优（夺）化（冠）的心路历程。标题中的“bert”指的是bert-base系列模型，包括bert/RoBERTa/NEZHA/MacBERT/ERNIE等，而取这个有点“标题党”的标题的主要原因，也是对答辩看到有些团队使用的bert+xgb这种“大力出奇迹”做法吐个槽。</p>
<h1><span id="qian-yan">前言</span><a href="#qian-yan" class="header-anchor"></a></h1><p>在<a href="https://xv44586.github.io/2020/11/08/ccf-qa/">上一篇</a>中,笔者对比赛做了简单说明，提出了四种baseline（QA Pair/QA Point/PET/Concat),并做了部分尝试（focal loss/对抗训练/梯度惩罚/kfold/post training),没看过的同学可以先看上篇，这里只简单再介绍一下：</p>
<h2><span id="sai-ti">赛题</span><a href="#sai-ti" class="header-anchor"></a></h2><p>本次赛题的任务是：给定IM交流片段，片段包含一个客户问题以及随后的经纪人若干IM消息，从这些随后的经纪人消息中找出一个是对客户问题的回答。</p>
<ul>
<li>数据示例<br>$$<br>\begin{array}{c|c|c|c|c}<br>\hline<br>\text{对话id} &amp;  \text{客户问题} &amp; \text{经纪人回复id} &amp; \text{经纪人回复内容} &amp; \text{回复标签} \\<br>\hline<br>1 &amp; \text{您好，请问这个户型有什么优缺点} &amp; 1 &amp; \text{你是想看看这套房子是吗} &amp; 0 \\<br>\hline<br>&amp; &amp; \text{2} &amp; \text{在的} &amp;\text{0} \\<br>\hline<br>&amp; &amp; \text{3} &amp; \text{此房房型方正 得房率高 多层不带电梯4/6楼<br>} &amp;\text{1} \\<br>\hline<br>\end{array}<br>$$</li>
<li>评测标准</li>
</ul>
<p>f1：2 * (精度 * 召回) / (精度 + 召回)</p>
<h2><span id="qa-pair">QA pair</span><a href="#qa-pair" class="header-anchor"></a></h2><p>由于回答列表是不连续的，所以不考虑问答之间的顺序关系，将其拆分为query-answer pair，然后进行判断。<br><img src="/2021/01/20/ccf-qa-2/pair.png" alt="pair"></p>
<h2><span id="qa-point">QA Point</span><a href="#qa-point" class="header-anchor"></a></h2><p>考虑对话连贯性、相关性，将所有回答顺序拼接后再与问题拼接，组成query-answer list，模型对一个问题的所有答案进行预测。此外，我们还给模型增加了“大局观”，即新增一个任务来预测全局所有回答中是否存在label为 1 的回答。<br><img src="/2021/01/20/ccf-qa-2/point.png" alt="point"></p>
<h2><span id="pattern-exploiting-training-pet">Pattern-Exploiting Training (PET)</span><a href="#pattern-exploiting-training-pet" class="header-anchor"></a></h2><p>此方案通过增加一个pattern，将任务转换为MLM任务，然后通过pattern的得分来判断对应的类别。<br>如本次比赛可以添加一个前缀pattern：“间接回答问题”/ “直接回答问题”，分别对应label 0 / 1，pattern的得分只需看第一个位置中“间”/“直”两个token的概率谁高即可。对于unlabel data，可以不增加pattern 进行mlm任务，这也在一定程度增加了模型的泛化能力。此外，通过多个不同pattern进行融合也能进一步提高其性能。<br><img src="/2021/01/20/ccf-qa-2/pet.png" alt="pet"></p>
<h2><span id="concat">Concat</span><a href="#concat" class="header-anchor"></a></h2><p>由于bert 中不同的transformer 层提取到的语义粒度不同，而不同粒度的信息对分类来说起到的作用也可能不同，所以可以将所有粒度的语义信息拼接后作为特征进行分类。<br><img src="/2021/01/20/ccf-qa-2/concat.png" alt="concat"></p>
<h2><span id="focal-loss">focal loss</span><a href="#focal-loss" class="header-anchor"></a></h2><p>由于针对性回答与非针对性回答在数量上有不小差距，大约3:1，所以也想到尝试在loss上进行调节。<br>最终结果是没有多少提升，猜测样本不均衡的问题影响是非常小的，所以将Binary-Crossentropy训练后的模型在train data上进行了predict，并借鉴之前<a href="https://xv44586.github.io/2020/10/14/focal-loss/">focal loss</a>中的方式分析了一下，画出对应的难易样本分布。根据图形上的分布结果，也证实了之前的猜测。<br><img src="/2021/01/20/ccf-qa-2/focalloss.png" alt="focalloss"></p>
<h2><span id="dui-kang-xun-lian">对抗训练</span><a href="#dui-kang-xun-lian" class="header-anchor"></a></h2><p>对抗训练主要尝试了<a href="https://kexue.fm/archives/7234" target="_blank" rel="noopener">FGM 方法对Embedding进行扰动</a>，线下对比提升大约一个点上下。<br>线下测试结果：</p>
<p>$$<br>\begin{array}{c|c}<br>\hline<br>\text{without adt} &amp; \text{with adt} \\<br>\hline<br>\text{0.831} &amp; \text{0.838} \\<br>\end{array}<br>$$</p>
<h2><span id="post-training">post training</span><a href="#post-training" class="header-anchor"></a></h2><p>上一篇中，提到post training 做的效果不好，然而pet 的效果又很好，两者比较矛盾，所以我也重新阅读了几篇关于优化bert 与post training 相关的论文，重新思考了一下，这篇就从重新做post training开始。</p>
<h1><span id="post-training">Post Training</span><a href="#post-training" class="header-anchor"></a></h1><p>post training一般包括两部分：Domain-Adaptive training 和 Task-Adaptive training，通过在同领域与任务数据上继续预训练，可以让模型更适应任务，有利于提高模型在下游的性能。而bert 在训练时主要有两个任务：mlm 与nsp ，接下来针对每个任务进行讨论。</p>
<h2><span id="mlm">MLM</span><a href="#mlm" class="header-anchor"></a></h2><p>在post training 阶段尝试进一步优化的只找到刘知远老师的<a href="https://arxiv.org/abs/2004.09733" target="_blank" rel="noopener">Train No Evil: Selective Masking for Task-Guided Pre-Training</a>，论文里的思路是通过建立一个二分类模型，来有针对性的选择token 来进行mask，不过由于这个方法比较麻烦，需要三个中间模型，所以没有尝试，不过这个论文给出了一个结论：在继续预训练的过程中，优化mask 策略，是可以进一步提高下游性能的。<br>让我们回归一下bert 的mask 策略即后续的改进：<br>$$<br>\begin{array}{c|c|}<br>\hline<br>\text{model} &amp; \text{mask sstrategy} \\<br>\hline<br>\text{bert} &amp; \text{random mask} \\<br>\hline<br>\text{RoBERTa} &amp; \text{dynamic mask} \\<br>\hline<br>\text{RoBERTa-wwm-ext} &amp; \text{whole word mask} \\<br>\hline<br>\text{ERNIE} &amp; \text{entity/phrase mask} \\<br>\hline<br>\text{SpanBERT} &amp; \text{n-gram mask}\\<br>\hline<br>\end{array}<br>$$</p>
<p>这里笔者思考后认为，不同的mask 策略本质区别是对更多的“固定搭配”进行同时mask，从而降低模型对局部、浅层信息的过拟合，增加任务的难度，提高模型的泛化能力。<br>所谓“固定搭配”，不仅仅包含词，或者说是更广义的“词”。字的固定搭配可以构成词，进一步固定搭配又可以形成短语。比如考虑“好好学习，天天向上”，“08北京奥运会”，如果只mask 其中一部分，是比较“容易”通过剩余的部分来还原的。<br>既然“固定搭配”是更广义的词，这里我们就可以来挖掘这些“固定搭配”了。最简单的方式就是新词/短语挖掘，而新词/短语挖掘最常用的方法是计算左右熵和紧密度，不过这种方式计算量较大，这次比赛笔者舍弃了这种方式，采用借鉴苏神的博客<a href="https://kexue.fm/archives/5476" target="_blank" rel="noopener">最小熵原理（二）：“当机立断”之词库构建</a>中的思路，用PMI表征紧密度，用相邻两个字之间的紧密度判断两者是否存在“固定搭配”,最终未被切分的为一个整体。最后将挖掘出的新词通过jieba 过滤掉已在词库内的，并只保留长度2~5的新词，添加到jieba的词库内。这里选择用jieba 做分词工具的原因是因为笔者用的是NEZHA，而NEZHA在训练时使用的就是jieba 处理的数据，这里与他保持一致，而长度选择上，主要借鉴spanBert中的结论。<br>最后挖掘了2736个新词，而如果是实际工作中，则可以进一步将积累的词也加入。<br><img src="/2021/01/20/ccf-qa-2/new_words.png" alt="new_words"><br>以上的方式中全程没有人为参与，所以新词的质量是无法保证的，即存在词的边界不准确。而此时的全词mask 退化为n-gram mask，依然是一种有效的提升方案。</p>
<h2><span id="nsp">nsp</span><a href="#nsp" class="header-anchor"></a></h2><p>原始bert 在训练时，句子级别的任务为nsp，而RoBERTa 中给出的结论是句子级别的任务没什么用，所以取消了句子级别的任务；而albert 中则将句子级别的任务切换为sop，而SpanBERT中则切换为sbo。这里笔者认为下游任务是句子级别的分类任务，所以句子级别的任务是有用的，不过由于nsp 会引入大量噪音，所以这里选择sop/aop：在qa pair格式的样本下互换qa(sop)，在q a-list格式的样本下，保持query 在最前面，只shuffle a-list(aop)。</p>
<p>$$<br>\begin{array}{c|c|c}<br>\hline<br> &amp; \text{without sop/aop} &amp; \text{with sop/aop} \\<br>\hline<br>\text{qa pair} &amp; \text{0.784} &amp; \text{0.79} \\<br>\hline<br>\text{q a-list} &amp; \text{0.799} &amp; \text{0.802} \\<br>\hline<br>\end{array}<br>$$</p>
<h2><span id="model-adaptive">model-adaptive</span><a href="#model-adaptive" class="header-anchor"></a></h2><p>由于样本的组织方式有qa pair 和 q a-list两种方式，而task 相关的数据是相对较小的，所以这里笔者认为两个阶段的样本组织方式相同的情况下，性能会更好，即：用qa pair格式post training后的模型，来微调qa pair格式的baseline，q a-list格式post training后的模型微调q a-list格式的baseline。</p>
<h2><span id="geng-xin-ci-shu">更新次数</span><a href="#geng-xin-ci-shu" class="header-anchor"></a></h2><p>这里参考邱锡鹏老师的<a href="http://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>，实验时每10 个epochs保存一次模型，最后通过在下游任务上的表现，得出与论文中基本一致的结论：更新10K steps左右模型在下游的表现是最好的。<br><img src="/2021/01/20/ccf-qa-2/update.png" alt="update"></p>
<h2><span id="zui-zhong-jie-guo">最终结果</span><a href="#zui-zhong-jie-guo" class="header-anchor"></a></h2><p>$$<br>\begin{array}{c|c|c}<br>\hline<br> \text{post-train/fine-tuning}&amp; \text{pair} &amp; \text{point} \\<br>\hline<br>\text{pair} &amp; \text{0.79} &amp; \text{0.794} \\<br>\hline<br>\text{point} &amp; \text{0.786} &amp; \text{0.802} \\<br>\hline<br>\end{array}<br>$$</p>
<p>此时我们认为已经将bert的能力最大化了，于是这里也尝试了在bert 后面接一些复杂的分类层（cnn/rnn/dgcnn/..),发现都无法进一步提高，所以也证实了之前的判断。</p>
<h1><span id="rong-ru-zhi-shi">融入知识</span><a href="#rong-ru-zhi-shi" class="header-anchor"></a></h1><p>既然从“内部”已经无法进一步提高bert的能力，所以此时尝试融入外部知识来增强。而融合的方式主要尝试了两种：  </p>
<ul>
<li><p>最底层注入<br>在Embedding 层融入外部的embedding。优点：更多的交互<br><img src="/2021/01/20/ccf-qa-2/inside.png" alt> </p>
</li>
<li><p>最顶层注入<br>在transformer output 层融入外部Embedding。优点：更灵活，不局限外部知识的形式（可以是Embedding，也可以说是其他特征，如手工特征）。<br><img src="/2021/01/20/ccf-qa-2/outside.png" alt></p>
</li>
</ul>
<p>在知识选择上，首先想到的是Graph EMbedding，参考<a href="http://arxiv.org/abs/2004.05707" target="_blank" rel="noopener">VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification</a>,我们重跑了一下论文的代码，发现无法重现其中的结论，而我对Graph Embedding也不熟，所以放弃了这个方案。<br>然后尝试简单的embedding，即用gensim 在task data上训练了一版词向量(dims=100),作为外部知识来实验。<br>线下测试结果：<br>$$<br>\begin{array}{c|c}<br>\hline<br> \text{model} &amp; \text{score} \\<br>\hline<br>\text{bert} &amp; \text{0.831} \\<br>\hline<br>\text{external-embedding bottom} &amp; \text{0.82} \\<br>\hline<br>\text{external-embedding top} &amp; \text{0.83} \\<br>\hline<br>\end{array}<br>$$</p>
<p>可以看到，两种方式都是无法进一步提高的，主要原因可能是：1.词向量的质量较差；2.词向量也是bert的“内部”知识；3.融入的方式或者调参没做好。</p>
<h1><span id="dui-bi-xue-xi">对比学习</span><a href="#dui-bi-xue-xi" class="header-anchor"></a></h1><p>在模型上，还能通过增加新的任务来尝试提高性能。而今年比较热的一个思路就是对比学习，所以这里尝试通过增加一个对比学习任务来提高性能。<br>对比学习的主要思路是拉近到正样本之间的距离，拉远到负样本之间的距离。<br><img src="/2021/01/20/ccf-qa-2/cl.png" alt><br>对比学习主要又分为两种：监督对比学习和分监督对比学习。监督对比学习中，将相同label的样本看做是正例，其他的为负例；而非监督对比学习中，则通过对每个样本构造一对view，view之间互为正例，其他的为负例。</p>
<h2><span id="fei-jian-du-dui-bi-xue-xi">非监督对比学习</span><a href="#fei-jian-du-dui-bi-xue-xi" class="header-anchor"></a></h2><p>非监督对比学习中，通过互换QA位置，同时随机mask 15% 的token，来构造一对view。</p>
<ul>
<li>对应的loss：</li>
</ul>
<p><img src="/2021/01/20/ccf-qa-2/sscl.png" alt></p>
<ul>
<li>对应的模型：</li>
</ul>
<p><img src="/2021/01/20/ccf-qa-2/sscl-model.png" alt></p>
<h2><span id="jian-du-dui-bi-xue-xi">监督对比学习</span><a href="#jian-du-dui-bi-xue-xi" class="header-anchor"></a></h2><p>这里主要follow <a href="http://arxiv.org/abs/2011.01403" target="_blank" rel="noopener">Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning</a>,修改对应loss.</p>
<ul>
<li>loss</li>
</ul>
<p><img src="/2021/01/20/ccf-qa-2/scl.png" alt></p>
<ul>
<li>model</li>
</ul>
<p><img src="/2021/01/20/ccf-qa-2/sc-model.png" alt></p>
<h2><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h2><ul>
<li>线下结果：</li>
</ul>
<p>$$<br>\begin{array}{c|c}<br>\hline<br> \text{model} &amp; \text{score} \\<br>\hline<br>\text{bert} &amp; \text{0.831} \\<br>\hline<br>\text{self-supervised contrastive learning} &amp; \text{0.80} \\<br>\hline<br>\text{supervised contrastive learning} &amp; \text{0.824} \\<br>\hline<br>\end{array}<br>$$</p>
<ul>
<li><p>非监督对比学习结果可视化<br><img src="/2021/01/20/ccf-qa-2/ssc-vis.png" alt></p>
</li>
<li><p>监督对比学习结果可视化<br><img src="/2021/01/20/ccf-qa-2/sc-vis.png" alt></p>
</li>
</ul>
<p>可以看到，两种方式都没有带来提升，而可视化图中可以看到，非监督对比学习的效果并不好，存在大量重叠但颜色不同的点，说明对比学习任务的结果不好，这里的原因猜测主要有两点：1.模型的设计与调参时有问题，batch size（32）太小，没有BN 层等，都有可能是性能不好的原因；2.构造view 的方式过于简单粗暴，由于样本长度大多较短，随机mask 后即有可能引入错误的label 信息，又可能引起view 间语义的gap过大，无法互为正例。<br>监督学习效果图中，不同label的数据被分到了不同的簇中，说明对比学习的还是相当不错，不过由于此次比赛中的label 代表的是“是否是针对问题的回答”，label 相同但内涵不同，所以强行将相同label的样本聚合，并不能带来提升。</p>
<h1><span id="shu-ju-zeng-qiang">数据增强</span><a href="#shu-ju-zeng-qiang" class="header-anchor"></a></h1><p>数据增强主要尝试了两种方式：EDA 和伪标签。</p>
<h2><span id="eda">EDA</span><a href="#eda" class="header-anchor"></a></h2><p>EDA主要包括四种方式：随机替换、随机删除、随机重复和随机互换。<br>由于词向量质量较差，所以操作时选择从当前句子中随机选取一个词作为“同义词”进行操作。<br>操作比例为10%，每个样本构造四个样本。<br>用训练过的模型对数据进行过滤，保留置信度高(&gt;0.7)的样本。</p>
<h2><span id="wei-biao-qian">伪标签</span><a href="#wei-biao-qian" class="header-anchor"></a></h2><p>用训练过的模型在test data 上进行预测，对预测结果按0.5 为阈值计算置信度并进行排序，保留前30%的样本加入训练集。 这里没有单纯按置信度过滤样本，是因为模型预测结果大多数大于0.95或小于0.05，而过多的测试数据进入训练集，会导致模型最终的结果是在拟合训练集中的label，而无法带来提高（充分学习后的模型在训练数据上的预测结果自然是训练时的label）。</p>
<h2><span id="shi-yan-jie-guo">实验结果</span><a href="#shi-yan-jie-guo" class="header-anchor"></a></h2><ul>
<li>线上结果</li>
</ul>
<p>$$<br>\begin{array}{c|c}<br>\hline<br> \text{without DA} &amp; \text{with DA} \\<br>\hline<br>\text{0.802} &amp; \text{0.806} \\<br>\hline<br>\end{array}<br>$$</p>
<h1><span id="zi-zheng-liu">自蒸馏</span><a href="#zi-zheng-liu" class="header-anchor"></a></h1><p>借助知识蒸馏，我们尝试了自蒸馏方案：即Teacher 与 Student 为同一个模型，Teacher模型先学习一遍后，对训练样本打上soft labels，Student 同时学习true labels 与 soft labels. </p>
<ul>
<li><p>soft labels：<br>$$<br>q_i = \frac{exp(\frac{z_i}{T})}{\sum_j exp(\frac{z_j}{T})}<br>$$</p>
</li>
<li><p>线下测试结果：<br>$$<br>\begin{array}{c|c}<br>\hline<br>\text{without KD} &amp; \text{with KD} \\<br>\hline<br>\text{0.831} &amp; \text{0.84} \\<br>\hline<br>\end{array}<br>$$</p>
</li>
</ul>
<h1><span id="shuffle-jie-ma">shuffle 解码</span><a href="#shuffle-jie-ma" class="header-anchor"></a></h1><p>对于q a-list 的模型，可以在预测时，对answer list 进行全排列，然后将结果投票，一来可以将answer label之间的影响降低，二来可以在非常小的成本下融合，也算是一种trick。不过此次比赛的数据对顺序比较敏感，shuffle后大多数情况下会降低模型的性能，所以最终融合后结果没提升反而降低了。</p>
<h1><span id="mo-xing-rong-he">模型融合</span><a href="#mo-xing-rong-he" class="header-anchor"></a></h1><p>为了提高模型的稳定性与泛化能力，我们进行了模型融合。融合时，我们期望模型间能“和而不同”：每个单模型的性能之间差异小（都要接近最优单模型），且模型之间差异尽量大(架构或者优化方案上差异尽量大）。根据以上策略，对QA Pair 与 QA Point两种模型进行融合。</p>
<h1><span id="shi-yan-zong-jie">实验总结</span><a href="#shi-yan-zong-jie" class="header-anchor"></a></h1><ul>
<li><p>能work的方案<br>$$<br>\begin{array}{c|c}<br>\hline<br>\text{task-adaptive training} &amp; \text{+1.5%~3%} \\<br>\hline<br>\text{加入新词} &amp; \text{+0.5%~1%} \\<br>\hline<br>\text{加入sop/aop} &amp; \text{+0.1%~0.3%} \\<br>\hline<br>\text{model-adaptive} &amp; \text{+0.5%~0.7%} \\<br>\hline<br>\text{对抗训练} &amp; \text{+0.5%~0.9%} \\<br>\hline<br>\text{EDA} &amp; \text{+0.3%~0.5%} \\<br>\hline<br>\text{模型融合} &amp; \text{+0.5%~0.7%} \\<br>\hline<br>\end{array}<br>$$</p>
</li>
<li><p>不能work的方案<br>$$<br>\begin{array}{c|c}<br>\hline<br>\text{external-embedding bottom} &amp; \text{-0.2%~0%} \\<br>\hline<br>\text{external-embedding top} &amp; \text{-0.1%~0%} \\<br>\hline<br>\text{self-supervised contrastive learning} &amp; \text{-0.4%~-0.2%} \\<br>\hline<br>\text{supervised contrastive learning} &amp; \text{-0.1%~0%} \\<br>\hline<br>\text{focal loss} &amp; \text{0%} \\<br>\hline<br>\text{shuffle trick} &amp; \text{-0.15%~0} \\<br>\end{array}<br>$$</p>
</li>
<li><p>线下有效但未提交<br>$$<br>\begin{array}{c|c}<br>\hline<br>\text{自蒸馏} &amp; \text{+0.5%~1%} \\<br>\hline<br>\text{伪标签} &amp; \text{+0.1%~0.3%} \\<br>\hline<br>\end{array}<br>$$</p>
</li>
</ul>
<p>对于PET ，在post training后的效果并不是很好，不过由于没有时间了，所以没有继续优化。这里提一下可以优化的点：1.可以增加解码空间；2.增加多个pattern 进行融合的方式尝试优化。笔者本人是比较喜欢PET 这个思路的，统一了两个阶段，所以可做的事还有很多。</p>
<h1><span id="bi-sai-jie-guo">比赛结果</span><a href="#bi-sai-jie-guo" class="header-anchor"></a></h1><p>比赛最终的线上成绩在A/B 榜均是第一，答辩阶段也得到了第一。</p>
<ul>
<li>A榜得分：<br><img src="/2021/01/20/ccf-qa-2/a-result.png" alt></li>
<li>B榜得分：<br><img src="/2021/01/20/ccf-qa-2/b-result.png" alt></li>
<li>答辩得分：<br><img src="/2021/01/20/ccf-qa-2/last-result.jpg" alt></li>
</ul>
<h1><span id="dai-ma">代码</span><a href="#dai-ma" class="header-anchor"></a></h1><p>比赛相关思路的代码开源在github上：<br><a href="https://github.com/xv44586/ccf_2020_qa_match" target="_blank" rel="noopener">ccf_2020_qa_match</a><br>欢迎大家尝试使用，有问题或者想法可以提issue，一起讨论。</p>
<h1><span id="zui-hou">最后</span><a href="#zui-hou" class="header-anchor"></a></h1><p>本文主要总结了此次ccf 问答匹配中的实验思路，而其中提出的四种baseline ，可以横向推广至所有的文本分类相关的任务中，而优化相关的方案，则可以应用在所有bert-base 模型上。 从最初打算“白嫖”一份数据，到最终拿到第一，算起来这应该是笔者第一次参加NLP的比赛，所以很幸运也很惊喜。<br>Enjoy！</p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>答辩头图</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>QA</tag>
        <tag>CCF</tag>
        <tag>Competition</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM Inference串讲</title>
    <url>/2023/03/10/llm-inf/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#generation">generation</a><ul>
<li><a href="#greedy-search">Greedy Search</a></li>
<li><a href="#beam-search">Beam Search</a></li>
<li><a href="#sampling">Sampling</a><ul>
<li><a href="#topk">topK</a></li>
<li><a href="#topp">topP</a></li>
<li><a href="#beam-search-sampling">beam-search sampling</a></li>
</ul>
</li>
<li><a href="#generate-parameters">generate parameters</a><ul>
<li><a href="#temperature">Temperature</a></li>
<li><a href="#penalty">* penalty</a></li>
<li><a href="#qiang-zhi-jin-zhi-te-ding-token-de-chu-xian">强制/禁止 特定token 的出现</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#gpu">GPU</a><ul>
<li><a href="#performance">performance</a></li>
</ul>
</li>
<li><a href="#gpt-generation">GPT generation</a><ul>
<li><a href="#ops-vs-bytes">ops vs bytes</a></li>
<li><a href="#attention">attention</a></li>
<li><a href="#cache">cache</a></li>
<li><a href="#ding-liang-ping-gu-you-hua-fang-an">定量评估优化方案</a></li>
</ul>
</li>
<li><a href="#jie-lun">结论</a></li>
<li><a href="#ref">ref</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div><br>本文主要概述一下当前LLM 是如何生成文本及为什么对应的资源（cost/latency)与prompt 和completion 都有关系。</p>
<h1><span id="generation">generation</span><a href="#generation" class="header-anchor"></a></h1><p>上一篇我们讲了为了解决自然语言问题，我们引入了统计语言模型：$S$ 表示一连串特定顺序排列的词$w_1$, $w_2$, …, $w_n$,其中$n$ 是序列的长度，则$S$ 出现的概率$P(S)=P(w_1,w_2,…w_n)$. 通过序列的概率，来判断对应句子是否合理。而这个概率$P(S)$ 很难估算，所以我们将其转化一下。首先，利用条件概率公式将其展开:<br>$$<br>P(S)=P(w_1​,w_2​,…,w_n​)=P(w_1​)∗P(w_2​∣w_1​)∗P(w_3​∣w_1​,w_2​)∗…∗P(w_n​∣w_1​,w_2​,…,w_{n−1}​)<br>$$<br>即：<br>$$<br>P(w_1^n​)=\prod​P(w_i​∣w_1^{i−1}​)<br>$$<br>然后用深度神经网络，对$P(w_i|w_1^{i-1})$ 进行建模，即：<br>$$<br>P(w_i|w_1^{i-1}) = g(w_1^{i-1})<br>$$</p>
<p>其中$g$ 是深度神经网络，如MLP/RNN/Transformer 等。</p>
<p>文本生成的过程，即我们在已有的深度神经网络$g$ 下，生成（采样）合理句子（序列）的过程。其中$g$ 能生成给定context  $C$  时下一个时刻对应的词表分布, 其中$C$ 是由$w_1$, $w_2$, …, $w_m$ 组成的序列。<br>目前我们训练$g$ 通常采用Teacher-Forcing 的方式：训练阶段，每个时刻$t$ 之前的label $y_{&lt;t}$都是已知的，就像是老师讲课一样，将每一步的正确解题思路都告诉你，你只需要跟着老师的思路一步一步推导就能得到正确答案；然而在inference 阶段，我们是没法知道当前时刻之前的真实label 是什么的，所以需要将所有可能的序列的概率都求解出来，最后在所有可能性中选择概率最大的。如同考试时没了参考步骤，只能尝试多个思路，最后保留我们觉得”正确“的。</p>
<p><img src="/2023/03/10/llm-inf/Pasted image 20230307215330.png" alt><br>求解过程是一个求解最优路径问题，然而这个计算量太大了。假设我们的词表大小是$V$, 生成序列的长度为$k$, 每一个时刻都需要对所有状态（整个词表）进行计算，即使我们的词表只有五万个词，要生成五个token 也需要计算25万次，这个计算量实在是太大了。所以我们需要一些更高效的生成策略。</p>
<blockquote>
<p>概率值$p$ 是一个范围在$0\sim 1$ ，多次连乘后会很快接近0，所以通常我们会将$max\prod p_i$ 转换为求$max \sum log(p_i)$ </p>
</blockquote>
<h2><span id="greedy-search">Greedy Search</span><a href="#greedy-search" class="header-anchor"></a></h2><p>Greedy search 的思路是：每次都选择概率最高的词作为最终采样结果，即: $w_t = argmax_wP(w|w_1^{t-1})$<br><img src="/2023/03/10/llm-inf/Pasted image 20230307223501.png" alt><br>该方法是缺点也很明显：局部最优的最终结果很可能不是全局最优，由于每次都是选局部最优，这也扼杀了模型找到全局最优的可能性。如上图中Greedy search 的结果是(The, nice, woman),而全局最优是(The, dog, has) .除此之外，模型的生成结果也不够”丰富“，甚至会出现不停重复之前的内容。</p>
<h2><span id="beam-search">Beam Search</span><a href="#beam-search" class="header-anchor"></a></h2><p>”多一个选择，多一次机会“。为了缓解Greedy search 的问题，我们在每次选择时，不再只保留最高概率的一个，而是所有候选中保留概率最高的N 个(num_beams/beam_width) path。<br><img src="/2023/03/10/llm-inf/Pasted image 20230308095954.png" alt><br>上图是num_beams=2 的示例：</p>
<ol>
<li>第一步时，在(The, dog)/(The, nice)/(The car)  三个路径中选择概率最高的两条path:(The, nice), (The, dog)</li>
<li>第二步时，在(The, nice, wowan)/(The, nice, house)/(The, nice, guy)/(The, dog, and)/(The,dog,runs)/(The,dog,has) 中选择路径概率（单步概率连乘）最高的两条path:(The, dog, has), (The, nice, woman)</li>
<li>依此类推，直到满足终止采样.</li>
</ol>
<p>此前Greedy Search 未找到的最优路径，此次通过Beam Search 找到了，但是依然无法保证每次都能找到全局最优。PS: num_beams =1 时，与Greedy Search 过程相同。</p>
<h2><span id="sampling">Sampling</span><a href="#sampling" class="header-anchor"></a></h2><p>以上两种方法都是确定性解码（deterministic），缺点就是不够丰富，不够”surprise“；为了让生成的内容更加的丰富多样有惊喜，我们可以采用另一种策略：采样（Sampling).</p>
<p>采样的基本思路是在概率分布上进行随机采样，选择一个作为下一个词$w_t$:<br>$$<br>w_t \sim P(w|w_1^{t-1})<br>$$<br>然而由于模型$g$ 距离真实的$P(w|w_1{t-1})$ 还是会有差距，直接按照$g$ 生成的”概率分布“采样风险太大，很容易走偏；一个折中的办法是控制候选集，在“低风险”范围内采样。<br>候选集的构造方式也有多个，常用的有topK/topP/beam-search.</p>
<h3><span id="topk">topK</span><a href="#topk" class="header-anchor"></a></h3><p>每次选择概率最大的K 个作为候选集，然后重新归一化，获取新的分布后进行采样。</p>
<ol>
<li>对词表进行排序，选择概率最高的K 个；</li>
<li>对候选集中的K 个候选的概率值进行归一化，构造K 个候选对应的分布$\widetilde{P}$</li>
<li>从分布$\widetilde{P}$ 中采样一个词作为$w_t$</li>
</ol>
<h3><span id="topp">topP</span><a href="#topp" class="header-anchor"></a></h3><p>topp sampling 又叫nucleus sampling,其思路是不在从通过保留最大的K 个作为候选，而是保留概率累计和范围$p$ 内的所有词作为候选集，候选集大小随着分布变化而动态调整。</p>
<ol>
<li>对词表进行排序，从大到小进行排列</li>
<li>依照概率大小，依次将词加入候选集，直到新增的词进入候选集后，整个候选集内的概率累计和大于$p$ 停止；</li>
<li>对候选集内的词进行归一化，构造新的分布$\widetilde{P}$</li>
<li>从分布$\widetilde{P}$ 采样一个词作为$w_t$</li>
</ol>
<p>topk 与topp 也可以一起使用，通常实现时是先进行topk，然后在topk 归一化后的候选集上进行topp 采样。</p>
<h3><span id="beam-search-sampling">beam-search sampling</span><a href="#beam-search-sampling" class="header-anchor"></a></h3><p>该方法是beam-search 的samling 版，其主要思路是： 在每次选择时，不在直接选择所有候选中概率最高的num_beams 个，而是从中采样。</p>
<h2><span id="generate-parameters">generate parameters</span><a href="#generate-parameters" class="header-anchor"></a></h2><h3><span id="temperature">Temperature</span><a href="#temperature" class="header-anchor"></a></h3><p>通常模型的输出是一些值（logits)而不是分布（probability distribution),我们需要将其转换成分布，转换通常使用的是softmax 函数：<br>$$\frac{\exp(z_i)}{\sum_jexp(z_j)}$$</p>
<p>softmax 函数的特点是：</p>
<ol>
<li>保持其原有的相对顺序;</li>
<li>累计和为1。</li>
</ol>
<p>然而其缺点也明显：很容易扩大/缩小内部元素的差异（softmax 变 max/mean），如 [11, 12, 13 ] softmax 后为[0.0900, 0.2447, 0.6652], 这将导致最终我们采样后的结果不够丰富；而[0.01,0.02,0.03] softmax 后[0.3300, 0.3333, 0.3367],这将导致最终的采样方法是在随机采样，生成不合理的序列。为为了解决这个问题，我们需要有个办法调节，让softmax 后的分布进一步符合我们的预期，对应的办法就是增加参数$T$:<br>$$\frac{\exp(z_i/T)}{\sum_jexp(z_j/T)}$$<br><img src="/2023/03/10/llm-inf/1_p1iKxUJcXDlSEZCpMCwNgg.gif" alt></p>
<p>$T$ 越大，分布越趋近均匀分布(uniform distribution)，采样结果随机性越大，生成的序列是不合理句子的概率就越高；<br>$T$ 越小，分布越趋近与单点分布(one-point distribution)，采样结果越趋近保持一致。</p>
<h3><span id="penalty">* penalty</span><a href="#penalty" class="header-anchor"></a></h3><p>除了temperature 这种对整体分布进行修改外，还有些场景需要我们对特定的某些token 的分布进行修改，此时就诞生了各种penalty，如repetition_penalty/diversity_penalty等参数。注意：这里是对其分布乘以一个系数，其结果有可能并不改变其大小顺序。</p>
<h3><span id="qiang-zhi-jin-zhi-te-ding-token-de-chu-xian">强制/禁止 特定token 的出现</span><a href="#qiang-zhi-jin-zhi-te-ding-token-de-chu-xian" class="header-anchor"></a></h3><p>一些场景下，我们要求更加苛刻：希望某些token 一定/永不 出现，同理，此时将满足条件的token 的分布直接调整为100%/0%，如bad_words_ids/no_repeat_ngram_size/force_words_ids 等参数。</p>
<h1><span id="gpu">GPU</span><a href="#gpu" class="header-anchor"></a></h1><p>下面已A100-80G SXM 为例，了解一下GPU 的基本信息。</p>
<p>A100 specifications<br><img src="/2023/03/10/llm-inf/Pasted image 20230307111416.png" alt><br>simplified view of the GPU architecture<br><img src="/2023/03/10/llm-inf/Pasted image 20230309090721.png" alt></p>
<p>Multiply-add operations per clock per SM<br><img src="/2023/03/10/llm-inf/Pasted image 20230308160737.png" alt></p>
<ul>
<li>FP16 Tensor Core peak dense throughputs:<br>$$1024 (Tensor cores) \ast 108 (SMs) \ast 1.41 GHz (clock rate)  \ast 2 (multiply-add) =  312 TFLOPS $$</li>
</ul>
<h2><span id="performance">performance</span><a href="#performance" class="header-anchor"></a></h2><p>性能主要受限于三个因素：内存带宽(memory bandwidth)，计算带宽(math bandwidth), 延时(latency).</p>
<p>假设我们访问内存花费的时间为$T_{mem}$ ,计算花费的时间为$T_{math}$, 计算和访问内存可以”你算上一个，我读/写 下一个“，这样两者的时间大部分都可以重叠，此时花费的所有时间就为$max (T_{mem}, T_{math})$.<br>$$T_{mem} = bytes/BW_{mem}$$<br>$$<br>  T_{math} = ops/BW_{math}<br>$$<br>当性能受限于计算带宽时：<br>$$<br>T_{math} &gt; T_{mem}<br>=&gt;<br>ops/BW_{math} &gt; bytes/BW_{mem} =&gt; ops/bytes &gt; BW_{math}/BW_{mem}<br>$$<br>假设我们使用float16 进行计算，而A100-80G SXM  对应的$BW_{math} / BW_{mem}=312e12 flops/2039e9 bytes=153  Flops/Byte$.<br>即在当前显卡下，如果一个函数的计算量与所需的存储量的比值&gt; 153 Flops/Byte 时，此时的性能主要受限于计算带宽，反之则受限于内存带宽。</p>
<blockquote>
<p> 如何计算一个矩阵乘法的flops<br> 假设$A∈R^{1×n}$  ,$B∈R^{n×1}$,计算$AB$ 则需要 n 次乘法运算(operations)和n 次加法运算，对应的就是 $2n$ operations, 即$2n$ flops；同理：$A∈R^{m×n}$, $B∈R^{n×p}$ ,则需要 $2mnp$ flops.</p>
</blockquote>
<blockquote>
<p>如何计算存储大小<br>通常我们使用float16/bfloat16 存储，对应一个参数需要2 个bytes ,对于矩阵$A∈R^{m×n}$ 则需要 $2 \ast m \ast n$ bytes</p>
</blockquote>
<p>下面做个最简单的矩阵乘法：$A∈R^{1×n}$，$B∈R^{n×p}$<br>$$ops/bytes = 2np / (2n + 2np + 2p) &lt; 1 &lt; 153$$<br>除了需要将矩阵读进来还需要将结果写进内存，参数使用半精度(2 bytes)，所以对应的总内存为: $2n + 2np + 2p$<br>此时的性能主要受限于内存带宽，如果此时将$A$ 扩大$153$ 倍至 $A∈R^{153×n}$ 消耗的计算时间是一样的，忽略内存带宽$(152 \ast 2 \ast n + 152 \ast 2 \ast p )$ bytes新增带来的影响，则扩大batch size 整体latency 基本不变。</p>
<p>具体深度学习Layer示例</p>
<table>
<thead>
<tr>
<th>operation</th>
<th>ops/bytes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear layer (4096 outputs, 1024 inputs, batch size 512)</td>
<td>315 Flops/B</td>
</tr>
<tr>
<td>Linear layer (4096 outputs, 1024 inputs, batch size 1)</td>
<td>1 Flops/B</td>
</tr>
<tr>
<td>Max pooling with 3x3 window and unit stride</td>
<td>2.25 Flops/B</td>
</tr>
</tbody>
</table>
<h1><span id="gpt-generation">GPT generation</span><a href="#gpt-generation" class="header-anchor"></a></h1><p>GPT2 model architecture</p>
<p><img src="/2023/03/10/llm-inf/Pasted image 20230310005359.png" alt></p>
<h2><span id="ops-vs-bytes">ops vs bytes</span><a href="#ops-vs-bytes" class="header-anchor"></a></h2><p>我们忽略所有的bias/layernorm/activation/add 操作,计算生成一个token 时的ops vs bytes, 切分head 与不切对应的ops/bytes 是一样的，为了简单我们按不切计算。</p>
<ol>
<li><p>Embedding<br>multiply:$E_{token}∈R^{V\times d_{model}}$,  $E_p\in R^{s_{maxseq} \times d_{models}}$<br>mem: $2 \ast (V \ast d_{model} + s_{maxseq} \ast d_{model})$<br>flops:   0</p>
</li>
<li><p>attention qkv<br>multiply: $t_e \in R^{1 \times d_{model}}, W_q, W_K, W_V\in R^{d_{model}\times d_{model}}, t_e \times W_q/W_k/W_v =&gt; q/k/v$<br>mem: $ 2 \ast 3\ast d_{model}^2$<br>flops: $ 2 \ast 3 \ast d_{model}^2$</p>
</li>
<li><p>attention output<br> multiply: $q,k,v\in R^{1\times d_{model}}, softmax((q\cdot k)\div \sqrt{d_{head}}) \cdot v =&gt; o$<br> mem: 0<br> flops: $2 \ast 2 \ast d_{model}$</p>
</li>
<li><p>output projection<br>multiply: $o \in R^{1\times d_{model}}$,   $W_o\in R^{d_{model}\times d_{model}}$, $o \times W_o =&gt; a$<br>mem: $2\ast d_{model}^2$<br>flops: $2 \ast d_{model}^2$</p>
</li>
<li><p>feed-forward<br> multiply:$a\in R^{1\times d_{model}}$,   $W_1\in R^{d_{model}\times 4d_{model}}$, $W_2\in R^{4d_{model}\times d_{model}}$, $a\times W_1 \times W_2 =&gt;z$<br> mem: $2 \ast 2 \ast d_{model} \ast 4d_{model} = 16 d_{model}^2$<br> flops: $2 \ast 2 \ast d_{model} \ast 4d_{model} =&gt; 16 d_{model}^2$</p>
</li>
</ol>
<p>剩下的计算就是重复$n\_layers$ 次2-5.现在我们来汇总一下计算总的ops / bytes:</p>
<p>$$<br>\frac{ops}{ bytes} = \frac{n_{layers} \ast (6d_{model}^2 + 4d_{model} + 2d_{model}^2 + 16d_{model}^2)}  {2 \ast (V \ast d_{model} + s_{maxseq} \ast d_{model}) + n_{layers} \ast (6d_{model}^2 + 2d_{model}^2 + 16d_{model}^2)}<br>$$</p>
<p>越大的模型$d_{model}$ 越大，如10B 的模型 $d_{model}=4096$, 所以这里我们去掉$d_{model}$ 的常数项：</p>
<p>$$<br>\frac{ops} { bytes} \approx \frac{n_{layers} \ast (6d_{model}^2 + 2d_{model}^2 + 16d_{model}^2)}  { n_{layers} \ast (6d_{model}^2 + 2d_{model}^2 + 16d_{model}^2)} = 1<br>$$</p>
<blockquote>
<p>速记法<br>对于transformer-base encoder/decoder :<br>ops $\approx 2 \ast Parameters$,<br>float16/bfloat16 下:bytes $\approx 2 \ast Parameters$.</p>
</blockquote>
<p>按照上面我们的结论，此时对于A100-80G 来说，性能主要受限于内存带宽，计算1 个token 与计算153个token 对应的latency 是一样的！为了提升整体性能，我们应该增加$batch\_size \ast input\_length$直到 inputs 扩大至153倍.</p>
<h2><span id="attention">attention</span><a href="#attention" class="header-anchor"></a></h2><p>现在回过头来再看一眼刚刚被我随意丢弃掉的一项ops：attention output<br>由于我们计算是由长度为1 的token 生成 1 个token， 假设输入的token 长度为$s$, 则：<br>$q,k,v \in R^{s\times d_{model}}$<br>flops: $ 2 \ast s \ast d_{model} \ast s + 2 \ast s \ast d_{model} \approx 2 \ast s^2 \ast d_{model}$  </p>
<p>随着inputs 的长度增加，达到$\sqrt{d_{model}}$ 时，这一项已经不能丢弃了，进一步增加，长度到$12 \ast \sqrt{d_{model}}$ 时，这一项已经与其他部分的ops 想当了，此时的$ops/bytes \approx 2$.<br>以10B 的模型为例，$d_{model} = 4096$ ,当输入的长度为64 时，attention output 这项ops 与其他部分想当；当长度增长为768 时，对应的$ops/bytes$ 增长到2，此时整体的计算量已经扩大了768 倍，$768 &gt; (153 / 2 ) $，已经转变为性能受限于计算带宽，增大batch 不会带来性能提升。</p>
<h2><span id="cache">cache</span><a href="#cache" class="header-anchor"></a></h2><p>随着整个生成过程的迭代，输入的长度会逐步增加，对应的每步所需的计算量也指数倍的增加，整体性能都被他拖垮了。<br>对于$Y = A \cdot X$ ,当$X$ ,$A$ 不变时，对应的结果$Y$ 是不变的；而在迭代过程中，模型的权重是不变的，即$X$ 都是一样的，对于$step_t$ 与 $step_{t-1}$ 来说，都会计算$A[:, :t-1]$ 对应的结果，既然重叠了，就不需要重复计算了，我们每次只需要计算最后一个token 就好了。<br>等等，有一个层只有计算没有weights，即其对应的$A$, $X$ 都来动态的，需要我们单独处理。这一层就是attention output。（哈，又是你。attention is all I want to remove！）</p>
<p>对于$step_t$ 来说，对应的计算是:<br>$$softmax(q[:, t-1:t] \cdot k[:,0:t] \div \sqrt(d_head) ) \cdot v[:,0:t]$$<br>此时我们缺少的是$k[:,:t-1]/v[:,:t-1]$ ,好在我们在$step_{t-1}$ 时已经计算过这两个结果了，只要把上一个时刻的这个结果“拿到”拼接回去即可。<br>完整的流程是：对于迭代$step_t$ ,把对应attention qkv 的结果$q[:,t]/k[:,t]/v[:,t]$ 拼接到之前的之前的缓存$q[:,:t-1]/k[:,:t-1]/v[:,:t-1]$ 后面，为下一个时刻准备；计算attention output 时，先将之前的$k\_cache[:,t-1]/v\_cache[:,:t-1]$ 拼接到$k[:,t]/v[:,t]$ 前面，还原真实的$k/v$.</p>
<p>经过cache 后，attention output 的计算回到了$O(s\cdot d_{model})$ 的水平，至少在大部分时候又可以基本忽略了。此外，随着$s$ 的增长，对应的cache 也在线性增长，需要的内存带宽也在增长，所以有些情况下可能增加cache 反而降低性能。</p>
<p>PS:不管是生成过程中的k_cache/v_cache 还是prompt cache 还是context cache，只要是可能重复计算，就值得思考要不要将之前的结果做cache，已减少后续的计算量。</p>
<h2><span id="ding-liang-ping-gu-you-hua-fang-an">定量评估优化方案</span><a href="#ding-liang-ping-gu-you-hua-fang-an" class="header-anchor"></a></h2><p>上面是一个相对简化的思路计算，而真实情况会比较复杂。比如我们忽略了一部分操作所需的ops；中间结果的每次写入/读取所需的内存带宽；CUDA Core 与 Tensor Core 的性能差异等；此外不同的硬件参数也不相同，具体的operation 实现也不相同，所以要定量给出优化方案实在太难，通常（主要是我）只能通过测试给出大致范围。</p>
<h1><span id="jie-lun">结论</span><a href="#jie-lun" class="header-anchor"></a></h1><p>现在回到最初的问题：为什么当前的LLM 生成文本时对应的资源与prompt 与 completion 都有关系：</p>
<ol>
<li>需要迭代$n$ 次，其中$n = len(completion)$</li>
<li>每次迭代所需要的资源都接近$O(s)$ ,其中$s_i=len_(prompt) + step_i$</li>
</ol>
<h1><span id="ref">ref</span><a href="#ref" class="header-anchor"></a></h1><p><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf" target="_blank" rel="noopener">NVIDIA A100 TENSOR CORE GPU</a></p>
<p><a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#gpu-arch" target="_blank" rel="noopener">dl-performance-gpu-background</a></p>
<p><a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/" target="_blank" rel="noopener">nvidia-ampere-architecture-in-depth</a></p>
<p><a href="https://huggingface.co/blog/how-to-generate" target="_blank" rel="noopener">how to generate</a></p>
<p><a href="https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71" target="_blank" rel="noopener">softmax-temperature</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>stable diffusion生成</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Inference</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>From zero to ChatGPT</title>
    <url>/2023/01/09/zero-to-chatgpt/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="toc">

<!-- toc -->
<ul>
<li><a href="#language-model">Language Model</a><ul>
<li><a href="#tong-ji-yu-yan-mo-xing">统计语言模型</a></li>
<li><a href="#shen-jing-wang-luo-yu-yan-mo-xing">神经网络语言模型</a></li>
<li><a href="#sheng-ji">升级</a></li>
<li><a href="#gpt-3">GPT-3</a></li>
</ul>
</li>
<li><a href="#prompt-engineering">Prompt engineering</a><ul>
<li><a href="#pet">PET</a></li>
<li><a href="#automated-discrete-prompt">Automated Discrete Prompt</a></li>
<li><a href="#automated-continuous-prompt">Automated Continuous Prompt</a></li>
<li><a href="#multi-step-reasong-san-bu-zou">Multi-Step Reasong（三步走)</a></li>
<li><a href="#cot">CoT</a></li>
<li><a href="#least-to-most-prompting">Least-to-Most Prompting</a></li>
<li><a href="#self-ask">self-ask</a></li>
<li><a href="#kuo-zhan-ce-shi">扩展测试</a></li>
</ul>
</li>
<li><a href="#emergent-abilities">Emergent Abilities</a><ul>
<li><a href="#scaling-up">Scaling Up</a></li>
</ul>
</li>
<li><a href="#alignment">Alignment</a><ul>
<li><a href="#fine-tuning-with-human-feedback">Fine-tuning with human feedback</a><ul>
<li><a href="#sft">SFT</a></li>
<li><a href="#feedme">FeedME</a></li>
<li><a href="#reinforcement-learning-with-human-feedback">Reinforcement learning with human feedback</a></li>
</ul>
</li>
<li><a href="#instruction-tuning">Instruction-tuning</a></li>
</ul>
</li>
<li><a href="#chatgpt">ChatGPT</a></li>
<li><a href="#fan-wai-pian">番外篇：</a><ul>
<li><a href="#ru-he-ti-sheng-llm-zai-mou-ge-zu-te-ding-ren-wu-shang-de-xing-neng">如何提升LLM在某个(组）特定任务上的性能</a><ul>
<li><a href="#fine-tuning">Fine-tuning</a></li>
<li><a href="#fine-tuning-with-human-feedback-1">Fine-tuning with human feedback</a></li>
<li><a href="#rag">RAG</a></li>
<li><a href="#prompt-engineering">Prompt Engineering</a></li>
<li><a href="#instruction-tuning-1">Instruction-tuning</a></li>
</ul>
</li>
<li><a href="#ru-he-jiang-neng-li-cong-da-mo-xing-qian-yi-dao-xiao-mo-xing-shang">如何将能力从大模型迁移到小模型上</a></li>
</ul>
</li>
<li><a href="#ref">Ref</a></li>
<li><a href="#guan-yu-tou-tu">关于头图</a></li>
</ul>
<!-- tocstop -->
<p></div></p>
<h1><span id="language-model">Language Model</span><a href="#language-model" class="header-anchor"></a></h1><p>长期一来，人类一直梦想着让机器替代人来完成各种工作，其中也包括语言相关工作，如翻译文字，识别语言，检索、生成文字等。为了完成这些目标，就需要机器理解语言。最早人们想到的办法是让机器模拟人类进行学习，如学习人类通过学习语法规则、词性、构词法、分析语句等学习语言。尤其是在乔姆斯基（Noam Chomsky 有史以来最伟大的语言学家）提出 “形式语言” 以后，人们更坚定了利用语法规则的办法进行文字处理的信念。遗憾的是，几十年过去了，在计算机处理语言领域，基于这个语法规则的方法几乎毫无突破。</p>
<h2><span id="tong-ji-yu-yan-mo-xing">统计语言模型</span><a href="#tong-ji-yu-yan-mo-xing" class="header-anchor"></a></h2><p>另一个对自然语言感兴趣的就是香农，他在很早就提出了用数学的方法来处理自然语言的想法。但是当时即使使用计算机技术，也无法进行大量的信息处理。不过随着计算机技术的发展，这个思路成了一种可能。<br>首先成功利用数学方法解决自然语言问题的是贾里尼克 (Fred Jelinek) 及他领导的IBM Wason实验室。贾里尼克提出的方法也十分简单：判断一个词序列（短语，句子，文档等）是否合理，就看他的可能性有多大。举个例子：判断“I have a pen” 翻译为中文”我有个笔“是否合理，只需要判断”I have a pen.我有个笔” 这个序列的可能性有多大。而如何判断一个词序列的可能性，就需要对这个词序列的概率进行建模，也就是统计语言模型：$S$ 表示一连串特定顺序排列的词$w_1$,$w_2$, …, $w_n$，n 是序列的长度，则$S$出现的概率$P(S)=P(w_1,w_2,…w_n)$.<br>但是这个概率$P(S)$ 很难估算，所以这里我们转化一下。首先，利用条件概率公式将其展开:<br>$$<br>P(S)=P(w_1,w_2,..w_n)=P(w_1)∗P(w_2|w_1)∗P(w_3|w_1,w_2)∗…∗P(w_n|w_1,w_2,..w_{n−1})<br>$$<br>即：<br>$$<br>P(w_{1}^{n})=\prod_{i=1}^{n}P(w_i|w_{1}^{i-1})<br>$$</p>
<p>接着，我们利用马尔可夫假设，即任意一个词$w_i$ 出现的概率只与其前一个词$w_{i-1})$（或有限的几个） 有关。于是，问题就变的简单了许多。对应的$S$ 的概率就变为:<br>$$<br>P(S)=P(w_{1}^{n})=\prod_{i=1}^{n}P(w_i|w_{1}^{i-1})\approx\prod_{i=1}^{n}P(w_i|w_{i-1})<br>$$<br>以上对应的便是一个二元模型，当然，如果词由其前面的$N-1$ 个词决定，则对应的是N元模型。</p>
<h2><span id="shen-jing-wang-luo-yu-yan-mo-xing">神经网络语言模型</span><a href="#shen-jing-wang-luo-yu-yan-mo-xing" class="header-anchor"></a></h2><p>统计语言模型有很多问题：1.训练语料中未出现过的词（句子）如何处理(OOV);2.长尾低频词如何平滑；3.one-hot 向量带来的维度灾难；4.未考虑词之间的相似性等。<br>为了解决上述问题，Yoshua Bengio(深度学习三巨头之一）在2003年提出用神经网络来建模语言模型，同时学习词的低维度的分布式表征(distributed representation),具体的：<br>1.不直接对$P(w_{1}^{n})$ 建模，而是对$P(w_i|w_{1}^{i-1})$进行建模;<br>2.简化求解时，不限制只能是左边的词，也可以含右边的词，即可以是一个上下文窗口(context) 内的所有词；<br>3.共享网络参数。</p>
<p>具体形式如下：<br>$$<br>P(w_t=i|w_{1}^{t-1})=f(i,w_{t-1},…,w_{t-n+1})=g(i, C(w_{t-1}),…,C(w_{t-n+1}))<br>$$<br><img src="/2023/01/09/zero-to-chatgpt/NNLM.png" alt="NNLM"><br>由于当时的计算机技术的限制，神经网络语言模型的概率结果往往都不好（一层MLP效果肯定好不了），所以当时主要还是用这个形式训练词向量。</p>
<h2><span id="sheng-ji">升级</span><a href="#sheng-ji" class="header-anchor"></a></h2><p>随着数据、算力、模型架构、范式等的升级，神经网络语言模型也得到了长足的发展。如模型架构从 mlp 到cnn/rnn 又到目前的transformer-base ，对应的能力也在不断发展，从之前只对$P(w_i|w_{i-1})$ 建模，通过”“并行”或“串行” 的方式，也可以对$P(w_i^n)$建模。求解NLP task 从原来的word2vector + ML 发展为pretrain + fine-tuning。目前最有代表性的就是BERT和GPT。<br><img src="/2023/01/09/zero-to-chatgpt/bert.jpeg" alt="BERT"><br><img src="/2023/01/09/zero-to-chatgpt/GPT.jpeg" alt="GPT"></p>
<p>BERT: 双向，autoencoding，MLM，encoder</p>
<p>GPT：left-to-right, autoregressive, LM, decoder</p>
<h2><span id="gpt-3">GPT-3</span><a href="#gpt-3" class="header-anchor"></a></h2><p>随着NLP进入BERT时代后，pretrain + fine tune 这种方式可以解决大量的NLP 任务，但是他依然有很多限制：<br>1.每个任务都需要大量的标注数据，这大大限制了模型的应用。此外，还有大量不好收集标注数据的任务存在；<br>2.虽然pretrain 阶段模型吸收了大量知识，但是fine-tuned 后模型又被“缩”到一个很窄的任务相关的分布上，这也导致了一些问题，比如在OOD（out-of-distribution) 上表现不好；<br>3.如果参考人类的话，人类通常不需要在大量的标注数据上学习后才能做任务，而只需要你明确告知你想让他干嘛（比如：将所给单词翻译为英语：红色-&gt;）或者给他几个例子(比如：蓝色-&gt;blue,绿色-&gt;green,红色-&gt;)，之后便能处理新的任务了。<br>而我们的一个终极目标就是希望模型能像人这样，灵活的学习如何帮助我们完成工作。一个可能的方向就是元学习（meta-learning)：学习如何学习。而在LM语境下，即我们希望LM 在训练的时候能获得大量的技能和模式识别的能力，而在预测的时候能快速将技能迁移到新任务或者识别出新任务的要求。为了解决这个问题，一个显现出一定有效性的方法就是”in-context learning”:用指令(instruction)或者少量示例(demonstrations)组成预训练语言模型的输入，期望模型生成的内容可以完成对应的任务。根据提供多少个示例，又可以分为zero-shot, one-shot, few-shot。<br><img src="/2023/01/09/zero-to-chatgpt/few-shot.png" alt="Learning via SGD vs In-context learning"></p>
<p><img src="/2023/01/09/zero-to-chatgpt/in-context.png" alt="In-context learning vs Fine-tuning"></p>
<p>虽然in-context learning 被证明具有一定的有效性，但是其结果相比fine-tuing 还有一定的距离。而随着预训练语言模型(PTM)规模的扩大(scaling up),对应的在下游task 上的表现也在逐步上升，所以OpenAI就猜想：PTM的进一步scaling up,对应的in-context learning 的能力是不是也会进一步提升？于是他们做了GPT-3 系列模型，最大的为GPT-3 175B。<br><img src="/2023/01/09/zero-to-chatgpt/abstract.jpeg" alt="gpt3 abstract"></p>
<p>最终的模型效果简单总结一下：一些任务上few-shot (zero-shot)能赶上甚至超过之前fine-tuned SOTA(如：PIQA),有些任务上还达不到之前的SOTA(如：OpenBookQA)；能做一些新task，如3位数算数。<br>不过他们也发现了模型存在的一些问题，并提出了一些可能的解决方案。（所以OpenAI 在2020 年就定下了未来的方向，持续投入至今）<br><img src="/2023/01/09/zero-to-chatgpt/limits.jpeg" alt="limitation and future directions"></p>
<h1><span id="prompt-engineering">Prompt engineering</span><a href="#prompt-engineering" class="header-anchor"></a></h1><p>zero-shot/few-shot 这种设定确实给NLP 社区带来了新的思路，但是$175B$ 的模型实在是太大了，即不好训练又不好微调也不好部署上线，如何在小模型上应用呢？此外，不同的pattern(prompt)下同一个task 的效果差距也非常大，如何找到效果最好的prompt 呢？于是大家就开始花式探索prompt，NLPer 也变成了prompt-engineer (误).PS：prompt 的语义目前即可以指模型的输入，也可以指输入的一部分。<br><img src="/2023/01/09/zero-to-chatgpt/prompt.jpeg" alt="prompt methods"></p>
<h2><span id="pet">PET</span><a href="#pet" class="header-anchor"></a></h2><p>PET(Pattern-Exploiting Training)应该是第一个（至少是我知道的）在小模型上在few-shot 设定下成功应用的工作。<br>PET 的主要思路是：</p>
<ol>
<li>用通顺的语言为task 构造一个pattern(prompt),如: “下面是{label}新闻。{x}”;</li>
<li>将label 映射为文字。如: “0-&gt;体育 ，1-&gt; 财经, 2-&gt;科技”;</li>
<li>将样本按照pattern 进行重构，冻结模型主体，只更新label 对应的token（embedding),继续LM (MLM) 训练；</li>
<li>预测时，将label 对应位置的token 再映射回label。</li>
</ol>
<p><img src="/2023/01/09/zero-to-chatgpt/bert-pet.jpeg" alt="BERT-PET"><br><img src="/2023/01/09/zero-to-chatgpt/gpt-pet.jpeg" alt="GPT-PET"></p>
<p>PET 在few-shot 的设定下，利用BERT-base 就能获得比GPT-3 175B 更好的结果。不过pattern 是需要人来构造的，pattern 的“好坏” 直接影响最终的效果。<br>思考：PET中的fine-tuning 是与其pretrain 的形式是一致的，而pretrain 与 fine-tuning 形式一致能够work 才是一种“自然”的事情，pretrain + fine-tuning 这种下游任务与预训练形式不一致能work 其实不是一个自然的事情，为什么pretrain + fine-tuning 能work 值得思考。</p>
<h2><span id="automated-discrete-prompt">Automated Discrete Prompt</span><a href="#automated-discrete-prompt" class="header-anchor"></a></h2><p>人来写prompt还是需要大量的时间和经验，而且，即使一个经验丰富的人，写出的prompt 也可能是次优的。为了解决这些问题，一种办法就是“自动”的帮助我们寻找最优的prompt。</p>
<ol>
<li>Prompt Mining: 该方法是在语料上统计输入X 与输出Y 之间的中间词或者依赖路径，选取最频繁的作为prompt,即：{X} {middle words} {Y}.</li>
<li>Prompt Paraphrasing: 该方法是基于语义的，首先构造种子prompts，然后将其转述成语义相近的表达作为候选prompts，通过在任务上进行验证，最终选择效果最优的。</li>
<li>Gradient-based Search: 通过梯度下降搜索的方式来寻找、组合词构成最优prompt。</li>
<li>Prompt Generation: 用NLG 的方式，直接生成模型的prompts。</li>
<li>Prompt Scoring: 构造模型对不同的prompt 进行打分，选择分数最高的prompt 作为最优prompt。<h2><span id="automated-continuous-prompt">Automated Continuous Prompt</span><a href="#automated-continuous-prompt" class="header-anchor"></a></h2>虽然PET最初在构造prompt 时认为prompt需要是通顺流畅的自然语言。而随着各种自动化方法构造出了很多虽然句子不通顺但是效果更好的prompt，大家也发现：通顺流畅的自然语言或者是自然语言的要求只是为了更好的实现预训练与下游任务的“一致性”，但是这并不是必须的，我们其实并不关心这个pattern 具体长什么样，我们真正关心的是他有哪些token 组成，都插入在什么位置，输出空间是什么，以及最重要的在下游任务上表现有多好。<br><img src="/2023/01/09/zero-to-chatgpt/p-tuning.jpeg" alt="P-tuning"></li>
</ol>
<p>如上图所示，其中[u*] 为unused token,我们在tuning 时依然冻结模型的参数，只微调[u1-u8] 这8个token。</p>
<ol>
<li>prompt tuning: 利用N 个unused token/new token 构造prompt， 然后微调这N 个token。其中N 是个超参数。</li>
<li>Tuning initialized with Discrete prompts:用手工构造的prompt 或者自动搜索的离散prompt 初始化需要微调的token，然后进行prompt tuning，有利于提高准去率。</li>
<li>Hard-Soft Prompt Hybrid Tuning: 这类方法将手动设计和自动学习相结合，增强prompt token 之间的相关性。如p-tuning 首先通过一个LSTM 训练手工设计的prompt中插入的可学习的token 来增强prompt 之间的相关性，让prompt token 更接近“自然语言”。</li>
<li>Prompt-parameter Tuning: 仅仅训练prompt token 效果不够好，将其与fine-tuning 结合。如prefix-tuning，在输入前增加可学习的prompt token 的同时，在模型每层都增加一部分可学习参数。<h2><span id="multi-step-reasong-san-bu-zou">Multi-Step Reasong（三步走)</span><a href="#multi-step-reasong-san-bu-zou" class="header-anchor"></a></h2>虽然大模型在很多task 都证明了其有效性，但是这些task 都是System 1 thinking，而System 2 thinking 任务需要更多的数学、逻辑以及常识推理。大模型对这类任务还做不好目前，如数学推理、符号推理等。</li>
</ol>
<blockquote><p>Our responses to these two scenarios demonstrate the differences between our slower thinking process and our instantaneous one. System 1 thinking is a near-instantaneous process; it happens automatically, intuitively, and with little effort. It’s driven by instinct and our experiences. System 2 thinking is slower and requires more effort. It is conscious and logical.</p>
</blockquote>
<p>   – <a href="https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking" target="_blank" rel="noopener">system-1-and-system-2-think</a></p>
<p>如GPT-3 175B 在GSM8K 上直接fine-tuning 也只能得到33% 的准确率，通过在fine-tuned model 上进行采样，再标注答案是否正确，然后训练一个verifier 来判断生成的答案是否正确，最终也只能得到55%，而一个9-12 岁的孩子平均能得到60%。所以，OpenAI 的研究员认为，如果想达到80% 以上，可能需要把模型扩大到$10**16$（175T?妈妈咪啊）。<br>然而，后续的工作Gopher 却给这个思路泼了盆冷水：即使继续放大模型，模型在这种推理任务上的表现也不会显著提升。也许语言模型就不能做推理这种system 2 thinking task。</p>
<h2><span id="cot">CoT</span><a href="#cot" class="header-anchor"></a></h2><p>“不就是个张麻子嘛，办他！”(误）不就是推理嘛，LLM 也能做，只需要向人学习一下就行了。<br><img src="/2023/01/09/zero-to-chatgpt/cot.png" alt="CoT"></p>
<p>回想读书时做数学应用题目，老师总是要求你写清解题步骤。而之前的方法总是让模型一步到位，直接给出答案，所以模型考不好。现在我们让模型像人类推理一样，先给出思考步骤(chain of thought)，然后再给出答案，模型的推理能力就能大大提高了。而这个思路，只需要few-shot(8 examples)就能达到58.1% 的准确率，超过之前GPT-3 175B fine-tuning + verifier （SOTA）。除了GSM8K 这种算术推理外，在符号推理、尝试推理任务上CoT 也是能将模型的性能提升一大截。<br>CoT 确实很酷，改变了我们之前对LLM 的认知，但是还不够酷：很多时候我们不一定能凑够8个样本（我就无法快速给出8个带有解题步骤的数学题)，那能不能在zero-shot 下让模型自己给出解题思路跟答案呢？<br><blockquote><p>“Let’s think step by step.”</p>
</blockquote><br>没错，答案就是这句话。只要在输入后面加一句”Let’s think step by step.”哄哄模型，模型就会自己按照CoT 的方式先生成解题思路，然后再生成对应的答案。（我第一次读完paper 的感觉就是离谱他妈给离谱开门，离谱到家了）PS:这句话是试出来的，还有很多类似的表达但是效果不如这句好。<br><img src="/2023/01/09/zero-to-chatgpt/zero-shot-cot.jpeg" alt="Zero-shot-CoT"></p>
<table>
<thead>
<tr>
<th>method</th>
<th style="text-align:center">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>9-12 year olds (Cobbe et al,.2021)</td>
<td style="text-align:center">60</td>
</tr>
<tr>
<td>Finetuned GPT-3 175B</td>
<td style="text-align:center">33</td>
</tr>
<tr>
<td>Finetuned GPT-3 + verifier</td>
<td style="text-align:center">55</td>
</tr>
<tr>
<td>PaLM 540B: standard prompting</td>
<td style="text-align:center">17.9</td>
</tr>
<tr>
<td>PaLM 540: chain of thought prompting</td>
<td style="text-align:center">58.1</td>
</tr>
<tr>
<td>GPT-3 175B + Complexity-based Consistency</td>
<td style="text-align:center">72.6</td>
</tr>
<tr>
<td>PaLM 540B: Cot + majority voting</td>
<td style="text-align:center">74.4</td>
</tr>
<tr>
<td>Codex 175B (GPT3.5) + complex chains of thought</td>
<td style="text-align:center">82.9</td>
</tr>
<tr>
<td>PaLM 540B: Zero-Shot</td>
<td style="text-align:center">12.5</td>
</tr>
<tr>
<td>PaLM 540B: Zero-Shot-Cot</td>
<td style="text-align:center">43</td>
</tr>
<tr>
<td>PaLM 540B: Zero-Shot-Cot + self consistency</td>
<td style="text-align:center">70.1</td>
</tr>
</tbody>
</table>
<p>Zero-Shot-Cot 就能获得43% 的准确率，而Zero-Shot-Cot + self consistency 甚至可以获得70.1的准确率。<br>Zero-Shot-CoT + self consistency: 按照Zero-Shot-Cot 的方式，通过采样(sample)让模型生成多个结果，然后对答案进行投票。<br>目前在GSM8K上的SOTA是82.9，看来不需要继续放大模型，只需要正确使用模型。</p>
<p>关于CoT 来源的问题，目前的主要推论是可能来自预训练时数据中包含了代码数据(code),主要论据为：1.GPT-3.5(Codex) 有CoT 能力，PaLM 也有，而其他LLM （包括原始GPT-3）却没有，这两个模型与其他模型的一个主要区别就是增加了代码数据；2.有工作认为CoT 与代码的自然语言翻译形式相同，所以CoT 可能来自这种能力的迁移。</p>
<h2><span id="least-to-most-prompting">Least-to-Most Prompting</span><a href="#least-to-most-prompting" class="header-anchor"></a></h2><p>如果仔细对比CoT 和之前的prompt 的话，其中最大的不同是CoT 模仿人类推理将过程分为多个阶段。而有些问题如组合泛化直接用CoT 也不好解决。于是就提出了另一种多步推理的方法，Least-to-Most Prompting:<br>首先将问题分解为子问题“To solve{Q}, we need to first solve: sub-q”，得到子问题的答案后再来给出最后的答案.<br><img src="/2023/01/09/zero-to-chatgpt/least-to-most.png" alt="Least-to-most"></p>
<h2><span id="self-ask">self-ask</span><a href="#self-ask" class="header-anchor"></a></h2><p>self-ask:先让LLM 自问自答生成多跳问题与答案，然后再生成最终的答案。<br><img src="/2023/01/09/zero-to-chatgpt/self-ask.jpeg" alt="self-ask"></p>
<h2><span id="kuo-zhan-ce-shi">扩展测试</span><a href="#kuo-zhan-ce-shi" class="header-anchor"></a></h2><p>以上的实验都是基于事实的推理，但是我想看看模型是不是有类似反事实推理的能力，所以我做了三个测试：<br>第一次我直接让他解释一个反事实的东西；第二次设定一个反事实(红色对应单词是”blue”), 基于此让他做一个翻译任务；第三次，在给出的例子里增加相应的事实(蓝色-&gt;blue),继续让他做这个翻译任务。<br><img src="/2023/01/09/zero-to-chatgpt/test1.png" alt="实验1"><br>实验1<br><img src="/2023/01/09/zero-to-chatgpt/test2.png" alt="实验2"><br>实验2<br><img src="/2023/01/09/zero-to-chatgpt/test3.png" alt="实验3"><br>实验3</p>
<p>三个测试结果显示模型确实有很强的推理能力，包括反事实的推理能力。此外，实验二、三显示模型有很强的基于prompt 推理的能力（in-context learning?），甚至要想更正prompt 里错误的信息需要花点心思才行。</p>
<p>PS：后面两次测试只是证明了模型”能“基于prompt 做推理，而无法证明模型”总是“基于prompt 做推理。</p>
<p>思考：</p>
<ol>
<li>目前流行的RAG(Retrieval-Augmented Generation)是不是基于模型具有的这种推理能力？</li>
<li>LLM 表现出的能胡说八道(Hallucinations) 是否也是由模型具有这种反事实推理带来的？以及如何让”胡说八道“变成”创造“。</li>
<li>这种能力也带来了一个问题：模型生成的答案并不是预训练数据优先(pretrain data first)，如果我们的prompt 里出现了反事实的东西(retrieval / dialog query/ demonstration),那模型就很可能生成一个”错误“ 的答案。</li>
</ol>
<h1><span id="emergent-abilities">Emergent Abilities</span><a href="#emergent-abilities" class="header-anchor"></a></h1><p>既然LLM 有这么多神奇的能力，包括Zero-Shot-CoT 这种推理能力。那我们之前这么多人用手工的或者自动的方式构造prompt，为什么没找到”Let’s think step by step”这句话呢？</p>
<p>原因可能是你的模型不够大（大力真的能出奇迹）。随着LLM不断的放大，当他大到一定规模时，他会突然显现出新的能力，即”涌现能”力(Emergent Abilities)。而即使是今天，我们大部分人接触的模型还是1B以下的，LLM 中被称作”small model” 的T5-11B 大部分人也用不起来，这就限制了我们发现LLM 的各种能力。<br>Emergency 的原始含义是指量变引起质变，即：<br><blockquote><p>Emergence is when quantitative changes in a system result in qualitative changes in behavior.</p>
</blockquote><br>而在LLM 语境下，其含义为在小模型中没有而在大模型中出现的能力，即:<br><blockquote><p>An ability is emergent if it is not present in smaller models but is present in larger models.</p>
</blockquote></p>
<h2><span id="scaling-up">Scaling Up</span><a href="#scaling-up" class="header-anchor"></a></h2><p><img src="/2023/01/09/zero-to-chatgpt/em.jpeg" alt="Emergent abilities"></p>
<p>上表是目前已有工作中涌现各种能力的模型及其最小规模。基本可以认为你需要至少68B parameter model (前提训练的OK)才能涌现新能力。而这里涌现新能力指的是性能优于随机，而要达到可用，你可能需要继续放大模型。如CoT 至少需要GPT-3 175B 才能优于精调小模型(t5-11b).</p>
<p>此外，与模型性能有关的不光有参数量，还有数据大小，数据质量，训练计算量，模型架构等.合理的比较应该是性能最好的LLM 在参数量上进行比较，然而我们目前还不知道如何训练让LLM 达到最优，所以同一个能力在不同模型上需要的参数量也不相同，如要涌现出2位数乘法的能力，只需要GPT-3 13B,而在LaMDA 却需要68B。</p>
<p>所以除了规模外，还有别的因素影响着是否能出现新能力：</p>
<ol>
<li>模型如何训练的，很多模型即使参数足够大，有些能力也可能不会出现。如原始GPT-3 175B、bloom-176B 等虽然参数够大，但是却都没有CoT 的能力。</li>
<li>LLM 的使用方法，fine-tuning/标准的prompt 方法在推理任务上效果不好，即使在GPT-3 175B 上效果也达不到中学生平均水平，而CoT 却只要100B parameter model 即可超越之前最好结果。</li>
<li>如何提升模型能力，在follow instruction 上,之前的工作认为至少需要68B parameter model 才能有效instruction-finetuning，而后续的flan-t5 却在11B 上就得到了更好的性能；GPT-3 经过RLFH 后的InstructGPT，在follow instruction 上， 1.3B 就已经比之前的GPT-3 175B 性能更好。</li>
<li>模型的架构，上面的结果都是transformer-based 的，而有工作验证了其他模型架构（RNN/MLP)，最后结论是其他架构即使放大，也无法像transformer-based model 一样涌现能力。<strong>again: attention is all you need!</strong></li>
</ol>
<h1><span id="alignment">Alignment</span><a href="#alignment" class="header-anchor"></a></h1><p>到目前为止，我们已经知道了LLM 有很多能力，而且随着模型规模的扩大，可能会出现更多的新能力。但是，有个问题却严重制约着他在实际中的应用：prompt engineering。仔细思考一下这个问题，其本质其实是模型认为处理一个task 的prompt 跟我们以为的不一样，如我们认为当我们说“问答：”时模型就应该知道后面的是一个QA task，而模型可能觉得，如果你想让我做QA task，你需要告诉我”妈咪妈咪哄”。</p>
<p>这就好比至尊宝已经得到了月光宝盒，但是却需要找到“般若波罗蜜”这句口诀然后喊出来才能穿越一样，而且环境稍微不同，具体穿越到哪还不一定。那更好的方式应该是我们拿到月光宝盒，然后说一句：我要穿越到白晶晶自杀前五分钟，然后我们就穿越到了对应的时空。</p>
<p>理想情况下，LLM 应该正确理解用户的指令，包括同一个任务的不同描述（LLM 应该对语义相同的instruction 表现一致，而非对prompt 的形式非常敏感）。而LLM 训练时的任务是预测下一个时刻的词(predict next token),而非处理用户的指令(follow instruction)，所以存在gap 也是很自然的事。为了缓解这个问题，一个方法就是进行“对齐”(Alignment)，缩小模型与人类对同一个instruction 之间理解的gap，从而让模型能更好的理解用户的指令。</p>
<h2><span id="fine-tuning-with-human-feedback">Fine-tuning with human feedback</span><a href="#fine-tuning-with-human-feedback" class="header-anchor"></a></h2><p>一种很直接的想法就是构造数据进行fine-tuning。所以为了让模型更好的理解人类的指令，我们需要通过人类反馈进行微调模型（human in the loop）。</p>
<h3><span id="sft">SFT</span><a href="#sft" class="header-anchor"></a></h3><p>构造人类真实场景中的指令即期望的输出，然后直接进行SFT（supervised fine-tuning）。</p>
<h3><span id="feedme">FeedME</span><a href="#feedme" class="header-anchor"></a></h3><p>进过SFT 后模型可能已经能很好的理解人类指令了，但是其答案可能有其他问题，如胡编乱造，包含色情敏感内容等，此外，靠人写数据成本高又耗时，所以我们可以对多个模型的结果进行打分(7分），然后在7/7 的数据上继续训练，对多个模型的最好结果进行蒸馏(distill)。这个方法叫FeedME(Feedback Made Easy).</p>
<h3><span id="reinforcement-learning-with-human-feedback">Reinforcement learning with human feedback</span><a href="#reinforcement-learning-with-human-feedback" class="header-anchor"></a></h3><p>即使我们从人写完整的样本转换为人给模型采样的结果进行打分，整个流程依然需要人参与，也限制了整个流程的加速。为了更高效的进行整个微调的流程，引入Reinforcement learning。该方法又叫RLHF（Reinforcement learning from human feedback）。<br><img src="/2023/01/09/zero-to-chatgpt/rlhf.jpeg" alt="RLHF"></p>
<p>具体流程：</p>
<ol>
<li>标注人员手写(prompt,completion),然后进行SFT。这里主要是得到一个比较好的初始化模型，即模型至少能有一定的follow instruction 的能力。</li>
<li>收集模型输出并进行打分，然后训练一个reward model。</li>
<li>利用强化学习优化模型。<br><img src="/2023/01/09/zero-to-chatgpt/rlhf-result.jpeg" alt="RLHF evalutions"></li>
</ol>
<p>结果上看，效果显著，1.3B 就超过了之前175B 的结果，而且随着模型增大，结果也在上升。</p>
<h2><span id="instruction-tuning">Instruction-tuning</span><a href="#instruction-tuning" class="header-anchor"></a></h2><p>虽然fine-tuning with human feedback 可以提升LLM 在真实场景中用户任务上(customer task)的性能，但是在学术任务（academic NLP tasks）上的性能却会有所下降，即使OpenAI 尝试在RL 中增加部分pretrain data同时增加LM loss来尝试缓解这个问题，但是依然没有解决。<br><img src="/2023/01/09/zero-to-chatgpt/ins.jpeg" alt="performance on benchmark"></p>
<p>如何解决这个问题呢？办法就是instruction-tuning：<br>利用academic NLP data，为其构造对应的zero-shot/few-shot/CoT pattern，然后进行fine-tuning。<br><img src="/2023/01/09/zero-to-chatgpt/ins-tuning.jpeg" alt="instruction finetuning"></p>
<p>instruction-tuning 效果显著：<br>1.不光能提升大模型在academic NLP benchmark 上的性能，也能提升小模型上的性能；<br>2.能提升instruction-tuning 时未见过的task 上的性能；<br>3.能解锁小模型上的CoT 能力；<br>4.随着任务数量的增加，对应的提升也会增加。<br>5.最重要的是也能提升LLM 理解人类真实指令(follow instruction)的能力。</p>
<p>ps: 虽然follow human instruction 的能力提升了，但是跟InstructGPT 谁的性能更好却没有对比，我猜应该是不如InstructGPT，实际应用/学术指标两者依然是天枰的两端。<br><img src="/2023/01/09/zero-to-chatgpt/flan.png" alt="Instruction-fintuning performance"></p>
<h1><span id="chatgpt">ChatGPT</span><a href="#chatgpt" class="header-anchor"></a></h1><p>那如何才能得到一个ChatGPT呢？</p>
<ol>
<li>首先我们需要一个具备各种能力(潜力)的LLM，所以它要足够大，训练的足够好。OpenAI 大概率也是为此重新训练了一个GPT-3 模型（GPT-3.5），主要论据为：1.原始GPT-3 175B和复现GPT-3 的OPT-175B 都没有CoT 能力，而GPT-3.5 有CoT；<br>2.原始的GPT-3 的窗口只有2048，而其对应的是绝对位置编码，现在的GPT-3.5最大窗口为8192。<br>3.原始的GPT-3 不能写代码，现在的可以。</li>
<li>标注人员手写符合人类的instruction data(最好再混合一些academic instruction data，如：Flan)，然后进行SFT，让模型能更好的follow instruction。</li>
<li>在对话场景下构造对应的instruction data，进一步fine-tuning with human feedback(RLHF加速流程).<br><img src="/2023/01/09/zero-to-chatgpt/chatgpt.png" alt="GPT-3/GPT-3.5 series"><h1><span id="fan-wai-pian">番外篇：</span><a href="#fan-wai-pian" class="header-anchor"></a></h1><h2><span id="ru-he-ti-sheng-llm-zai-mou-ge-zu-te-ding-ren-wu-shang-de-xing-neng">如何提升LLM在某个(组）特定任务上的性能</span><a href="#ru-he-ti-sheng-llm-zai-mou-ge-zu-te-ding-ren-wu-shang-de-xing-neng" class="header-anchor"></a></h2>虽然LLM 具有很多能力，但在实际场景中，我们可能只使用其中的一个或一组特定的能力，那如何提升LLM 在某个特定任务上的性能呢？答案是：不确定。<h3><span id="fine-tuning">Fine-tuning</span><a href="#fine-tuning" class="header-anchor"></a></h3>另一个思考就是构造大量的supervised data 直接fine-tuning。<br>Gopher 中针对对话任务做了对比实验。<br>Dialog-Tuned Gopher: fine-tuning Gopher on 5B tokens of curated dialog dataset from MassiveWeb<br>Dialog-Prompted Gopher: few-shot<br><img src="/2023/01/09/zero-to-chatgpt/dialog.png" alt="prompt vs fine-tuning"></li>
</ol>
<p>可以看到，fine-tuning 后的模型性能与直接prompt 的基本持平（甚至有点下降的趋势），并没有带来任何提升。</p>
<p>而Codex(GPT-3) 针对代码(code) 做了fine-tuning，利用159G github code data 在GPT-3 上进行fine-tuning，模型从基本无法处理代码任务提升到当时的SOTA，甚至只需要12B 就能达到从0 到72% 。<br><img src="/2023/01/09/zero-to-chatgpt/codex.png" alt="Codex"></p>
<h3><span id="fine-tuning-with-human-feedback">Fine-tuning with human feedback</span><a href="#fine-tuning-with-human-feedback" class="header-anchor"></a></h3><p>之前我们提到通过RLHF 可以进行alignment，让模型更好的follow instruction。但是，这种对齐也会对模型的性能带来一定的损失，又叫“对齐税”（alignment tax)。<br><img src="/2023/01/09/zero-to-chatgpt/ins.jpeg" alt="benchmark performance"></p>
<p>在学术NLP的benchmark 上，code-davinci-2（base model of text-davinci-2/text-davinci-3)的性能都是优于fine-tuning 后的模型。</p>
<h3><span id="rag">RAG</span><a href="#rag" class="header-anchor"></a></h3><p>另外一种常用的方案就是RAG（Retrieval-Augmented Generation）<br><img src="/2023/01/09/zero-to-chatgpt/rag.jpeg" alt="open-domain QA performance"></p>
<p>从实验结果上看，RAG 能带来一定的提升，但是有限，不如prompt 方法带来的提升明显。</p>
<p>而另一个工作说，RAG 是带来提升还是下降跟别的因素有关，如在QA 上，他可能跟对应实体的知名度(popularity) 有关。LLM 已经存储了知名度高的实体信息，而RAG 并不能带来性能提升，反而由于retrieval 到错误信息而导致性能下降，对于知名度低的实体通过RAG 是能带来显著提升的。<br><img src="/2023/01/09/zero-to-chatgpt/popular.png" alt="Popularity vs retrieval"></p>
<h3><span id="prompt-engineering">Prompt Engineering</span><a href="#prompt-engineering" class="header-anchor"></a></h3><p>在CoT 出来之前，我们一度认为LLM 可能需要继续进行指数级的扩大才能线性提升其推理能力，而CoT 的出现解锁了模型的推理能力。所以，一个可能的方案可能是在特定任务上继续寻找他的“般若波罗蜜”。不过笔者认为，这只是一个过渡期而非常态，随着RLHF/Instruction-tuning 等方法的发展，未来模型的使用一定会越来越简单便捷。</p>
<h3><span id="instruction-tuning">Instruction-tuning</span><a href="#instruction-tuning" class="header-anchor"></a></h3><p>instruction-tuning 已经证明了他的有效性，如flan-t5,flan-PaLM 经过instruction-tuning 后，其性能都得到了提升。</p>
<h2><span id="ru-he-jiang-neng-li-cong-da-mo-xing-qian-yi-dao-xiao-mo-xing-shang">如何将能力从大模型迁移到小模型上</span><a href="#ru-he-jiang-neng-li-cong-da-mo-xing-qian-yi-dao-xiao-mo-xing-shang" class="header-anchor"></a></h2><ol>
<li>instruction-tuning，通过大量的instruction-data 进行fine-tuning，可以解锁小模型上对应的能力，但是相对大模型，通常还是有差距。</li>
<li>压缩模型，如有工作将OPT-175B 蒸馏至75B，性能基本无损。（但是75B依然很大啊大佬！）。</li>
<li>蒸馏，让性能更好的大模型，生成大量的task-data，然后在小模型上进行fine-tuning，但是这可能需要生成很多data，鉴于LLM 都比较贵，所以这个可能需要很多钱。</li>
</ol>
<h1><span id="ref">Ref</span><a href="#ref" class="header-anchor"></a></h1><p>数学之美</p>
<p><a href="http://www.iro.umontreal.ca/~vincentp/Publications/lm_jmlr.pdf" target="_blank" rel="noopener">A Neural Probabilistic Language Model</a></p>
<p><a href="http://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">Language Models are Few-Shot Learners</a></p>
<p><a href="https://arxiv.org/pdf/2205.01068.pdf" target="_blank" rel="noopener">OPT: Open Pre-trained Transformer Language Models</a></p>
<p><a href="http://arxiv.org/abs/2107.13586" target="_blank" rel="noopener">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a></p>
<p><a href="https://spaces.ac.cn/archives/8295/comment-page-1" target="_blank" rel="noopener">P-tuning：自动构建模版，释放语言模型潜能</a> </p>
<p><a href="http://arxiv.org/abs/2201.11903" target="_blank" rel="noopener">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></p>
<p><a href="http://arxiv.org/abs/2205.11916" target="_blank" rel="noopener">Large Language Models are Zero-Shot Reasoners </a></p>
<p><a href="http://arxiv.org/abs/2203.11171" target="_blank" rel="noopener">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></p>
<p><a href="https://openreview.net/pdf?id=yf1icZHC-l9" target="_blank" rel="noopener">Complexity-based Prompting for Multi-Step Reasoning. </a> </p>
<p><a href="http://arxiv.org/abs/2205.10625" target="_blank" rel="noopener">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models </a></p>
<p><a href="http://arxiv.org/abs/2210.03350" target="_blank" rel="noopener">Measuring and Narrowing the Compositionality Gap in Language Models</a> </p>
<p><a href="http://arxiv.org/abs/2206.07682" target="_blank" rel="noopener">Emergent Abilities of Large Language Models</a> </p>
<p><a href="http://arxiv.org/abs/2203.02155" target="_blank" rel="noopener">Training language models to follow instructions with human feedback</a></p>
<p><a href="http://arxiv.org/abs/2110.14168" target="_blank" rel="noopener">Training Verifiers to Solve Math Word Problems</a></p>
<p><a href="http://arxiv.org/abs/2210.11416" target="_blank" rel="noopener">Scaling Instruction-Finetuned Language Models</a></p>
<p><a href="https://arxiv.org/pdf/2112.11446.pdf" target="_blank" rel="noopener">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</a></p>
<p><a href="https://beta.openai.com/docs/model-index-for-researchers" target="_blank" rel="noopener">model-index-for-researchers</a> </p>
<p><a href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1" target="_blank" rel="noopener">How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources </a></p>
<p><a href="https://openai.com/blog/instruction-following/" target="_blank" rel="noopener">instruction-following</a> </p>
<p><a href="https://arxiv.org/pdf/2209.10063.pdf" target="_blank" rel="noopener">GENERATE RATHER THAN RETRIEVE: LARGE LANGUAGE MODELS ARE STRONG CONTEXT GENERATORS</a></p>
<p><a href="https://akariasai.github.io/files/llm_memorization.pdf" target="_blank" rel="noopener">When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories.</a> </p>
<p><a href="https://arxiv.org/pdf/2107.03374.pdf" target="_blank" rel="noopener">Evaluating Large Language Models Trained on Code</a></p>
<h1><span id="guan-yu-tou-tu">关于头图</span><a href="#guan-yu-tou-tu" class="header-anchor"></a></h1><p>ChatGPT BG from <a href="https://openai.com/blog/chatgpt/" target="_blank" rel="noopener">https://openai.com/blog/chatgpt/</a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>In-context learning</tag>
        <tag>GPT-3</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
</search>
